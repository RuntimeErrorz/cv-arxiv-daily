## Updated on 2024-06-28

## MVS

|Published Date|Title|Journal|Code|Comments|Authors
|---|---|---|---|---|---|
|**2024-06-26**|**[DoubleTake: Geometry Guided Depth Estimation](http://arxiv.org/abs/2406.18387v1)**|None|None|None|Mohamed Sayed et.al.|
|**2024-06-19**|**[MVSBoost: An Efficient Point Cloud-based 3D Reconstruction](http://arxiv.org/abs/2406.13515v1)**|None|None|The work is under review|Umair Haroon et.al.|
|**2024-06-01**|**[Topo4D: Topology-Preserving Gaussian Splatting for High-Fidelity 4D Head Capture](http://arxiv.org/abs/2406.00440v1)**|None|None|None|X. Li et.al.|
|**2024-05-27**|**[SDL-MVS: View Space and Depth Deformable Learning Paradigm for Multi-View Stereo Reconstruction in Remote Sensing](http://arxiv.org/abs/2405.17140v1)**|None|None|None|Yong-Qiang Mao et.al.|
|**2024-05-24**|**[Transparent Object Depth Completion](http://arxiv.org/abs/2405.15299v1)**|None|None|None|Yifan Zhou et.al.|
|**2024-05-21**|**[Cross-spectral Gated-RGB Stereo Depth Estimation](http://arxiv.org/abs/2405.12759v1)**|None|None|None|Samuel Brucker et.al.|
|**2024-05-20**|**[Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo](http://arxiv.org/abs/2405.12218v1)**|None|None|Project page: https://mvsgaussian.github.io/|Tianqi Liu et.al.|
|**2024-05-15**|**[RobustMVS: Single Domain Generalized Deep Multi-view Stereo](http://arxiv.org/abs/2405.09131v1)**|None|**[link](https://github.com/toughstonex/mvs_evaluation_benchmark)**|Accepted to TCSVT. Code will be released at:   https://github.com/ToughStoneX/Robust-MVS. Benchmark will be released at:   https://github.com/ToughStoneX/MVS_Evaluation_Benchmark|Hongbin Xu et.al.|
|**2024-05-14**|**[The RoboDrive Challenge: Drive Anytime Anywhere in Any Condition](http://arxiv.org/abs/2405.08816v2)**|None|None|ICRA 2024; 32 pages, 24 figures, 5 tables; Code at   https://robodrive-24.github.io/|Lingdong Kong et.al.|
|**2024-05-13**|**[SceneFactory: A Workflow-centric and Unified Framework for Incremental Scene Modeling](http://arxiv.org/abs/2405.07847v1)**|None|None|None|Yijun Yuan et.al.|
|**2024-05-10**|**[MGS-SLAM: Monocular Sparse Tracking and Gaussian Mapping with Depth Smooth Regularization](http://arxiv.org/abs/2405.06241v1)**|None|None|This work has been submitted to the IEEE for possible publication.   Copyright may be transferred without notice, after which this version may no   longer be accessible|Pengcheng Zhu et.al.|
|**2024-05-08**|**[Geometry-Informed Distance Candidate Selection for Adaptive Lightweight Omnidirectional Stereo Vision with Fisheye Images](http://arxiv.org/abs/2405.05355v1)**|None|None|None|Conner Pulling et.al.|
|**2024-04-25**|**[Depth Supervised Neural Surface Reconstruction from Airborne Imagery](http://arxiv.org/abs/2404.16429v1)**|None|None|None|Vincent Hackstein et.al.|
|**2024-04-21**|**[Generalizable Novel-View Synthesis using a Stereo Camera](http://arxiv.org/abs/2404.13541v1)**|None|None|Accepted to CVPR 2024. Project page URL:   https://jinwonjoon.github.io/stereonerf/|Haechan Lee et.al.|
|**2024-04-11**|**[GoMVS: Geometrically Consistent Cost Aggregation for Multi-View Stereo](http://arxiv.org/abs/2404.07992v1)**|None|**[link](https://github.com/wuuu3511/gomvs)**|CVPR 2024. Project page: https://wuuu3511.github.io/gomvs/ Code:   https://github.com/Wuuu3511/GoMVS|Jiang Wu et.al.|
|**2024-04-10**|**[MoCha-Stereo: Motif Channel Attention Network for Stereo Matching](http://arxiv.org/abs/2404.06842v2)**|The IEEE/CVF Conference on Computer Vision and Pattern Recognition   2024|**[link](https://github.com/zyangchen/mocha-stereo)**|Accepted to CVPR 2024|Ziyang Chen et.al.|
|**2024-04-08**|**[Learning Topology Uniformed Face Mesh by Volume Rendering for Multi-view Reconstruction](http://arxiv.org/abs/2404.05606v1)**|None|None|None|Yating Wang et.al.|
|**2024-04-08**|**[Adaptive Learning for Multi-view Stereo Reconstruction](http://arxiv.org/abs/2404.05181v1)**|None|None|None|Qinglu Min et.al.|
|**2024-04-04**|**[MVD-Fusion: Single-view 3D via Depth-consistent Multi-view Generation](http://arxiv.org/abs/2404.03656v1)**|None|None|Project page: https://mvd-fusion.github.io/|Hanzhe Hu et.al.|
|**2024-04-02**|**[CHOSEN: Contrastive Hypothesis Selection for Multi-View Depth Refinement](http://arxiv.org/abs/2404.02225v1)**|None|None|None|Di Qiu et.al.|
|**2024-03-18**|**[GNeRP: Gaussian-guided Neural Reconstruction of Reflective Objects with Noisy Polarization Priors](http://arxiv.org/abs/2403.11899v1)**|None|None|Accepted to ICLR 2024 Poster. For the Appendix, please see   http://yukiumi13.github.io/gnerp_page|LI Yang et.al.|
|**2024-03-14**|**[Intention-driven Ego-to-Exo Video Generation](http://arxiv.org/abs/2403.09194v2)**|None|None|None|Hongchen Luo et.al.|
|**2024-03-12**|**[Adaptive Fusion of Single-View and Multi-View Depth for Autonomous Driving](http://arxiv.org/abs/2403.07535v1)**|None|**[link](https://github.com/junda24/afnet)**|Accepted to CVPR 2024|JunDa Cheng et.al.|
|**2024-02-22**|**[GaussianPro: 3D Gaussian Splatting with Progressive Propagation](http://arxiv.org/abs/2402.14650v1)**|None|None|See the project page for code, data:   https://kcheng1021.github.io/gaussianpro.github.io|Kai Cheng et.al.|
|**2024-02-19**|**[SDGE: Stereo Guided Depth Estimation for 360$^\circ$ Camera Sets](http://arxiv.org/abs/2402.11791v4)**|None|None|None|Jialei Xu et.al.|
|**2024-01-28**|**[Multi-Person 3D Pose Estimation from Multi-View Uncalibrated Depth Cameras](http://arxiv.org/abs/2401.15616v1)**|None|None|17 pages including appendix|Yu-Jhe Li et.al.|
|**2024-01-25**|**[Range-Agnostic Multi-View Depth Estimation With Keyframe Selection](http://arxiv.org/abs/2401.14401v1)**|None|**[link](https://github.com/andreaconti/ramdepth)**|3DV 2024 Project Page   https://andreaconti.github.io/projects/range_agnostic_multi_view_depth GitHub   Page https://github.com/andreaconti/ramdepth.git|Andrea Conti et.al.|
|**2024-01-23**|**[PSDF: Prior-Driven Neural Implicit Surface Learning for Multi-view Reconstruction](http://arxiv.org/abs/2401.12751v1)**|None|None|None|Wanjuan Su et.al.|
|**2024-01-22**|**[Boosting Multi-view Stereo with Late Cost Aggregation](http://arxiv.org/abs/2401.11751v2)**|None|**[link](https://github.com/wuuu3511/lamvsnet)**|Code and models are available at https://github.com/Wuuu3511/LAMVSNET|Jiang Wu et.al.|
|**2024-01-22**|**[MVSFormer++: Revealing the Devil in Transformer's Details for Multi-View Stereo](http://arxiv.org/abs/2401.11673v1)**|ICLR(International Conference on Learning Representations) 2024|**[link](https://github.com/maybelx/mvsformerplusplus)**|Accepted to ICLR2024|Chenjie Cao et.al.|
|**2024-01-17**|**[3D Scene Geometry Estimation from 360$^\circ$ Imagery: A Survey](http://arxiv.org/abs/2401.09252v1)**|ACM Comput. Surv. 55, 4, Article 68, 2023|None|Published in ACM Computing Surveys|Thiago Lopes Trugillo da Silveira et.al.|
|**2024-01-12**|**[SD-MVS: Segmentation-Driven Deformation Multi-View Stereo with Spherical Refinement and EM optimization](http://arxiv.org/abs/2401.06385v1)**|None|None|10 pages, 9 figures, published to AAAI2024|Zhenlong Yuan et.al.|
|**2023-12-26**|**[Learning Deformable Hypothesis Sampling for Accurate PatchMatch Multi-View Stereo](http://arxiv.org/abs/2312.15970v1)**|None|**[link](https://github.com/geo-tell/ds-pmnet)**|None|Hongjie Li et.al.|
|**2023-12-23**|**[NoPose-NeuS: Jointly Optimizing Camera Poses with Neural Implicit Surfaces for Multi-view Reconstruction](http://arxiv.org/abs/2312.15238v1)**|UniReps: the First Workshop on Unifying Representations in Neural   Models (2023)|None|None|Mohamed Shawky Sabae et.al.|
|**2023-12-21**|**[DUSt3R: Geometric 3D Vision Made Easy](http://arxiv.org/abs/2312.14132v1)**|None|**[link](https://github.com/naver/dust3r)**|None|Shuzhe Wang et.al.|
|**2023-12-14**|**[CT-MVSNet: Efficient Multi-View Stereo with Cross-scale Transformer](http://arxiv.org/abs/2312.08594v2)**|None|**[link](https://github.com/wscstrive/ct-mvsnet)**|Accepted at the 30th International Conference on Multimedia   Modeling(MMM'24 Oral)|Sicheng Wang et.al.|
|**2023-12-08**|**[MVDD: Multi-View Depth Diffusion Models](http://arxiv.org/abs/2312.04875v3)**|None|None|None|Zhen Wang et.al.|
|**2023-11-17**|**[Point Cloud Self-supervised Learning via 3D to Multi-view Masked Autoencoder](http://arxiv.org/abs/2311.10887v1)**|None|**[link](https://github.com/zhimin-c/multiview-mae)**|None|Zhimin Chen et.al.|
|**2023-11-11**|**[Polarimetric PatchMatch Multi-View Stereo](http://arxiv.org/abs/2311.07600v1)**|None|None|None|Jinyu Zhao et.al.|
|**2023-11-07**|**[High-fidelity 3D Reconstruction of Plants using Neural Radiance Field](http://arxiv.org/abs/2311.04154v1)**|None|None|None|Kewei Hu et.al.|
|**2023-11-02**|**[Novel View Synthesis from a Single RGBD Image for Indoor Scenes](http://arxiv.org/abs/2311.01065v1)**|None|None|2nd International Conference on Image Processing, Computer Vision and   Machine Learning, November 2023|Congrui Hetang et.al.|
|**2023-10-31**|**[Joint Depth Prediction and Semantic Segmentation with Multi-View SAM](http://arxiv.org/abs/2311.00134v1)**|None|None|To appear in the 2024 IEEE/CVF Winter Conference on Applications of   Computer Vision|Mykhailo Shvets et.al.|
|**2023-10-30**|**[GC-MVSNet: Multi-View, Multi-Scale, Geometrically-Consistent Multi-View Stereo](http://arxiv.org/abs/2310.19583v3)**|Proceedings of the IEEE/CVF Winter Conference on Applications of   Computer Vision (WACV) 2024|**[link](https://github.com/vkvats/GC-MVSNet)**|Accepted in WACV 2024 Link:   https://openaccess.thecvf.com/content/WACV2024/html/Vats_GC-MVSNet_Multi-View_Multi-Scale_Geometrically-Consistent_Multi-View_Stereo_WACV_2024_paper.html|Vibhas K. Vats et.al.|
|**2023-10-03**|**[RSRD: A Road Surface Reconstruction Dataset and Benchmark for Safe and Comfortable Autonomous Driving](http://arxiv.org/abs/2310.02262v1)**|None|None|None|Tong Zhao et.al.|
|**2023-09-29**|**[When Epipolar Constraint Meets Non-local Operators in Multi-View Stereo](http://arxiv.org/abs/2309.17218v1)**|None|**[link](https://github.com/tqtqliu/et-mvsnet)**|ICCV2023|Tianqi Liu et.al.|
|**2023-09-26**|**[3D Reconstruction with Generalizable Neural Fields using Scene Priors](http://arxiv.org/abs/2309.15164v2)**|None|None|Project Page: https://oasisyang.github.io/neural-prior|Yang Fu et.al.|
|**2023-09-23**|**[MP-MVS: Multi-Scale Windows PatchMatch and Planar Prior Multi-View Stereo](http://arxiv.org/abs/2309.13294v1)**|None|**[link](https://github.com/rongxuantan/mp-mvs)**|None|Rongxuan Tan et.al.|
|**2023-09-17**|**[A Critical Analysis of Internal Reliability for Uncertainty Quantification of Dense Image Matching in Multi-view Stereo](http://arxiv.org/abs/2309.09379v2)**|ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial   Information Sciences, 2023|None|Figure 8|Debao Huang et.al.|
|**2023-09-01**|**[Dense Voxel 3D Reconstruction Using a Monocular Event Camera](http://arxiv.org/abs/2309.00385v1)**|None|None|None|Haodong Chen et.al.|
|**2023-09-01**|**[SparseSat-NeRF: Dense Depth Supervised Neural Radiance Fields for Sparse Satellite Images](http://arxiv.org/abs/2309.00277v1)**|None|**[link](https://github.com/lulinzhang/sps-nerf)**|ISPRS Annals 2023|Lulin Zhang et.al.|
|**2023-08-26**|**[Disjoint Pose and Shape for 3D Face Reconstruction](http://arxiv.org/abs/2308.13903v1)**|None|None|ICCV workshops 2023|Raja Kumar et.al.|
|**2023-08-19**|**[Textureless-aware Segmentation and Correlative Refinement Guided Multi-View Stereo](http://arxiv.org/abs/2308.09990v2)**|None|None|None|Zhenlong Yuan et.al.|
|**2023-08-17**|**[ARAI-MVSNet: A multi-view stereo depth estimation network with adaptive depth range and depth interval](http://arxiv.org/abs/2308.09022v1)**|None|**[link](https://github.com/zs670980918/arai-mvsnet)**|None|Song Zhang et.al.|
|**2023-08-17**|**[V-FUSE: Volumetric Depth Map Fusion with Long-Range Constraints](http://arxiv.org/abs/2308.08715v1)**|None|**[link](https://github.com/nburgdorfer/v-fuse)**|ICCV 2023|Nathaniel Burgdorfer et.al.|
|**2023-08-15**|**[ObjectSDF++: Improved Object-Compositional Neural Implicit Surfaces](http://arxiv.org/abs/2308.07868v2)**|None|**[link](https://github.com/qianyiwu/objectsdf_plus)**|ICCV 2023. Project Page: https://qianyiwu.github.io/objectsdf++ Code:   https://github.com/QianyiWu/objectsdf_plus|Qianyi Wu et.al.|
|**2023-08-14**|**[A One Stop 3D Target Reconstruction and multilevel Segmentation Method](http://arxiv.org/abs/2308.06974v1)**|None|**[link](https://github.com/ganlab/ostra)**|None|Jiexiong Xu et.al.|
|**2023-08-09**|**[WaveNeRF: Wavelet-based Generalizable Neural Radiance Fields](http://arxiv.org/abs/2308.04826v2)**|None|None|Accepted to ICCV 2023. Project website:   https://mxuai.github.io/WaveNeRF/|Muyu Xu et.al.|
|**2023-08-07**|**[Learning Photometric Feature Transform for Free-form Object Scan](http://arxiv.org/abs/2308.03492v1)**|None|None|None|Xiang Feng et.al.|
|**2023-08-04**|**[ES-MVSNet: Efficient Framework for End-to-end Self-supervised Multi-View Stereo](http://arxiv.org/abs/2308.02191v1)**|None|None|arXiv admin note: text overlap with arXiv:2203.03949 by other authors|Qiang Zhou et.al.|
|**2023-08-02**|**[Tirtha -- An Automated Platform to Crowdsource Images and Create 3D Models of Heritage Sites](http://arxiv.org/abs/2308.01246v2)**|None|**[link](https://github.com/smlab-niser/tirtha-public)**|Accepted at The 28th International ACM Conference on 3D Web   Technology (Web3D 2023)|Jyotirmaya Shivottam et.al.|
|**2023-07-16**|**[RayMVSNet++: Learning Ray-based 1D Implicit Fields for Accurate Multi-View Stereo](http://arxiv.org/abs/2307.10233v1)**|None|None|IEEE Transactions on Pattern Analysis and Machine Intelligence. arXiv   admin note: substantial text overlap with arXiv:2204.01320|Yifei Shi et.al.|
|**2023-07-18**|**[Constraining Depth Map Geometry for Multi-View Stereo: A Dual-Depth Approach with Saddle-shaped Depth Cells](http://arxiv.org/abs/2307.09160v1)**|None|**[link](https://github.com/dive128/dmvsnet)**|Accepted by ICCV 2023|Xinyi Ye et.al.|
|**2023-07-03**|**[MVDiffusion: Enabling Holistic Multi-view Image Generation with Correspondence-Aware Diffusion](http://arxiv.org/abs/2307.01097v7)**|None|**[link](https://github.com/Tangshitao/MVDiffusion)**|Project page, https://mvdiffusion.github.io; NeurIPS 2023   (spotlight); Compressed camera-ready version|Shitao Tang et.al.|
|**2023-06-22**|**[One at a Time: Progressive Multi-step Volumetric Probability Learning for Reliable 3D Scene Perception](http://arxiv.org/abs/2306.12681v4)**|None|None|AAAI2024|Bohan Li et.al.|
|**2023-06-16**|**[C2F2NeUS: Cascade Cost Frustum Fusion for High Fidelity and Generalizable Neural Surface Reconstruction](http://arxiv.org/abs/2306.10003v2)**|None|None|Accepted by ICCV2023|Luoyuan Xu et.al.|
|**2023-06-14**|**[SimpleMapping: Real-Time Visual-Inertial Dense Mapping with Deep Multi-View Stereo](http://arxiv.org/abs/2306.08648v3)**|None|None|None|Yingye Xin et.al.|
|**2023-06-12**|**[Instant Multi-View Head Capture through Learnable Registration](http://arxiv.org/abs/2306.07437v1)**|None|**[link](https://github.com/TimoBolkart/TEMPEH)**|Conference on Computer Vision and Pattern Recognition (CVPR) 2023|Timo Bolkart et.al.|
|**2023-06-01**|**[DiffInDScene: Diffusion-based High-Quality 3D Indoor Scene Generation](http://arxiv.org/abs/2306.00519v4)**|None|**[link](https://github.com/akirahero/diffindscene)**|Updated: new work|Xiaoliang Ju et.al.|
|**2023-05-31**|**[A technique to jointly estimate depth and depth uncertainty for unmanned aerial vehicles](http://arxiv.org/abs/2305.19780v1)**|None|**[link](https://github.com/michael-fonder/m4depthu)**|The code is available at https://github.com/michael-fonder/M4DepthU|Michaël Fonder et.al.|
|**2023-05-28**|**[OccCasNet: Occlusion-aware Cascade Cost Volume for Light Field Depth Estimation](http://arxiv.org/abs/2305.17710v1)**|None|**[link](https://github.com/chaowentao/occcasnet)**|None|Wentao Chao et.al.|
|**2023-05-24**|**[Incremental Dense Reconstruction from Monocular Video with Guided Sparse Feature Volume Fusion](http://arxiv.org/abs/2305.14918v1)**|None|None|8 pages, 5 figures, RA-L 2023|Xingxing Zuo et.al.|
|**2023-05-18**|**[MVPSNet: Fast Generalizable Multi-view Photometric Stereo](http://arxiv.org/abs/2305.11167v1)**|None|None|None|Dongxu Zhao et.al.|
|**2023-05-17**|**[CostFormer:Cost Transformer for Cost Aggregation in Multi-view Stereo](http://arxiv.org/abs/2305.10320v1)**|None|None|Accepted by IJCAI-23|Weitao Chen et.al.|
|**2023-05-10**|**[FusionDepth: Complement Self-Supervised Monocular Depth Estimation with Cost Volume](http://arxiv.org/abs/2305.06036v1)**|None|None|None|Zhuofei Huang et.al.|
|**2023-04-28**|**[CVRecon: Rethinking 3D Geometric Feature Learning For Neural Reconstruction](http://arxiv.org/abs/2304.14633v3)**|None|None|Accepted by ICCV 2023|Ziyue Feng et.al.|
|**2023-04-26**|**[Multi-View Stereo Representation Revisit: Region-Aware MVSNet](http://arxiv.org/abs/2304.13614v2)**|None|None|CVPR 2023|Yisu Zhang et.al.|
|**2023-04-20**|**[A Comparative Neural Radiance Field (NeRF) 3D Analysis of Camera Poses from HoloLens Trajectories and Structure from Motion](http://arxiv.org/abs/2304.10664v1)**|None|None|7 pages, 5 figures. Will be published in the ISPRS The International   Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences|Miriam Jäger et.al.|
|**2023-04-09**|**[BEVStereo++: Accurate Depth Estimation in Multi-view 3D Object Detection via Dynamic Temporal Stereo](http://arxiv.org/abs/2304.04185v1)**|None|None|None|Yinhao Li et.al.|
|**2023-04-08**|**[POEM: Reconstructing Hand in a Point Embedded Multi-view Stereo](http://arxiv.org/abs/2304.04038v2)**|None|**[link](https://github.com/lixiny/poem)**|Accepted by CVPR 2023. (v2 fix typos)|Lixin Yang et.al.|
|**2023-04-04**|**[End-to-End Latency Optimization of Multi-view 3D Reconstruction for Disaster Response](http://arxiv.org/abs/2304.01488v1)**|None|None|2022 10th IEEE International Conference on Mobile Cloud Computing,   Services, and Engineering (MobileCloud)|Xiaojie Zhang et.al.|
|**2023-04-04**|**[FineRecon: Depth-aware Feed-forward Network for Detailed 3D Reconstruction](http://arxiv.org/abs/2304.01480v2)**|None|**[link](https://github.com/apple/ml-finerecon)**|ICCV 2023|Noah Stier et.al.|
|**2023-03-30**|**[S-VolSDF: Sparse Multi-View Stereo Regularization of Neural Implicit Surfaces](http://arxiv.org/abs/2303.17712v2)**|None|None|ICCV 2023, Project page: https://hao-yu-wu.github.io/s-volsdf/|Haoyu Wu et.al.|
|**2023-03-29**|**[DORT: Modeling Dynamic Objects in Recurrent for Multi-Camera 3D Object Detection and Tracking](http://arxiv.org/abs/2303.16628v2)**|None|**[link](https://github.com/smartbot-pjlab/dort)**|None|Qing Lian et.al.|
|**2023-03-29**|**[Multi-View Azimuth Stereo via Tangent Space Consistency](http://arxiv.org/abs/2303.16447v1)**|None|**[link](https://github.com/xucao-42/mvas)**|CVPR 2023 camera-ready. Appendices after references. 16 pages, 20   figures. Project page: https://xucao-42.github.io/mvas_homepage/|Xu Cao et.al.|
|**2023-03-28**|**[Sparse Depth-Guided Attention for Accurate Depth Completion: A Stereo-Assisted Monitored Distillation Approach](http://arxiv.org/abs/2303.15840v3)**|None|None|7 pages, 8 figures, references added|Jia-Wei Guo et.al.|
|**2023-03-27**|**[TMO: Textured Mesh Acquisition of Objects with a Mobile Device by using Differentiable Rendering](http://arxiv.org/abs/2303.15060v1)**|None|None|Accepted to CVPR23. Project Page: https://jh-choi.github.io/TMO/|Jaehoon Choi et.al.|
|**2023-03-17**|**[Hierarchical Prior Mining for Non-local Multi-View Stereo](http://arxiv.org/abs/2303.09758v1)**|None|None|None|Chunlin Ren et.al.|
|**2023-03-15**|**[RefiNeRF: Modelling dynamic neural radiance fields with inconsistent or missing camera parameters](http://arxiv.org/abs/2303.08695v1)**|None|None|None|Shuja Khalid et.al.|
|**2023-03-12**|**[Iterative Geometry Encoding Volume for Stereo Matching](http://arxiv.org/abs/2303.06615v2)**|None|**[link](https://github.com/gangweix/igev)**|Accepted to CVPR 2023|Gangwei Xu et.al.|
|**2023-03-11**|**[Rethinking the Multi-view Stereo from the Perspective of Rendering-based Augmentation](http://arxiv.org/abs/2303.06418v1)**|None|None|This is a technical report of team Ewrfcas (Fudan University) in the   GigaMVS reconstruction competition. Our method achieved the 1st performance   in the 2022 reconstruction benchmark|Chenjie Cao et.al.|
|**2023-03-10**|**[MVImgNet: A Large-scale Dataset of Multi-view Images](http://arxiv.org/abs/2303.06042v1)**|None|None|To be appear in CVPR2023. Project page:   https://gaplab.cuhk.edu.cn/projects/MVImgNet/|Xianggang Yu et.al.|
|**2023-02-28**|**[HelixSurf: A Robust and Efficient Neural Implicit Surface Learning of Indoor Scenes with Iterative Intertwined Regularization](http://arxiv.org/abs/2302.14340v2)**|None|**[link](https://github.com/gorilla-lab-scut/helixsurf)**|None|Zhihao Liang et.al.|
|**2023-02-20**|**[Unsupervised OmniMVS: Efficient Omnidirectional Depth Inference via Establishing Pseudo-Stereo Supervision](http://arxiv.org/abs/2302.09922v2)**|None|None|None|Zisong Chen et.al.|
|**2023-02-14**|**[Visibility-Aware Pixelwise View Selection for Multi-View Stereo Matching](http://arxiv.org/abs/2302.07182v1)**|None|None|8 pages|Zhentao Huang et.al.|
|**2023-01-21**|**[Dense RGB SLAM with Neural Implicit Maps](http://arxiv.org/abs/2301.08930v2)**|None|None|Accepted by ICLR 2023; Camera-Ready Version; The code is at   poptree.github.io/DIM-SLAM|Heng Li et.al.|
|**2022-12-24**|**[Polarimetric Multi-View Inverse Rendering](http://arxiv.org/abs/2212.12721v1)**|None|None|Paper accepted in IEEE Transactions on Pattern Analysis and Machine   Intelligence (2022). arXiv admin note: substantial text overlap with   arXiv:2007.08830|Jinyu Zhao et.al.|
|**2022-12-13**|**[DELS-MVS: Deep Epipolar Line Search for Multi-View Stereo](http://arxiv.org/abs/2212.06626v1)**|None|None|accepted at WACV 2023|Christian Sormann et.al.|
|**2022-11-30**|**[Rethinking Disparity: A Depth Range Free Multi-View Stereo Based on Disparity](http://arxiv.org/abs/2211.16905v2)**|None|**[link](https://github.com/Yannnnnnnnnnnn/DispMVS_release)**|Accepted at the Thirty-Seventh AAAI Conference on Artificial   Intelligence (AAAI23)|Qingsong Yan et.al.|
|**2022-10-20**|**[Multi-View Guided Multi-View Stereo](http://arxiv.org/abs/2210.11467v1)**|None|**[link](https://github.com/andreaconti/multi-view-guided-multi-view-stereo)**|IROS 2022. First two authors contributed equally. Project page:   https://github.com/andreaconti/multi-view-guided-multi-view-stereo|Matteo Poggi et.al.|
|**2022-10-14**|**[Multi-View Photometric Stereo Revisited](http://arxiv.org/abs/2210.07670v1)**|None|None|Accepted for publication at IEEE/CVF WACV 2023. Draft info: 10 pages,   5 figure, and 3 tables|Berk Kaya et.al.|
|**2022-10-14**|**[Deep PatchMatch MVS with Learned Patch Coplanarity, Geometric Consistency and Adaptive Pixel Sampling](http://arxiv.org/abs/2210.07582v1)**|None|None|None|Jae Yong Lee et.al.|
|**2022-10-05**|**[Multi-Camera Collaborative Depth Prediction via Consistent Structure Estimation](http://arxiv.org/abs/2210.02009v1)**|None|None|None|Jialei Xu et.al.|
|**2022-10-03**|**[CLIP2Point: Transfer CLIP to Point Cloud Classification with Image-Depth Pre-training](http://arxiv.org/abs/2210.01055v3)**|None|**[link](https://github.com/tyhuang0428/CLIP2Point)**|Accepted by ICCV2023|Tianyu Huang et.al.|
|**2022-09-21**|**[BEVStereo: Enhancing Depth Estimation in Multi-view 3D Object Detection with Dynamic Temporal Stereo](http://arxiv.org/abs/2209.10248v1)**|None|**[link](https://github.com/megvii-basedetection/bevstereo)**|None|Yinhao Li et.al.|
|**2022-09-13**|**[A Benchmark and a Baseline for Robust Multi-view Depth Estimation](http://arxiv.org/abs/2209.06681v1)**|None|**[link](https://github.com/lmb-freiburg/robustmvd)**|Accepted at 3DV 2022|Philipp Schröppel et.al.|
|**2022-08-31**|**[Multi-View Reconstruction using Signed Ray Distance Functions (SRDF)](http://arxiv.org/abs/2209.00082v2)**|None|None|None|Pierre Zins et.al.|
|**2022-08-31**|**[SimpleRecon: 3D Reconstruction Without 3D Convolutions](http://arxiv.org/abs/2208.14743v1)**|None|None|ECCV2022 version with improved timings. 14 pages + 5 pages of   references|Mohamed Sayed et.al.|
|**2022-08-19**|**[Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning](http://arxiv.org/abs/2208.09170v1)**|None|**[link](https://github.com/jeffwang987/movedepth)**|code: https://github.com/JeffWang987/MOVEDepth|Xiaofeng Wang et.al.|
|**2022-08-04**|**[MVSFormer: Multi-View Stereo by Learning Robust Image Features and Temperature-based Depth](http://arxiv.org/abs/2208.02541v3)**|None|**[link](https://github.com/ewrfcas/mvsformer)**|None|Chenjie Cao et.al.|
|**2022-07-30**|**[Learning Pseudo Front Depth for 2D Forward-Looking Sonar-based Multi-view Stereo](http://arxiv.org/abs/2208.00233v1)**|None|**[link](https://github.com/sollynoay/epssn)**|Accepted at IROS 2022|Yusheng Wang et.al.|
|**2022-07-27**|**[Towards the Probabilistic Fusion of Learned Priors into Standard Pipelines for 3D Reconstruction](http://arxiv.org/abs/2207.13464v1)**|None|None|Accepted at ICRA 2020|Tristan Laidlow et.al.|
|**2022-07-25**|**[Cost Volume Pyramid Network with Multi-strategies Range Searching for Multi-view Stereo](http://arxiv.org/abs/2207.12032v1)**|None|**[link](https://github.com/SibylGao/MSCVP-MVSNet)**|Accepted by CGI2022|Shiyu Gao et.al.|
|**2022-07-25**|**[nLMVS-Net: Deep Non-Lambertian Multi-View Stereo](http://arxiv.org/abs/2207.11876v2)**|None|None|Accepted to WACV 2023|Kohei Yamashita et.al.|
|**2022-07-24**|**[Semi-supervised Deep Multi-view Stereo](http://arxiv.org/abs/2207.11699v4)**|None|None|This paper is accepted in ACMMM-2023. The code is released at:   https://github.com/ToughStoneX/Semi-MVS|Hongbin Xu et.al.|
|**2022-07-21**|**[KD-MVS: Knowledge Distillation Based Self-supervised Learning for Multi-view Stereo](http://arxiv.org/abs/2207.10425v2)**|None|**[link](https://github.com/megvii-research/kd-mvs)**|None|Yikang Ding et.al.|
|**2022-07-18**|**[Revisiting PatchMatch Multi-View Stereo for Urban 3D Reconstruction](http://arxiv.org/abs/2207.08439v1)**|None|None|Poster presentation at IEEE Intelligent Vehicles Symposium (IV 2022,   https://iv2022.com/)|Marco Orsingher et.al.|
|**2022-07-18**|**[Efficient View Clustering and Selection for City-Scale 3D Reconstruction](http://arxiv.org/abs/2207.08434v1)**|None|None|Oral presentation at ICIAP 2021 (https://www.iciap2021.org/)|Marco Orsingher et.al.|
|**2022-06-22**|**[Monocular Spherical Depth Estimation with Explicitly Connected Weak Layout Cues](http://arxiv.org/abs/2206.11358v1)**|ISPRS Journal of Photogrammetry and Remote Sensing, Volume 183,   January 2022, Pages 269-285|None|Project page at https://vcl3d.github.io/ExplicitLayoutDepth/|Nikolaos Zioulis et.al.|
|**2022-06-21**|**[Enhancing Multi-view Stereo with Contrastive Matching and Weighted Focal Loss](http://arxiv.org/abs/2206.10360v1)**|None|None|5 pages, 3 figures; Accepted to ICIP2022|Yikang Ding et.al.|
|**2022-06-16**|**[Virtual Correspondence: Humans as a Cue for Extreme-View Geometry](http://arxiv.org/abs/2206.08365v1)**|None|None|CVPR 2022. Project page:   https://people.csail.mit.edu/weichium/virtual-correspondence/|Wei-Chiu Ma et.al.|
|**2022-06-01**|**[MonoSDF: Exploring Monocular Geometric Cues for Neural Implicit Surface Reconstruction](http://arxiv.org/abs/2206.00665v2)**|None|None|Project page: https://niujinshuchong.github.io/monosdf/|Zehao Yu et.al.|
|**2022-05-31**|**[Geo-Neus: Geometry-Consistent Neural Implicit Surfaces Learning for Multi-view Reconstruction](http://arxiv.org/abs/2205.15848v1)**|None|None|None|Qiancheng Fu et.al.|
|**2022-05-28**|**[RIAV-MVS: Recurrent-Indexing an Asymmetric Volume for Multi-View Stereo](http://arxiv.org/abs/2205.14320v3)**|None|None|CVPR 2023|Changjiang Cai et.al.|
|**2022-05-28**|**[WT-MVSNet: Window-based Transformers for Multi-view Stereo](http://arxiv.org/abs/2205.14319v1)**|None|None|None|Jinli Liao et.al.|
|**2022-05-25**|**[Multiview Textured Mesh Recovery by Differentiable Rendering](http://arxiv.org/abs/2205.12468v3)**|None|**[link](https://github.com/l1346792580123/diff)**|None|Lixiang Lin et.al.|
|**2022-05-14**|**[SaiNet: Stereo aware inpainting behind objects with generative networks](http://arxiv.org/abs/2205.07014v1)**|None|None|Presented at AI4CC workshop at CVPR|Violeta Menéndez González et.al.|
|**2022-05-08**|**[Non-parametric Depth Distribution Modelling based Depth Inference for Multi-view Stereo](http://arxiv.org/abs/2205.03783v1)**|None|**[link](https://github.com/nvlabs/np-cvp-mvsnet)**|CVPR 2022|Jiayu Yang et.al.|
|**2022-05-05**|**[Neural 3D Scene Reconstruction with the Manhattan-world Assumption](http://arxiv.org/abs/2205.02836v2)**|None|**[link](https://github.com/zju3dv/manhattan_sdf)**|CVPR 2022 Oral. Project page: https://zju3dv.github.io/manhattan_sdf|Haoyu Guo et.al.|
|**2022-05-05**|**[Exploiting Correspondences with All-pairs Correlations for Multi-view Depth Estimation](http://arxiv.org/abs/2205.02481v1)**|None|None|10 pages, 9 figures|Kai Cheng et.al.|
|**2022-04-22**|**[Leveraging Deepfakes to Close the Domain Gap between Real and Synthetic Images in Facial Capture Pipelines](http://arxiv.org/abs/2204.10746v2)**|None|None|None|Winnie Lin et.al.|
|**2022-04-15**|**[MVSTER: Epipolar Transformer for Efficient Multi-View Stereo](http://arxiv.org/abs/2204.07346v1)**|None|**[link](https://github.com/jeffwang987/mvster)**|Code: https://github.com/JeffWang987/MVSTER|Xiaofeng Wang et.al.|
|**2022-04-08**|**[Investigating Spherical Epipolar Rectification for Multi-View Stereo 3D Reconstruction](http://arxiv.org/abs/2204.04141v1)**|None|None|to be published in ISPRS Congress 2022|Mostafa Elhashash et.al.|
|**2022-04-04**|**[RayMVSNet: Learning Ray-based 1D Implicit Fields for Accurate Multi-View Stereo](http://arxiv.org/abs/2204.01320v1)**|None|None|cvpr 2022, 11 pages|Junhua Xi et.al.|
|**2022-04-04**|**[Aligning Silhouette Topology for Self-Adaptive 3D Human Pose Recovery](http://arxiv.org/abs/2204.01276v1)**|None|None|NeurIPS 2021|Mugalodi Rakesh et.al.|
|**2022-03-27**|**[SuperMVS: Non-Uniform Cost Volume For High-Resolution Multi-View Stereo](http://arxiv.org/abs/2203.14331v2)**|None|None|None|Tao Zhang et.al.|
|**2022-03-24**|**[RayTran: 3D pose estimation and shape reconstruction of multiple objects from videos with ray-traced transformers](http://arxiv.org/abs/2203.13296v2)**|None|None|ECCV 2022 camera ready|Michał J. Tyszkiewicz et.al.|
|**2022-03-23**|**[Event-Based Dense Reconstruction Pipeline](http://arxiv.org/abs/2203.12270v1)**|None|None|None|Kun Xiao et.al.|
|**2022-03-22**|**[PlaneMVS: 3D Plane Reconstruction from Multi-View Stereo](http://arxiv.org/abs/2203.12082v3)**|None|**[link](https://github.com/oppo-us-research/planemvs)**|CVPR 2022; source code: https://github.com/oppo-us-research/PlaneMVS|Jiachen Liu et.al.|
|**2022-03-16**|**[DiFT: Differentiable Differential Feature Transform for Multi-View Stereo](http://arxiv.org/abs/2203.08435v1)**|None|None|None|Kaizhang Kang et.al.|
|**2022-03-08**|**[RC-MVSNet: Unsupervised Multi-View Stereo with Neural Rendering](http://arxiv.org/abs/2203.03949v4)**|None|**[link](https://github.com/boese0601/rc-mvsnet)**|Accepted by ECCV 2022, Project Page:   https://boese0601.github.io/rc-mvsnet/|Di Chang et.al.|
|**2022-03-04**|**[PatchMVSNet: Patch-wise Unsupervised Multi-View Stereo for Weakly-Textured Surface Reconstruction](http://arxiv.org/abs/2203.02156v1)**|None|None|None|Haonan Dong et.al.|
|**2022-03-02**|**[DDL-MVS: Depth Discontinuity Learning for MVS Networks](http://arxiv.org/abs/2203.01391v3)**|None|**[link](https://github.com/Mirmix/ddlmvs)**|None|Nail Ibrahimli et.al.|
|**2022-02-26**|**[Accurate Human Body Reconstruction for Volumetric Video](http://arxiv.org/abs/2202.13118v1)**|None|None|2021 International Conference on 3D Immersion (IC3D)|Decai Chen et.al.|
|**2022-02-26**|**[Uncertainty-Aware Deep Multi-View Photometric Stereo](http://arxiv.org/abs/2202.13071v2)**|None|None|Accepted for publication in IEEE/CVF CVPR 2022. (11 Pages, 6 Figures,   3 Tables)|Berk Kaya et.al.|
|**2022-01-21**|**[Point-NeRF: Point-based Neural Radiance Fields](http://arxiv.org/abs/2201.08845v7)**|In Proceedings of the IEEE/CVF Conference on Computer Vision and   Pattern Recognition (pp. 5438-5448) (2022)|**[link](https://github.com/Xharlie/pointnerf)**|Accepted to CVPR 2022 (Oral)|Qiangeng Xu et.al.|
|**2022-01-19**|**[A Confidence-based Iterative Solver of Depths and Surface Normals for Deep Multi-view Stereo](http://arxiv.org/abs/2201.07609v1)**|None|**[link](https://github.com/thuzhaowang/idn-solver)**|17 pages, 13 figures, 7 tables. ICCV 2021|Wang Zhao et.al.|
|**2022-01-05**|**[Rethinking Depth Estimation for Multi-View Stereo: A Unified Representation](http://arxiv.org/abs/2201.01501v3)**|None|**[link](https://github.com/prstrive/unimvsnet)**|CVPR 2022 Accepted|Rui Peng et.al.|
|**2022-01-04**|**[Detailed Facial Geometry Recovery from Multi-View Images by Learning an Implicit Function](http://arxiv.org/abs/2201.01016v2)**|None|**[link](https://github.com/zhuhao-nju/mvfr)**|AAAI 2022 Oral, updated to camera ready version|Yunze Xiao et.al.|
|**2021-12-18**|**[3D Instance Segmentation of MVS Buildings](http://arxiv.org/abs/2112.09902v2)**|None|None|14 figures, 12 figures|Jiazhou Chen et.al.|
|**2021-12-15**|**[Multi-View Depth Estimation by Fusing Single-View Depth Probability with Multi-View Geometry](http://arxiv.org/abs/2112.08177v2)**|None|**[link](https://github.com/baegwangbin/magnet)**|CVPR 2022 (oral)|Gwangbin Bae et.al.|
|**2021-12-13**|**[VirtualCube: An Immersive 3D Video Communication System](http://arxiv.org/abs/2112.06730v2)**|None|None|Project page:   https://www.microsoft.com/en-us/research/project/virtualcube/|Yizhong Zhang et.al.|
|**2021-12-12**|**[MVLayoutNet:3D layout reconstruction with multi-view panoramas](http://arxiv.org/abs/2112.06133v1)**|None|None|None|Zhihua Hu et.al.|
|**2021-12-11**|**[Curvature-guided dynamic scale networks for Multi-view Stereo](http://arxiv.org/abs/2112.05999v3)**|None|**[link](https://github.com/truongkhang/cds-mvsnet)**|Accepted to ICLR 2022|Khang Truong Giang et.al.|
|**2021-12-09**|**[IterMVS: Iterative Probability Estimation for Efficient Multi-View Stereo](http://arxiv.org/abs/2112.05126v1)**|None|**[link](https://github.com/fangjinhuawang/itermvs)**|None|Fangjinhua Wang et.al.|
|**2021-12-06**|**[Input-level Inductive Biases for 3D Reconstruction](http://arxiv.org/abs/2112.03243v2)**|None|None|CVPR 2022, including supplemental material|Wang Yifan et.al.|
|**2021-12-04**|**[PointCLIP: Point Cloud Understanding by CLIP](http://arxiv.org/abs/2112.02413v1)**|None|**[link](https://github.com/zrrskywalker/pointclip)**|Open sourced, Code and Model Available|Renrui Zhang et.al.|
|**2021-12-04**|**[Generalized Binary Search Network for Highly-Efficient Multi-View Stereo](http://arxiv.org/abs/2112.02338v1)**|None|**[link](https://github.com/mizhenxing/gbi-net)**|16 pages|Zhenxing Mi et.al.|
|**2021-12-02**|**[Dimensions of Motion: Monocular Prediction through Flow Subspaces](http://arxiv.org/abs/2112.01502v4)**|None|None|Project page at https://dimensions-of-motion.github.io/|Richard Strong Bowen et.al.|
|**2021-12-01**|**[FaSS-MVS -- Fast Multi-View Stereo with Surface-Aware Semi-Global Matching from UAV-borne Monocular Imagery](http://arxiv.org/abs/2112.00821v1)**|None|None|None|Boitumelo Ruf et.al.|
|**2021-12-01**|**[Multi-View Stereo with Transformer](http://arxiv.org/abs/2112.00336v1)**|None|None|None|Jie Zhu et.al.|
|**2021-12-01**|**[3DVNet: Multi-View Depth Prediction and Volumetric Refinement](http://arxiv.org/abs/2112.00202v1)**|None|**[link](https://github.com/alexrich021/3dvnet)**|10 pages, 6 figures, 3 tables. Accepted to 3DV 2021|Alexander Rich et.al.|
|**2021-11-29**|**[TransMVSNet: Global Context-aware Multi-view Stereo Network with Transformers](http://arxiv.org/abs/2111.14600v1)**|None|**[link](https://github.com/megviirobot/transmvsnet)**|None|Yikang Ding et.al.|
|**2021-11-29**|**[IB-MVS: An Iterative Algorithm for Deep Multi-View Stereo based on Binary Decisions](http://arxiv.org/abs/2111.14420v1)**|None|None|accepted at BMVC 2021|Christian Sormann et.al.|
|**2021-10-16**|**[Multi-View Stereo Network with attention thin volume](http://arxiv.org/abs/2110.08556v2)**|None|None|None|Zihang Wan et.al.|
|**2021-10-14**|**[DeepMoCap: Deep Optical Motion Capture Using Multiple Depth Sensors and Retro-Reflectors](http://arxiv.org/abs/2110.07283v1)**|Sensors, 19(2), 282, 2019|**[link](https://github.com/tofis/deepmocap)**|None|Anargyros Chatzitofis et.al.|
|**2021-10-13**|**[Non-local Recurrent Regularization Networks for Multi-view Stereo](http://arxiv.org/abs/2110.06436v1)**|None|None|None|Qingshan Xu et.al.|
|**2021-10-11**|**[Neural Radiance Fields Approach to Deep Multi-View Photometric Stereo](http://arxiv.org/abs/2110.05594v1)**|None|None|Accepted for publication at IEEE/CVF WACV 2022. 18 pages|Berk Kaya et.al.|
|**2021-10-11**|**[Differentiable Stereopsis: Meshes from multiple views using differentiable rendering](http://arxiv.org/abs/2110.05472v2)**|In CVPR 2022 (pp. 8635-8644)|**[link](https://github.com/shubham-goel/ds)**|In CVPR2022. Project webpage: https://shubham-goel.github.io/ds/|Shubham Goel et.al.|
|**2021-10-06**|**[Topologically Consistent Multi-View Face Inference Using Volumetric Sampling](http://arxiv.org/abs/2110.02948v1)**|None|None|International Conference on Computer Vision (ICCV)|Tianye Li et.al.|
|**2021-09-06**|**[Single-Camera 3D Head Fitting for Mixed Reality Clinical Applications](http://arxiv.org/abs/2109.02740v2)**|None|None|None|Tejas Mane et.al.|
|**2021-09-06**|**[Point-Based Neural Rendering with Per-View Optimization](http://arxiv.org/abs/2109.02369v2)**|In Computer Graphics Forum, Vol. 10. Wiley Online Library, 29-43   (2021)|None|https://repo-sam.inria.fr/fungraph/differentiable-multi-view/|Georgios Kopanas et.al.|
|**2021-09-02**|**[NerfingMVS: Guided Optimization of Neural Radiance Fields for Indoor Multi-view Stereo](http://arxiv.org/abs/2109.01129v3)**|None|**[link](https://github.com/weiyithu/nerfingmvs)**|To appear in ICCV 2021 (Oral). Project page:   https://weiyithu.github.io/NerfingMVS/|Yi Wei et.al.|
|**2021-08-30**|**[Digging into Uncertainty in Self-supervised Multi-view Stereo](http://arxiv.org/abs/2108.12966v2)**|None|**[link](https://github.com/toughstonex/u-mvs)**|This paper is accepted by ICCV-21 as a poster presentation|Hongbin Xu et.al.|
|**2021-08-19**|**[PatchMatch-RL: Deep MVS with Pixelwise Depth, Normal, and Visibility](http://arxiv.org/abs/2108.08943v1)**|None|**[link](https://github.com/leejaeyong7/patchmatch-rl)**|Accepted to ICCV 2021 for oral presentation|Jae Yong Lee et.al.|
|**2021-08-19**|**[VolumeFusion: Deep Depth Fusion for 3D Scene Reconstruction](http://arxiv.org/abs/2108.08623v1)**|None|None|ICCV 2021 Accepted|Jaesung Choe et.al.|
|**2021-08-09**|**[NeuralMVS: Bridging Multi-View Stereo and Novel View Synthesis](http://arxiv.org/abs/2108.03880v2)**|None|**[link](https://github.com/ais-bonn/neural_mvs)**|Accepted for International Joint Conference on Neural Networks   (IJCNN) 2022. Code available at https://github.com/AIS-Bonn/neural_mvs|Radu Alexandru Rosu et.al.|
|**2021-08-09**|**[AA-RMVSNet: Adaptive Aggregation Recurrent Multi-view Stereo Network](http://arxiv.org/abs/2108.03824v1)**|None|**[link](https://github.com/qt-zhu/aa-rmvsnet)**|None|Zizhuang Wei et.al.|
|**2021-08-05**|**[MFuseNet: Robust Depth Estimation with Learned Multiscopic Fusion](http://arxiv.org/abs/2108.02448v2)**|None|None|IEEE International Conference on Robotics and Automation (ICRA) +   IEEE Robotics and Automation Letters (RA-L). arXiv admin note: substantial   text overlap with arXiv:2001.08212|Weihao Yuan et.al.|
|**2021-07-28**|**[Improving Multi-View Stereo via Super-Resolution](http://arxiv.org/abs/2107.13261v1)**|None|None|None|Eugenio Lomurno et.al.|
|**2021-07-13**|**[Scalable Surface Reconstruction with Delaunay-Graph Neural Networks](http://arxiv.org/abs/2107.06130v3)**|Computer Graphics Forum 2021|**[link](https://github.com/raphaelsulzer/dgnn)**|The presentation of this work at SGP 2021 is available at   https://youtu.be/KIrCDGhS10o|Raphael Sulzer et.al.|
|**2021-07-09**|**[Prior-Guided Multi-View 3D Head Reconstruction](http://arxiv.org/abs/2107.04277v2)**|Received by IEEE Transactions on Multimedia (2021)|None|13 pages, 14 figures|Xueying Wang et.al.|
|**2021-07-05**|**[TransformerFusion: Monocular RGB Scene Reconstruction using Transformers](http://arxiv.org/abs/2107.02191v1)**|None|None|Video: https://youtu.be/LIpTKYfKSqw|Aljaž Božič et.al.|
|**2021-06-18**|**[Deep Learning for Multi-View Stereo via Plane Sweep: A Survey](http://arxiv.org/abs/2106.15328v2)**|None|None|None|Qingtian Zhu et.al.|
|**2021-04-30**|**[Deep Multi-View Stereo gone wild](http://arxiv.org/abs/2104.15119v2)**|None|**[link](https://github.com/fdarmon/wild_deep_mvs)**|Accepted to 3DV2021|François Darmon et.al.|
|**2021-04-29**|**[The Temporal Opportunist: Self-Supervised Multi-Frame Monocular Depth](http://arxiv.org/abs/2104.14540v2)**|None|**[link](https://github.com/nianticlabs/manydepth)**|CVPR 2021|Jamie Watson et.al.|
|**2021-04-27**|**[MVS2D: Efficient Multi-view Stereo via Attention-Driven 2D Convolutions](http://arxiv.org/abs/2104.13325v2)**|None|**[link](https://github.com/zhenpeiyang/MVS2D)**|Our code is released at https://github.com/zhenpeiyang/MVS2D|Zhenpei Yang et.al.|
|**2021-04-21**|**[Real-time dense 3D Reconstruction from monocular video data captured by low-cost UAVs](http://arxiv.org/abs/2104.10515v1)**|None|None|8 pages, 4 figures|Max Hermann et.al.|
|**2021-04-16**|**[Data-Driven 3D Reconstruction of Dressed Humans From Sparse Views](http://arxiv.org/abs/2104.08013v4)**|3DV 2021|**[link](https://gitlab.inria.fr/pzins/data-driven-3d-reconstruction-of-dressed-humans-from-sparse-views)**|Presented at 3DV 2021. Code is released at   https://gitlab.inria.fr/pzins/data-driven-3d-reconstruction-of-dressed-humans-from-sparse-views/|Pierre Zins et.al.|
|**2021-04-14**|**[Stereo Radiance Fields (SRF): Learning View Synthesis for Sparse Views of Novel Scenes](http://arxiv.org/abs/2104.06935v1)**|IEEE Conference on Computer Vision and Pattern Recognition (CVPR)   2021|None|IEEE Conference on Computer Vision and Pattern Recognition (CVPR)   2021|Julian Chibane et.al.|
|**2021-04-13**|**[PHI-MVS: Plane Hypothesis Inference Multi-view Stereo for Large-Scale Scene Reconstruction](http://arxiv.org/abs/2104.06165v1)**|None|None|None|Shang Sun et.al.|
|**2021-04-12**|**[Self-supervised Multi-view Stereo via Effective Co-Segmentation and Data-Augmentation](http://arxiv.org/abs/2104.05374v1)**|None|**[link](https://github.com/ToughStoneX/Self-Supervised-MVS)**|This paper is accepted by AAAI-21 with a Distinguished Paper Award|Hongbin Xu et.al.|
|**2021-04-07**|**[Self-supervised Learning of Depth Inference for Multi-view Stereo](http://arxiv.org/abs/2104.02972v1)**|None|**[link](https://github.com/JiayuYANG/Self-supervised-CVP-MVSNet)**|CVPR 2021|Jiayu Yang et.al.|
|**2021-03-29**|**[MVSNeRF: Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo](http://arxiv.org/abs/2103.15595v2)**|None|**[link](https://github.com/apchenstu/mvsnerf)**|Project Page: https://apchenstu.github.io/mvsnerf/   Code:https://github.com/apchenstu/mvsnerf|Anpei Chen et.al.|
|**2021-03-27**|**[Learning Efficient Photometric Feature Transform for Multi-view Stereo](http://arxiv.org/abs/2103.14794v1)**|None|None|None|Kaizhang Kang et.al.|
|**2021-03-26**|**[DDR-Net: Learning Multi-Stage Multi-View Stereo With Dynamic Depth Range](http://arxiv.org/abs/2103.14275v1)**|None|**[link](https://github.com/Tangshengku/DDR-Net)**|None|Puyuan Yi et.al.|
|**2020-12-18**|**[Boosting Monocular Depth Estimation with Lightweight 3D Point Fusion](http://arxiv.org/abs/2012.10296v2)**|None|None|10 pages, 9 figures|Lam Huynh et.al.|
|**2020-12-06**|**[Shape From Tracing: Towards Reconstructing 3D Object Geometry and SVBRDF Material from Images via Differentiable Path Tracing](http://arxiv.org/abs/2012.03939v1)**|None|**[link](https://github.com/brownvc/shapefromtracing)**|Will be published at 3DV 2020|Purvi Goel et.al.|
|**2020-12-03**|**[DeepVideoMVS: Multi-View Stereo on Video with Recurrent Spatio-Temporal Fusion](http://arxiv.org/abs/2012.02177v3)**|None|**[link](https://github.com/ardaduz/deep-video-mvs)**|CVPR 2021|Arda Düzçeker et.al.|
|**2020-12-02**|**[PatchmatchNet: Learned Multi-View Patchmatch Stereo](http://arxiv.org/abs/2012.01411v1)**|None|**[link](https://github.com/FangjinhuaWang/PatchmatchNet)**|None|Fangjinhua Wang et.al.|
|**2020-12-02**|**[A Photogrammetry-based Framework to Facilitate Image-based Modeling and Automatic Camera Tracking](http://arxiv.org/abs/2012.01044v1)**|In Proceedings of the 16th International Joint Conference on   Computer Vision, Imaging and Computer Graphics Theory and Applications -   Volume 1: GRAPP, 106-112, 2021|**[link](https://github.com/SBCV/Blender-Addon-Photogrammetry-Importer)**|None|Sebastian Bullinger et.al.|
|**2020-12-01**|**[Facetwise Mesh Refinement for Multi-View Stereo](http://arxiv.org/abs/2012.00564v1)**|None|None|Accepted as Oral ICPR2020|Andrea Romanoni et.al.|
|**2020-11-30**|**[How Good MVSNets Are at Depth Fusion](http://arxiv.org/abs/2011.14761v1)**|None|None|7 pages, 6 figures, 1 table. Accepted to ICMV 2020|Oleg Voynov et.al.|
|**2020-11-29**|**[RGBD-Net: Predicting color and depth images for novel views synthesis](http://arxiv.org/abs/2011.14398v2)**|None|None|19 pages, 15 figures. Code will be available at:   https://github.com/phongnhhn92/RGBDNet|Phong Nguyen-Ha et.al.|
|**2020-11-26**|**[Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks](http://arxiv.org/abs/2011.13118v3)**|None|**[link](https://github.com/xxlong0/ESTDepth)**|None|Xiaoxiao Long et.al.|
|**2020-11-25**|**[Attention Aware Cost Volume Pyramid Based Multi-view Stereo Network for 3D Reconstruction](http://arxiv.org/abs/2011.12722v1)**|None|**[link](https://github.com/ArthasMil/AACVP-MVSNet)**|None|Anzhu Yu et.al.|
|**2020-11-24**|**[MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera](http://arxiv.org/abs/2011.11814v3)**|Proceedings of the IEEE/CVF Conference on Computer Vision and   Pattern Recognition (CVPR), 2021, pp. 6112-6122|**[link](https://github.com/Brummi/MonoRec)**|CVPR 2021, Project page with video can be found under   https://vision.in.tum.de/research/monorec. 14 pages, 10 figures, 5 tables|Felix Wimbauer et.al.|
|**2020-11-20**|**[RidgeSfM: Structure from Motion via Robust Pairwise Matching Under Depth Uncertainty](http://arxiv.org/abs/2011.10359v1)**|None|**[link](https://github.com/facebookresearch/RidgeSfM)**|Presenting at 3DV 2020. Source code released at   https://github.com/facebookresearch/RidgeSfM|Benjamin Graham et.al.|
|**2020-11-18**|**[Dehazing Cost Volume for Deep Multi-view Stereo in Scattering Media with Airlight and Scattering Coefficient Estimation](http://arxiv.org/abs/2011.09114v2)**|None|**[link](https://github.com/yfujimura/DCV-release)**|14 pages, extended version of our ACCV2020 paper|Yuki Fujimura et.al.|
|**2020-11-14**|**[Stable View Synthesis](http://arxiv.org/abs/2011.07233v2)**|None|**[link](https://github.com/intel-isl/StableViewSynthesis)**|Published at CVPR 2021, https://youtu.be/gqgXIY09htI|Gernot Riegler et.al.|
|**2020-11-02**|**[SLAM in the Field: An Evaluation of Monocular Mapping and Localization on Challenging Dynamic Agricultural Environment](http://arxiv.org/abs/2011.01122v2)**|None|None|accepted to WACV 2021, acknowledgment added|Fangwen Shu et.al.|
|**2020-10-23**|**[BP-MVSNet: Belief-Propagation-Layers for Multi-View-Stereo](http://arxiv.org/abs/2010.12436v1)**|None|None|accepted at 3DV 2020|Christian Sormann et.al.|
|**2020-10-17**|**[MeshMVS: Multi-View Stereo Guided Mesh Reconstruction](http://arxiv.org/abs/2010.08682v3)**|None|None|None|Rakesh Shrestha et.al.|
|**2020-10-15**|**[Empty Cities: a Dynamic-Object-Invariant Space for Visual SLAM](http://arxiv.org/abs/2010.07646v1)**|None|**[link](https://github.com/bertabescos/EmptyCities_SLAM)**|None|Berta Bescos et.al.|
|**2020-10-02**|**[Image-based underwater 3D reconstruction for Cultural Heritage: from image collection to 3D. Critical steps and considerations](http://arxiv.org/abs/2010.00928v1)**|None|None|Pre-submission version of the manuscript|Dimitrios Skarlatos et.al.|
|**2020-09-28**|**[Learning to Adapt Multi-View Stereo by Self-Supervision](http://arxiv.org/abs/2009.13278v1)**|None|None|19 pages, including supplementary, accepted and presented in BMVC   2020|Arijit Mallick et.al.|
|**2020-09-07**|**[Improved Modeling of 3D Shapes with Multi-view Depth Maps](http://arxiv.org/abs/2009.03298v1)**|None|None|None|Kamal Gupta et.al.|
|**2020-08-19**|**[DeepHandMesh: A Weakly-supervised Deep Encoder-Decoder Framework for High-fidelity Hand Mesh Modeling](http://arxiv.org/abs/2008.08213v1)**|None|**[link](https://github.com/facebookresearch/DeepHandMesh)**|Published at ECCV 2020 (Oral)|Gyeongsik Moon et.al.|
|**2020-08-18**|**[Visibility-aware Multi-view Stereo Network](http://arxiv.org/abs/2008.07928v2)**|None|**[link](https://github.com/jzhangbs/Vis-MVSNet)**|Accepted to BMVC 2020|Jingyang Zhang et.al.|
|**2020-07-21**|**[Dense Hybrid Recurrent Multi-view Stereo Net with Dynamic Consistency Checking](http://arxiv.org/abs/2007.10872v1)**|ECCV2020|**[link](https://github.com/yhw-yhw/D2HC-RMVSNet)**|Accepted by ECCV2020 as Spotlight|Jianfeng Yan et.al.|
|**2020-07-17**|**[Polarimetric Multi-View Inverse Rendering](http://arxiv.org/abs/2007.08830v1)**|None|None|Paper accepted in ECCV 2020|Jinyu Zhao et.al.|
|**2020-07-15**|**[PVSNet: Pixelwise Visibility-Aware Multi-View Stereo Network](http://arxiv.org/abs/2007.07714v1)**|None|None|None|Qingshan Xu et.al.|
|**2020-07-13**|**[UnRectDepthNet: Self-Supervised Monocular Depth Estimation using a Generic Framework for Handling Common Camera Distortion Models](http://arxiv.org/abs/2007.06676v4)**|None|None|Minor fixes added after IROS 2020 Camera ready submission. IROS 2020   presentation video - https://www.youtube.com/watch?v=3Br2KSWZRrY|Varun Ravi Kumar et.al.|
|**2020-04-30**|**[M^3VSNet: Unsupervised Multi-metric Multi-view Stereo Network](http://arxiv.org/abs/2005.00363v2)**|None|**[link](https://github.com/whubaichuan/M3VSNet)**|The original top-level version is arXiv:2004.09722v2 but I upload the   similar version to arXiv:2005.00363 mistakenly, which is overlapped with   arXiv:2004.09722v2. So the submission is to make the two addresses keeping   the same version|Baichuan Huang et.al.|
|**2020-04-26**|**[Learning to Autofocus](http://arxiv.org/abs/2004.12260v3)**|None|None|CVPR 2020|Charles Herrmann et.al.|
|**2020-04-21**|**[M^3VSNet: Unsupervised Multi-metric Multi-view Stereo Network](http://arxiv.org/abs/2004.09722v2)**|None|**[link](https://github.com/whubaichuan/M3VSNet)**|Welcome to communicate with the author by the repo   https://github.com/whubaichuan/M3VSNet|Baichuan Huang et.al.|
|**2020-04-02**|**[Novel View Synthesis of Dynamic Scenes with Globally Coherent Depths from a Monocular Camera](http://arxiv.org/abs/2004.01294v1)**|None|None|This paper is accepted to CVPR 2020|Jae Shin Yoon et.al.|
|**2020-03-29**|**[Fast-MVSNet: Sparse-to-Dense Multi-View Stereo With Learned Propagation and Gauss-Newton Refinement](http://arxiv.org/abs/2003.13017v1)**|None|**[link](https://github.com/svip-lab/FastMVSNet)**|Accepted by CVPR2020|Zehao Yu et.al.|
|**2020-03-27**|**[Deep 3D Capture: Geometry and Reflectance from Sparse Multi-View Images](http://arxiv.org/abs/2003.12642v2)**|None|None|Accepted to CVPR 2020|Sai Bi et.al.|
|**2020-03-19**|**[DELTAS: Depth Estimation by Learning Triangulation And densification of Sparse points](http://arxiv.org/abs/2003.08933v2)**|None|**[link](https://github.com/magicleap/DELTAS)**|ECCV 2020|Ayan Sinha et.al.|
|**2020-03-02**|**[A-TVSNet: Aggregated Two-View Stereo Network for Multi-View Stereo Depth Estimation](http://arxiv.org/abs/2003.00711v1)**|None|**[link](https://github.com/daiszh/A-TVSNet)**|None|Sizhang Dai et.al.|
|**2020-03-02**|**[A Novel Recurrent Encoder-Decoder Structure for Large-Scale Multi-view Stereo Reconstruction from An Open Aerial Dataset](http://arxiv.org/abs/2003.00637v3)**|None|None|None|Jin Liu et.al.|
|**2020-02-21**|**[Leveraging Photogrammetric Mesh Models for Aerial-Ground Feature Point Matching Toward Integrated 3D Reconstruction](http://arxiv.org/abs/2002.09085v2)**|None|**[link](https://github.com/saedrna/RenderMatch)**|Accepted for publication in ISPRS Journal of Photogrammetry and   Remote Sensing|Qing Zhu et.al.|
|**2020-01-22**|**[Active Perception with A Monocular Camera for Multiscopic Vision](http://arxiv.org/abs/2001.08212v1)**|None|**[link](https://github.com/weihaosky/MFuseNet)**|8 pages|Weihao Yuan et.al.|
|**2020-01-21**|**[Depth Completion Using a View-constrained Deep Prior](http://arxiv.org/abs/2001.07791v3)**|None|None|None|Pallabi Ghosh et.al.|
|**2019-12-26**|**[Learning Inverse Depth Regression for Multi-View Stereo with Correlation Cost Volume](http://arxiv.org/abs/1912.11746v1)**|None|**[link](https://github.com/GhiXu/CIDER)**|Accepted by AAAI-2020|Qingshan Xu et.al.|
|**2019-12-26**|**[Planar Prior Assisted PatchMatch Multi-View Stereo](http://arxiv.org/abs/1912.11744v1)**|None|**[link](https://github.com/GhiXu/ACMP)**|Accepted by AAAI-2020|Qingshan Xu et.al.|
|**2019-12-13**|**[Cascade Cost Volume for High-Resolution Multi-View Stereo and Stereo Matching](http://arxiv.org/abs/1912.06378v3)**|None|**[link](https://github.com/alibaba/cascade-stereo)**|Accepted by CVPR2020 Oral|Xiaodong Gu et.al.|
|**2019-12-06**|**[Pyramid Multi-view Stereo Net with Self-adaptive View Aggregation](http://arxiv.org/abs/1912.03001v2)**|ECCV2020|**[link](https://github.com/yhw-yhw/PVAMVSNet)**|Accepted by ECCV2020 as a Poster|Hongwei Yi et.al.|
|**2019-12-03**|**[Joint Graph-based Depth Refinement and Normal Estimation](http://arxiv.org/abs/1912.01306v2)**|None|None|None|Mattia Rossi et.al.|
|**2019-12-01**|**[DeepC-MVS: Deep Confidence Prediction for Multi-View Stereo Reconstruction](http://arxiv.org/abs/1912.00439v3)**|None|None|changes in V3: re-worked confidence prediction scheme, re-organized   text, updated experiments; changes in V2: a reference was updated|Andreas Kuhn et.al.|
|**2019-11-27**|**[Deep Stereo using Adaptive Thin Volume Representation with Uncertainty Awareness](http://arxiv.org/abs/1911.12012v2)**|None|None|Accepted to CVPR 2020 (Oral)|Shuo Cheng et.al.|
|**2019-11-24**|**[Normal Assisted Stereo Depth Estimation](http://arxiv.org/abs/1911.10444v3)**|None|**[link](https://github.com/udaykusupati/Normal-Assisted-Stereo)**|None|Uday Kusupati et.al.|
|**2019-11-22**|**[BlendedMVS: A Large-scale Dataset for Generalized Multi-view Stereo Networks](http://arxiv.org/abs/1911.10127v2)**|None|**[link](https://github.com/YoYo000/BlendedMVS)**|Accepted to CVPR2020|Yao Yao et.al.|
|**2019-10-18**|**[Eye in the Sky: Drone-Based Object Tracking and 3D Localization](http://arxiv.org/abs/1910.08259v1)**|None|None|Accepted to ACMMM2019|Haotian Zhang et.al.|
|**2019-10-07**|**[Leveraging Vision Reconstruction Pipelines for Satellite Imagery](http://arxiv.org/abs/1910.02989v2)**|None|None|Project Page: https://kai-46.github.io/VisSat/|Kai Zhang et.al.|
|**2019-09-06**|**[Self-supervised Dense 3D Reconstruction from Monocular Endoscopic Video](http://arxiv.org/abs/1909.03101v1)**|None|None|None|Xingtong Liu et.al.|
|**2019-08-30**|**[MVS^2: Deep Unsupervised Multi-view Stereo with Multi-View Symmetry](http://arxiv.org/abs/1908.11526v1)**|None|None|Accepted by International Conference on 3D Vision (3DV 2019) as ORAL   presentation|Yuchao Dai et.al.|
|**2019-08-23**|**[Multi-Spectral Visual Odometry without Explicit Stereo Matching](http://arxiv.org/abs/1908.08814v1)**|None|None|None|Weichen Dai et.al.|
|**2019-08-17**|**[OmniMVS: End-to-End Learning for Omnidirectional Stereo Matching](http://arxiv.org/abs/1908.06257v1)**|None|**[link](https://github.com/hyu-cvlab/omnimvs-pytorch)**|Accepted by ICCV 2019|Changhee Won et.al.|
|**2019-08-12**|**[Point-Based Multi-View Stereo Network](http://arxiv.org/abs/1908.04422v1)**|None|**[link](https://github.com/callmeray/PointMVSNet)**|Accepted as ICCV 2019 oral presentation|Rui Chen et.al.|
|**2019-08-04**|**[Adversarial View-Consistent Learning for Monocular Depth Estimation](http://arxiv.org/abs/1908.01301v1)**|None|None|BMVC 2019 Spotlight|Yixuan Liu et.al.|
|**2019-06-20**|**[3D Instance Segmentation via Multi-Task Metric Learning](http://arxiv.org/abs/1906.08650v2)**|None|None|None|Jean Lahoud et.al.|
|**2019-06-03**|**[Y-GAN: A Generative Adversarial Network for Depthmap Estimation from Multi-camera Stereo Images](http://arxiv.org/abs/1906.00932v1)**|None|None|Accepted for Presentation at the ICML 2019 LatinX in AI Research   Workshop|Miguel Alonso Jr et.al.|
|**2019-05-21**|**[Mesh-based Camera Pairs Selection and Occlusion-Aware Masking for Mesh Refinement](http://arxiv.org/abs/1905.08502v1)**|None|None|Accepted for publication in Pattern Recognition Letters|Andrea Romanoni et.al.|
|**2019-04-25**|**[Learning the Depths of Moving People by Watching Frozen People](http://arxiv.org/abs/1904.11111v1)**|None|None|CVPR 2019 (Oral)|Zhengqi Li et.al.|
|**2019-04-17**|**[Render4Completion: Synthesizing Multi-View Depth Maps for 3D Shape Completion](http://arxiv.org/abs/1904.08366v4)**|None|None|ICCV 2019 workshop on Geometry meets Deep Learning|Tao Hu et.al.|
|**2019-04-17**|**[Multi-Scale Geometric Consistency Guided Multi-View Stereo](http://arxiv.org/abs/1904.08103v1)**|None|None|Accepted by CVPR2019|Qingshan Xu et.al.|
|**2019-03-26**|**[TAPA-MVS: Textureless-Aware PAtchMatch Multi-View Stereo](http://arxiv.org/abs/1903.10929v1)**|None|None|None|Andrea Romanoni et.al.|
|**2019-03-12**|**[Image Classification base on PCA of Multi-view Deep Representation](http://arxiv.org/abs/1903.04814v1)**|None|None|None|Yaoqi Sun et.al.|
|**2019-02-27**|**[Shallow Water Bathymetry Mapping from UAV Imagery based on Machine Learning](http://arxiv.org/abs/1902.10733v3)**|Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci., XLII-2/W10,   9-16, 2019|None|8 pages, 9 figures|Panagiotis Agrafiotis et.al.|
|**2019-02-27**|**[Recurrent MVSNet for High-resolution Multi-view Stereo Depth Inference](http://arxiv.org/abs/1902.10556v1)**|None|**[link](https://github.com/YoYo000/MVSNet)**|Accepted by CVPR2019|Yao Yao et.al.|
|**2019-02-20**|**[Dense Depth Estimation in Monocular Endoscopy with Self-supervised Learning Methods](http://arxiv.org/abs/1902.07766v2)**|None|**[link](https://github.com/lppllppl920/EndoscopyDepthEstimation-Pytorch)**|Accepted to IEEE Transactions on Medical Imaging|Xingtong Liu et.al.|
|**2019-01-12**|**[NRMVS: Non-Rigid Multi-View Stereo](http://arxiv.org/abs/1901.03910v1)**|None|None|None|Matthias Innmann et.al.|
|**2018-12-21**|**[Wireless Software Synchronization of Multiple Distributed Cameras](http://arxiv.org/abs/1812.09366v2)**|None|None|Main: 9 pages, 10 figures. Supplemental: 3 pages, 5 figures|Sameer Ansari et.al.|
|**2018-11-26**|**[IGNOR: Image-guided Neural Object Rendering](http://arxiv.org/abs/1811.10720v2)**|None|None|Video: https://youtu.be/s79HG9yn7QM|Justus Thies et.al.|
|**2018-11-05**|**[A Differential Volumetric Approach to Multi-View Photometric Stereo](http://arxiv.org/abs/1811.01984v2)**|None|None|None|Fotios Logothetis et.al.|
|**2018-09-28**|**[Extrinsic camera calibration method and its performance evaluation](http://arxiv.org/abs/1809.11073v1)**|None|None|arXiv admin note: text overlap with arXiv:1809.11066|Jacek Komorowski et.al.|
|**2018-09-28**|**[Face Recognition Based on Sequence of Images](http://arxiv.org/abs/1809.11069v1)**|None|None|None|Jacek Komorowski et.al.|
|**2018-09-24**|**[Towards Automated Post-Earthquake Inspections with Deep Learning-based Condition-Aware Models](http://arxiv.org/abs/1809.09195v1)**|None|None|None|Vedhus Hoskere et.al.|
|**2018-07-16**|**[Learning and Matching Multi-View Descriptors for Registration of Point Clouds](http://arxiv.org/abs/1807.05653v2)**|None|None|None|Lei Zhou et.al.|
|**2018-06-29**|**[Action Recognition for Depth Video using Multi-view Dynamic Images](http://arxiv.org/abs/1806.11269v3)**|None|**[link](https://github.com/3huo/MVDI)**|accepted by Information Sciences|Yang Xiao et.al.|
|**2018-06-25**|**[Self-supervised Learning for Dense Depth Estimation in Monocular Endoscopy](http://arxiv.org/abs/1806.09521v2)**|None|None|11 pages, 5 figures|Xingtong Liu et.al.|
|**2018-05-17**|**[Recurrent Neural Network for Learning DenseDepth and Ego-Motion from Video](http://arxiv.org/abs/1805.06558v1)**|None|None|None|Rui Wang et.al.|
|**2018-04-23**|**[Deep cross-domain building extraction for selective depth estimation from oblique aerial imagery](http://arxiv.org/abs/1804.08302v3)**|ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci., IV-1,   125-132, 2018|None|Accepted in the ISPRS Annals of the Photogrammetry, Remote Sensing   and Spatial Information Science|Boitumelo Ruf et.al.|
|**2018-04-14**|**[Physics-driven Fire Modeling from Multi-view Images](http://arxiv.org/abs/1804.05261v1)**|None|**[link](https://github.com/Garoe/bath-fire-shader)**|None|Garoe Dorta et.al.|
|**2018-04-02**|**[DeepMVS: Learning Multi-view Stereopsis](http://arxiv.org/abs/1804.00650v1)**|None|**[link](https://github.com/phuang17/DeepMVS)**|CVPR 2018. Project page: https://phuang17.github.io/DeepMVS/ Code:   https://github.com/phuang17/DeepMVS|Po-Han Huang et.al.|
|**2018-04-02**|**[MegaDepth: Learning Single-View Depth Prediction from Internet Photos](http://arxiv.org/abs/1804.00607v4)**|None|None|updated paper for 'MegaDepth: Learning Single-View Depth Prediction   from Internet Photos', CVPR, 2018|Zhengqi Li et.al.|
|**2018-03-22**|**[Prioritized Multi-View Stereo Depth Map Generation Using Confidence Prediction](http://arxiv.org/abs/1803.08323v1)**|None|None|This paper was accepted to ISPRS Journal of Photogrammetry and Remote   Sensing   (https://www.journals.elsevier.com/isprs-journal-of-photogrammetry-and-remote-sensing)   on March 21, 2018. The official version will be made available on   ScienceDirect (https://www.sciencedirect.com)|Christian Mostegel et.al.|
|**2018-03-21**|**[Robust Depth Estimation from Auto Bracketed Images](http://arxiv.org/abs/1803.07702v1)**|None|None|To appear in CVPR 2018. Total 9 pages|Sunghoon Im et.al.|
|**2018-01-17**|**[Multi-View Stereo 3D Edge Reconstruction](http://arxiv.org/abs/1801.05606v1)**|None|**[link](https://github.com/abignoli/EdgeGraph3D)**|Accepted for WACV 2018|Andrea Bignoli et.al.|
|**2018-01-04**|**[A Large Dataset for Improving Patch Matching](http://arxiv.org/abs/1801.01466v3)**|None|**[link](https://github.com/rmitra/PS-Dataset)**|None|Rahul Mitra et.al.|
|**2017-11-28**|**[Super-Resolution for Overhead Imagery Using DenseNets and Adversarial Learning](http://arxiv.org/abs/1711.10312v1)**|None|None|9 pages, 9 figures, WACV 2018 submission|Marc Bosch et.al.|
|**2017-09-22**|**[High-Resolution Shape Completion Using Deep Neural Networks for Global Structure and Local Geometry Inference](http://arxiv.org/abs/1709.07599v1)**|None|None|8 pages paper, 11 pages supplementary material, ICCV spotlight paper|Xiaoguang Han et.al.|
|**2017-09-18**|**[Joint Estimation of Camera Pose, Depth, Deblurring, and Super-Resolution from a Blurred Image Sequence](http://arxiv.org/abs/1709.05745v1)**|None|None|accepted to ICCV 2017|Haesol Park et.al.|
|**2017-08-25**|**[Stereo DSO: Large-Scale Direct Sparse Visual Odometry with Stereo Cameras](http://arxiv.org/abs/1708.07878v1)**|None|None|ICCV 2017|Rui Wang et.al.|
|**2017-05-25**|**[Plan3D: Viewpoint and Trajectory Optimization for Aerial Multi-View Stereo Reconstruction](http://arxiv.org/abs/1705.09314v2)**|None|None|31 pages, 12 figures, 9 tables|Benjamin Hepp et.al.|
|**2017-05-02**|**[Active Image-based Modeling with a Toy Drone](http://arxiv.org/abs/1705.01010v3)**|None|None|To be published on International Conference on Robotics and   Automation 2018, Brisbane, Australia. Project Page:   https://huangrui815.github.io/active-image-based-modeling/ The author's   personal page: http://www.sfu.ca/~rha55/|Rui Huang et.al.|
|**2017-05-02**|**[Scalable Surface Reconstruction from Point Clouds with Extreme Scale and Density Diversity](http://arxiv.org/abs/1705.00949v1)**|None|None|This paper was accepted to the IEEE Conference on Computer Vision and   Pattern Recognition (CVPR), 2017. The copyright was transfered to IEEE   (ieee.org). The official version of the paper will be made available on IEEE   Xplore (R) (ieeexplore.ieee.org). This version of the paper also contains the   supplementary material, which will not appear IEEE Xplore (R)|Christian Mostegel et.al.|
|**2017-01-24**|**[Improved Descriptors for Patch Matching and Reconstruction](http://arxiv.org/abs/1701.06854v4)**|None|None|9 pages, ICCV Workshop on Compact and Efficient Feature   Representation and Learning (CEFRL), 2017|Rahul Mitra et.al.|
|**2016-11-22**|**[Single-View and Multi-View Depth Fusion](http://arxiv.org/abs/1611.07245v2)**|None|None|Accepted for publication in IEEE Robotics and Automation Letters|José M. Fácil et.al.|
|**2016-10-14**|**[Recurrent 3D Attentional Networks for End-to-End Active Object Recognition](http://arxiv.org/abs/1610.04308v4)**|None|None|None|Min Liu et.al.|
|**2016-09-21**|**[Production-Level Facial Performance Capture Using Deep Convolutional Neural Networks](http://arxiv.org/abs/1609.06536v2)**|None|None|Final SCA 2017 version|Samuli Laine et.al.|
|**2016-09-05**|**[Efficient Volumetric Fusion of Airborne and Street-Side Data for Urban Reconstruction](http://arxiv.org/abs/1609.01345v1)**|None|None|To appear in ICPR 2016|András Bódis-Szomorú et.al.|
|**2016-05-06**|**[UAV-based Autonomous Image Acquisition with Multi-View Stereo Quality Assurance by Confidence Prediction](http://arxiv.org/abs/1605.01923v1)**|None|None|This paper was accepted to the 7th International Workshop on Computer   Vision in Vehicle Technology (CVVT 2016) and will appear in IEEE Conference   on Computer Vision and Pattern Recognition Workshops (CVPRW), 2016. The   copyright was transferred to IEEE (ieee.org). The paper will be available on   IEEE Xplore(ieeexplore.ieee.org). This version of the paper also contains the   supplementary material|Christian Mostegel et.al.|
|**2016-04-21**|**[Automatic 3D Reconstruction of Manifold Meshes via Delaunay Triangulation and Mesh Sweeping](http://arxiv.org/abs/1604.06258v1)**|None|None|in IEEE Winter Conference on Applications of Computer Vision (WACV)   2016|Andrea Romanoni et.al.|
|**2016-04-11**|**[Semantic 3D Reconstruction with Continuous Regularization and Ray Potentials Using a Visibility Consistency Constraint](http://arxiv.org/abs/1604.02885v3)**|None|**[link](https://github.com/nsavinov/ray_potentials)**|Accepted as a spotlight oral paper by CVPR 2016. Code at   https://github.com/nsavinov/ray_potentials/|Nikolay Savinov et.al.|
|**2015-05-03**|**[Detail-preserving and Content-aware Variational Multi-view Stereo Reconstruction](http://arxiv.org/abs/1505.00389v1)**|None|None|14 pages,16 figures. Submitted to IEEE Transaction on image   processing|Zhaoxin Li et.al.|
|**2015-03-16**|**[PiMPeR: Piecewise Dense 3D Reconstruction from Multi-View and Multi-Illumination Images](http://arxiv.org/abs/1503.04598v2)**|None|None|None|Reza Sabzevari et.al.|

## Depth Estimation

|Published Date|Title|Journal|Code|Comments|Authors
|---|---|---|---|---|---|
|**2024-06-25**|**[Depth-Guided Semi-Supervised Instance Segmentation](http://arxiv.org/abs/2406.17413v1)**|None|None|12 pages, 6 figures, 4 tables|Xin Chen et.al.|
|**2024-06-20**|**[Uncertainty and Self-Supervision in Single-View Depth](http://arxiv.org/abs/2406.14226v1)**|None|None|Doctoral thesis|Javier Rodriguez-Puigvert et.al.|
|**2024-06-19**|**[WaterMono: Teacher-Guided Anomaly Masking and Enhancement Boosting for Robust Underwater Self-Supervised Monocular Depth Estimation](http://arxiv.org/abs/2406.13344v1)**|None|**[link](https://github.com/oucvisiongroup/watermono)**|None|Yilin Ding et.al.|
|**2024-06-18**|**[Depth Anywhere: Enhancing 360 Monocular Depth Estimation via Perspective Distillation and Unlabeled Data Augmentation](http://arxiv.org/abs/2406.12849v1)**|None|None|Project page: https://albert100121.github.io/Depth-Anywhere/|Ning-Hsu Wang et.al.|
|**2024-06-18**|**[GeoBench: Benchmarking and Analyzing Monocular Geometry Estimation Models](http://arxiv.org/abs/2406.12671v2)**|None|**[link](https://github.com/aim-uofa/geobench)**|Code and Benchmark are available at:   https://github.com/aim-uofa/GeoBench|Yongtao Ge et.al.|
|**2024-06-17**|**[DistillNeRF: Perceiving 3D Scenes from Single-Glance Images by Distilling Neural Fields and Foundation Model Features](http://arxiv.org/abs/2406.12095v1)**|None|None|None|Letian Wang et.al.|
|**2024-06-17**|**[MEDeA: Multi-view Efficient Depth Adjustment](http://arxiv.org/abs/2406.12048v1)**|None|None|None|Mikhail Artemyev et.al.|
|**2024-06-16**|**[3D Gaze Tracking for Studying Collaborative Interactions in Mixed-Reality Environments](http://arxiv.org/abs/2406.11003v1)**|None|None|9 pages, 8 figures, conference, submitted to ICMI 2024|Eduardo Davalos et.al.|
|**2024-06-15**|**[GenMM: Geometrically and Temporally Consistent Multimodal Data Generation for Video and LiDAR](http://arxiv.org/abs/2406.10722v1)**|None|None|None|Bharat Singh et.al.|
|**2024-06-14**|**[The BabyView dataset: High-resolution egocentric videos of infants' and young children's everyday experiences](http://arxiv.org/abs/2406.10447v1)**|None|None|9 pages, 2 figures, 4 tables and SI. Submitted to NeurIPS Datasets   and Benchmarks|Bria Long et.al.|
|**2024-06-14**|**[D-NPC: Dynamic Neural Point Clouds for Non-Rigid View Synthesis from Monocular Video](http://arxiv.org/abs/2406.10078v1)**|None|None|16 pages, 5 figures, 10 tables. Project page:   https://moritzkappel.github.io/projects/dnpc|Moritz Kappel et.al.|
|**2024-06-14**|**[DurLAR: A High-fidelity 128-channel LiDAR Dataset with Panoramic Ambient and Reflectivity Imagery for Multi-modal Autonomous Driving Applications](http://arxiv.org/abs/2406.10068v1)**|Proc. Int. Conf. on 3D Vision (3DV 2021)|**[link](https://github.com/l1997i/DurLAR)**|Accepted by 3DV 2021; 13 pages, 14 figures; Dataset at   https://github.com/l1997i/durlar|Li Li et.al.|
|**2024-06-14**|**[Unsupervised Monocular Depth Estimation Based on Hierarchical Feature-Guided Diffusion](http://arxiv.org/abs/2406.09782v1)**|None|None|None|Runze Liu et.al.|
|**2024-06-13**|**[Depth Anything V2](http://arxiv.org/abs/2406.09414v1)**|None|**[link](https://github.com/DepthAnything/Depth-Anything-V2)**|Project page: https://depth-anything-v2.github.io|Lihe Yang et.al.|
|**2024-06-13**|**[WonderWorld: Interactive 3D Scene Generation from a Single Image](http://arxiv.org/abs/2406.09394v2)**|None|None|Project website: https://WonderWorld-2024.github.io/|Hong-Xing Yu et.al.|
|**2024-06-13**|**[Scale-Invariant Monocular Depth Estimation via SSI Depth](http://arxiv.org/abs/2406.09374v1)**|None|None|To appear in Proc. SIGGRAPH, 2024. Project webpage:   https://yaksoy.github.io/sidepth/|S. Mahdi H. Miangoleh et.al.|
|**2024-06-13**|**[Multiple Prior Representation Learning for Self-Supervised Monocular Depth Estimation via Hybrid Transformer](http://arxiv.org/abs/2406.08928v1)**|None|**[link](https://github.com/mvme-hbut/mprlnet)**|28 pages, 12 figures|Guodong Sun et.al.|
|**2024-06-13**|**[ToSA: Token Selective Attention for Efficient Vision Transformers](http://arxiv.org/abs/2406.08816v1)**|None|None|Accepted at CVPRW 2024|Manish Kumar Singh et.al.|
|**2024-06-11**|**[Back to the Color: Learning Depth to Specific Color Transformation for Unsupervised Depth Estimation](http://arxiv.org/abs/2406.07741v2)**|None|**[link](https://github.com/BlueEg/back2color)**|None|Yufan Zhu et.al.|
|**2024-06-11**|**[PLT-D3: A High-fidelity Dynamic Driving Simulation Dataset for Stereo Depth and Scene Flow](http://arxiv.org/abs/2406.07667v1)**|None|None|None|Joshua Tokarsky et.al.|
|**2024-06-11**|**[RS-DFM: A Remote Sensing Distributed Foundation Model for Diverse Downstream Tasks](http://arxiv.org/abs/2406.07032v1)**|None|None|None|Zhechao Wang et.al.|
|**2024-06-10**|**[PatchRefiner: Leveraging Synthetic Data for Real-Domain High-Resolution Monocular Metric Depth Estimation](http://arxiv.org/abs/2406.06679v1)**|None|None|None|Zhenyu Li et.al.|
|**2024-06-09**|**[Self-supervised Adversarial Training of Monocular Depth Estimation against Physical-World Attacks](http://arxiv.org/abs/2406.05857v1)**|None|**[link](https://github.com/Bob-cheng/DepthModelHardening)**|Accepted in TPAMI'24. Extended from our ICLR'23 publication   (arXiv:2301.13487). arXiv admin note: substantial text overlap with   arXiv:2301.13487|Zhiyuan Cheng et.al.|
|**2024-06-09**|**[RefGaussian: Disentangling Reflections from 3D Gaussian Splatting for Realistic Rendering](http://arxiv.org/abs/2406.05852v1)**|None|None|None|Rui Zhang et.al.|
|**2024-06-07**|**[Normal-guided Detail-Preserving Neural Implicit Functions for High-Fidelity 3D Surface Reconstruction](http://arxiv.org/abs/2406.04861v1)**|None|None|Original version. Project page with images and code:   https://sn-nir.github.io/|Aarya Patel et.al.|
|**2024-06-07**|**[UVCPNet: A UAV-Vehicle Collaborative Perception Network for 3D Object Detection](http://arxiv.org/abs/2406.04647v1)**|None|None|None|Yuchao Wang et.al.|
|**2024-06-06**|**[MambaDepth: Enhancing Long-range Dependency for Self-Supervised Fine-Structured Monocular Depth Estimation](http://arxiv.org/abs/2406.04532v1)**|None|None|None|Ionuţ Grigore et.al.|
|**2024-06-06**|**[Flash3D: Feed-Forward Generalisable 3D Scene Reconstruction from a Single Image](http://arxiv.org/abs/2406.04343v1)**|None|None|Project page: https://www.robots.ox.ac.uk/~vgg/research/flash3d/|Stanislaw Szymanowicz et.al.|
|**2024-06-06**|**[Neural Surface Reconstruction from Sparse Views Using Epipolar Geometry](http://arxiv.org/abs/2406.04301v1)**|None|None|None|Kaichen Zhou et.al.|
|**2024-06-04**|**[VHS: High-Resolution Iterative Stereo Matching with Visual Hull Priors](http://arxiv.org/abs/2406.02552v1)**|None|None|None|Markus Plack et.al.|
|**2024-06-03**|**[L-MAGIC: Language Model Assisted Generation of Images with Coherence](http://arxiv.org/abs/2406.01843v1)**|None|**[link](https://github.com/intellabs/mmpano)**|accepted to CVPR 2024|Zhipeng Cai et.al.|
|**2024-06-03**|**[Learning Temporally Consistent Video Depth from Video Diffusion Priors](http://arxiv.org/abs/2406.01493v2)**|None|None|None|Jiahao Shao et.al.|
|**2024-06-03**|**[Self-Supervised Geometry-Guided Initialization for Robust Monocular Visual Odometry](http://arxiv.org/abs/2406.00929v1)**|None|None|8 pages. 5 figures. This work has been submitted to the IEEE for   possible publication. Copyright may be transferred without notice, after   which this version may no longer be accessible|Takayuki Kanai et.al.|
|**2024-06-01**|**[MoDGS: Dynamic Gaussian Splatting from Causually-captured Monocular Videos](http://arxiv.org/abs/2406.00434v1)**|None|None|None|Qingming Liu et.al.|
|**2024-05-30**|**[Uncertainty-guided Optimal Transport in Depth Supervised Sparse-View 3D Gaussian](http://arxiv.org/abs/2405.19657v1)**|None|None|10pages|Wei Sun et.al.|
|**2024-05-27**|**[Consistency Regularisation for Unsupervised Domain Adaptation in Monocular Depth Estimation](http://arxiv.org/abs/2405.17704v1)**|None|None|Accepted to Conference on Lifelong Learning Agents (CoLLAs) 2024|Amir El-Ghoussani et.al.|
|**2024-05-27**|**[Benchmarking and Improving Bird's Eye View Perception Robustness in Autonomous Driving](http://arxiv.org/abs/2405.17426v1)**|None|**[link](https://github.com/Daniel-xsy/RoboBEV)**|Preprint; 17 pages, 13 figures, 11 tables; Code at this https URL:   https://github.com/Daniel-xsy/RoboBEV|Shaoyuan Xie et.al.|
|**2024-05-27**|**[All-day Depth Completion](http://arxiv.org/abs/2405.17315v1)**|None|None|8 pages, 4 figures|Vadim Ezhov et.al.|
|**2024-05-27**|**[GenWarp: Single Image to Novel Views with Semantic-Preserving Generative Warping](http://arxiv.org/abs/2405.17251v1)**|None|None|Project page: https://GenWarp-NVS.github.io|Junyoung Seo et.al.|
|**2024-05-27**|**[SDL-MVS: View Space and Depth Deformable Learning Paradigm for Multi-View Stereo Reconstruction in Remote Sensing](http://arxiv.org/abs/2405.17140v1)**|None|None|None|Yong-Qiang Mao et.al.|
|**2024-05-27**|**[DINO-SD: Champion Solution for ICRA 2024 RoboDepth Challenge](http://arxiv.org/abs/2405.17102v1)**|None|None|Outstanding Champion in the RoboDepth Challenge (ICRA24)   https://robodrive-24.github.io/|Yifan Mao et.al.|
|**2024-05-27**|**[Evaluation of Multi-task Uncertainties in Joint Semantic Segmentation and Monocular Depth Estimation](http://arxiv.org/abs/2405.17097v1)**|None|None|Submitted to Forum Bildverarbeitung 2024. arXiv admin note:   substantial text overlap with arXiv:2402.10580|Steven Landgraf et.al.|
|**2024-05-27**|**[DCPI-Depth: Explicitly Infusing Dense Correspondence Prior to Unsupervised Monocular Depth Estimation](http://arxiv.org/abs/2405.16960v1)**|None|None|13 pages, 7 figures|Mengtan Zhang et.al.|
|**2024-05-27**|**[ContrastAlign: Toward Robust BEV Feature Alignment via Contrastive Learning for Multi-Modal 3D Object Detection](http://arxiv.org/abs/2405.16873v2)**|None|None|None|Ziying Song et.al.|
|**2024-05-27**|**[Estimating Depth of Monocular Panoramic Image with Teacher-Student Model Fusing Equirectangular and Spherical Representations](http://arxiv.org/abs/2405.16858v1)**|None|None|None|Jingguo Liu et.al.|
|**2024-05-26**|**[Splat-SLAM: Globally Optimized RGB-only SLAM with 3D Gaussians](http://arxiv.org/abs/2405.16544v1)**|None|**[link](https://github.com/eriksandstroem/splat-slam)**|21 pages|Erik Sandström et.al.|
|**2024-05-24**|**[Transparent Object Depth Completion](http://arxiv.org/abs/2405.15299v1)**|None|None|None|Yifan Zhou et.al.|
|**2024-05-24**|**[MonoDETRNext: Next-generation Accurate and Efficient Monocular 3D Object Detection Method](http://arxiv.org/abs/2405.15176v1)**|None|None|None|Pan Liao et.al.|
|**2024-05-23**|**[EvGGS: A Collaborative Learning Framework for Event-based Generalizable Gaussian Splatting](http://arxiv.org/abs/2405.14959v2)**|None|**[link](https://github.com/mercerai/evggs)**|None|Jiaxu Wang et.al.|
|**2024-05-23**|**[Ghost-Stereo: GhostNet-based Cost Volume Enhancement and Aggregation for Stereo Matching Networks](http://arxiv.org/abs/2405.14520v1)**|None|None|None|Xingguang Jiang et.al.|
|**2024-05-23**|**[Enhanced Object Tracking by Self-Supervised Auxiliary Depth Estimation Learning](http://arxiv.org/abs/2405.14195v1)**|None|None|None|Zhenyu Wei et.al.|
|**2024-05-21**|**[Cross-spectral Gated-RGB Stereo Depth Estimation](http://arxiv.org/abs/2405.12759v1)**|None|None|None|Samuel Brucker et.al.|
|**2024-05-20**|**[Depth Reconstruction with Neural Signed Distance Fields in Structured Light Systems](http://arxiv.org/abs/2405.12006v1)**|None|None|10 pages, 8 figures, accepted by 3DV 2024|Rukun Qiao et.al.|
|**2024-05-20**|**[Depth Prompting for Sensor-Agnostic Depth Estimation](http://arxiv.org/abs/2405.11867v1)**|None|None|Accepted at CVPR 2024|Jin-Hwi Park et.al.|
|**2024-05-19**|**[CRF360D: Monocular 360 Depth Estimation via Spherical Fully-Connected CRFs](http://arxiv.org/abs/2405.11564v1)**|None|None|None|Zidong Cao et.al.|
|**2024-05-18**|**[Dusk Till Dawn: Self-supervised Nighttime Stereo Depth Estimation using Visual Foundation Models](http://arxiv.org/abs/2405.11158v1)**|None|**[link](https://github.com/madhubabuv/dtd)**|The paper is published at ICRA 2024|Madhu Vankadari et.al.|
|**2024-05-17**|**[Accurate Training Data for Occupancy Map Prediction in Automated Driving Using Evidence Theory](http://arxiv.org/abs/2405.10575v1)**|None|**[link](https://github.com/boschresearch/evidential-occupancy)**|None|Jonas Kälble et.al.|
|**2024-05-16**|**[Towards Task-Compatible Compressible Representations](http://arxiv.org/abs/2405.10244v2)**|None|**[link](https://github.com/adeandrade/research)**|To be published in ICME Workshops 2024|Anderson de Andrade et.al.|
|**2024-05-16**|**[KPNDepth: Depth Estimation of Lane Images under Complex Rainy Environment](http://arxiv.org/abs/2405.09964v1)**|None|None|None|Zhengxu Shi et.al.|
|**2024-05-14**|**[CLIP with Quality Captions: A Strong Pretraining for Vision Tasks](http://arxiv.org/abs/2405.08911v1)**|None|None|None|Pavan Kumar Anasosalu Vasu et.al.|
|**2024-05-14**|**[The RoboDrive Challenge: Drive Anytime Anywhere in Any Condition](http://arxiv.org/abs/2405.08816v2)**|None|None|ICRA 2024; 32 pages, 24 figures, 5 tables; Code at   https://robodrive-24.github.io/|Lingdong Kong et.al.|
|**2024-05-13**|**[SceneFactory: A Workflow-centric and Unified Framework for Incremental Scene Modeling](http://arxiv.org/abs/2405.07847v1)**|None|None|None|Yijun Yuan et.al.|
|**2024-05-10**|**[Ensuring UAV Safety: A Vision-only and Real-time Framework for Collision Avoidance Through Object Detection, Tracking, and Distance Estimation](http://arxiv.org/abs/2405.06749v2)**|None|None|accepted at ICUAS 2024|Vasileios Karampinis et.al.|
|**2024-05-10**|**[MGS-SLAM: Monocular Sparse Tracking and Gaussian Mapping with Depth Smooth Regularization](http://arxiv.org/abs/2405.06241v1)**|None|None|This work has been submitted to the IEEE for possible publication.   Copyright may be transferred without notice, after which this version may no   longer be accessible|Pengcheng Zhu et.al.|
|**2024-05-06**|**[A Construct-Optimize Approach to Sparse View Synthesis without Camera Pose](http://arxiv.org/abs/2405.03659v2)**|None|None|None|Kaiwen Jiang et.al.|
|**2024-05-03**|**[M${^2}$Depth: Self-supervised Two-Frame Multi-camera Metric Depth Estimation](http://arxiv.org/abs/2405.02004v1)**|None|None|None|Yingshuang Zou et.al.|
|**2024-03-24**|**[Configurable Learned Holography](http://arxiv.org/abs/2405.01558v2)**|None|None|14 pages, 5 figures|Yicheng Zhan et.al.|
|**2024-05-02**|**[Domain-Transferred Synthetic Data Generation for Improving Monocular Depth Estimation](http://arxiv.org/abs/2405.01113v1)**|None|None|None|Seungyeop Lee et.al.|
|**2024-05-01**|**[Depth Priors in Removal Neural Radiance Fields](http://arxiv.org/abs/2405.00630v2)**|None|None|16 pages|Zhihao Guo et.al.|
|**2024-04-30**|**[Invisible Stitch: Generating Smooth 3D Scenes with Depth Inpainting](http://arxiv.org/abs/2404.19758v1)**|None|None|Project page: https://research.paulengstler.com/invisible-stitch/|Paul Engstler et.al.|
|**2024-04-30**|**[Masked Spatial Propagation Network for Sparsity-Adaptive Depth Refinement](http://arxiv.org/abs/2404.19294v1)**|None|**[link](https://github.com/jyjunmcl/mspn_sdr)**|None|Jinyoung Jun et.al.|
|**2024-04-29**|**[Simple-RF: Regularizing Sparse Input Radiance Fields with Simpler Solutions](http://arxiv.org/abs/2404.19015v3)**|None|None|The source code for our model can be found on our project page:   https://nagabhushansn95.github.io/publications/2024/Simple-RF.html. arXiv   admin note: substantial text overlap with arXiv:2309.03955|Nagabhushan Somraj et.al.|
|**2024-04-27**|**[Underwater Variable Zoom: Depth-Guided Perception Network for Underwater Image Enhancement](http://arxiv.org/abs/2404.17883v3)**|None|**[link](https://github.com/windysprint/uvz)**|None|Zhixiong Huang et.al.|
|**2024-04-26**|**[A Novel Spike Transformer Network for Depth Estimation from Event Cameras via Cross-modality Knowledge Distillation](http://arxiv.org/abs/2404.17335v2)**|None|None|16 pages|Xin Zhang et.al.|
|**2024-04-25**|**[The Third Monocular Depth Estimation Challenge](http://arxiv.org/abs/2404.16831v2)**|None|None|To appear in CVPRW2024|Jaime Spencer et.al.|
|**2024-04-25**|**[MonoPCC: Photometric-invariant Cycle Constraint for Monocular Depth Estimation of Endoscopic Images](http://arxiv.org/abs/2404.16571v2)**|None|None|11 pages, 10 figures|Zhiwei Wang et.al.|
|**2024-04-25**|**[Promoting CNNs with Cross-Architecture Knowledge Distillation for Efficient Monocular Depth Estimation](http://arxiv.org/abs/2404.16386v1)**|None|None|None|Zhimeng Zheng et.al.|
|**2024-03-22**|**[Metric3D v2: A Versatile Monocular Geometric Foundation Model for Zero-shot Metric Depth and Surface Normal Estimation](http://arxiv.org/abs/2404.15506v1)**|None|None|Our project page is at https://JUGGHM.github.io/Metric3Dv2. arXiv   admin note: substantial text overlap with arXiv:2307.10984|Mu Hu et.al.|
|**2024-04-23**|**[SGFormer: Spherical Geometry Transformer for 360 Depth Estimation](http://arxiv.org/abs/2404.14979v1)**|None|None|None|Junsong Zhang et.al.|
|**2024-04-23**|**[Mining Supervision for Dynamic Regions in Self-Supervised Monocular Depth Estimation](http://arxiv.org/abs/2404.14908v1)**|None|**[link](https://github.com/hoangchuongnguyen/mono-consistent-depth)**|Accepted to CVPR2024|Hoang Chuong Nguyen et.al.|
|**2024-04-22**|**[Self-Supervised Monocular Depth Estimation in the Dark: Towards Data Distribution Compensation](http://arxiv.org/abs/2404.13854v1)**|None|None|Accepted by IJCAI2024|Haolin Yang et.al.|
|**2024-04-21**|**[GScream: Learning 3D Geometry and Feature Consistent Gaussian Splatting for Object Removal](http://arxiv.org/abs/2404.13679v1)**|None|None|Project Page: https://w-ted.github.io/publications/gscream|Yuxin Wang et.al.|
|**2024-04-20**|**[High-fidelity Endoscopic Image Synthesis by Utilizing Depth-guided Neural Surfaces](http://arxiv.org/abs/2404.13437v1)**|None|None|None|Baoru Huang et.al.|
|**2024-04-18**|**[SPIdepth: Strengthened Pose Information for Self-supervised Monocular Depth Estimation](http://arxiv.org/abs/2404.12501v1)**|None|None|None|Mykola Lavreniuk et.al.|
|**2024-04-18**|**[BLINK: Multimodal Large Language Models Can See but Not Perceive](http://arxiv.org/abs/2404.12390v3)**|None|None|Multimodal Benchmark, Project Url: https://zeyofu.github.io/blink/|Xingyu Fu et.al.|
|**2024-04-17**|**[How to deal with glare for improved perception of Autonomous Vehicles](http://arxiv.org/abs/2404.10992v1)**|None|None|14 pages, 9 figures, Accepted IEEE TIV|Muhammad Z. Alam et.al.|
|**2024-04-12**|**[Into the Fog: Evaluating Multiple Object Tracking Robustness](http://arxiv.org/abs/2404.10534v1)**|None|**[link](https://github.com/nadezola/intothefog_mot17)**|None|Nadezda Kirillova et.al.|
|**2024-04-15**|**[Digging into contrastive learning for robust depth estimation with diffusion models](http://arxiv.org/abs/2404.09831v3)**|None|None|8 pages,6 figures|Jiyuan Wang et.al.|
|**2024-04-15**|**[Virtually Enriched NYU Depth V2 Dataset for Monocular Depth Estimation: Do We Need Artificial Augmentation?](http://arxiv.org/abs/2404.09469v1)**|Proceedings of the IEEE Conference on Computer Vision and Pattern   Recognition Workshops, pages 6177-6186, 2024|**[link](https://github.com/abrain-one/anyu)**|None|Dmitry Ignatov et.al.|
|**2024-04-14**|**[In My Perspective, In My Hands: Accurate Egocentric 2D Hand Pose and Action Recognition](http://arxiv.org/abs/2404.09308v1)**|None|None|Accepted at: The 18th IEEE International Conference on Automatic Face   and Gesture Recognition|Wiktor Mucha et.al.|
|**2024-04-12**|**[On the Robustness of Language Guidance for Low-Level Vision Tasks: Findings from Depth Estimation](http://arxiv.org/abs/2404.08540v1)**|None|**[link](https://github.com/agneet42/robustness_depth_lang)**|Accepted to CVPR 2024. Project webpage:   https://agneetchatterjee.com/robustness_depth_lang/|Agneet Chatterjee et.al.|
|**2024-04-11**|**[Depth Estimation using Weighted-loss and Transfer Learning](http://arxiv.org/abs/2404.07686v1)**|None|None|None|Muhammad Adeel Hafeez et.al.|
|**2024-04-11**|**[GLID: Pre-training a Generalist Encoder-Decoder Vision Model](http://arxiv.org/abs/2404.07603v1)**|None|None|CVPR 2024|Jihao Liu et.al.|
|**2024-04-11**|**[Implicit and Explicit Language Guidance for Diffusion-based Visual Perception](http://arxiv.org/abs/2404.07600v2)**|None|None|None|Hefeng Wang et.al.|
|**2024-04-11**|**[Stereo-LiDAR Depth Estimation with Deformable Propagation and Learned Disparity-Depth Conversion](http://arxiv.org/abs/2404.07545v1)**|None|**[link](https://github.com/sjtu-visys/sdg-depth)**|Accepted in ICRA 2024. 8 pages, 6 figures|Ang Li et.al.|
|**2024-04-10**|**[Self-supervised Monocular Depth Estimation on Water Scenes via Specular Reflection Prior](http://arxiv.org/abs/2404.07176v1)**|None|None|16 pages, 8 figures|Zhengyang Lu et.al.|
|**2024-04-10**|**[MonoSelfRecon: Purely Self-Supervised Explicit Generalizable 3D Reconstruction of Indoor Scenes from Monocular RGB Views](http://arxiv.org/abs/2404.06753v1)**|None|None|None|Runfa Li et.al.|
|**2024-04-09**|**[RoadBEV: Road Surface Reconstruction in Bird's Eye View](http://arxiv.org/abs/2404.06605v2)**|None|**[link](https://github.com/ztsrxh/roadbev)**|Dataset page: https://thu-rsxd.com/rsrd Code:   https://github.com/ztsrxh/RoadBEV|Tong Zhao et.al.|
|**2024-04-09**|**[ZeST: Zero-Shot Material Transfer from a Single Image](http://arxiv.org/abs/2404.06425v1)**|None|None|Project Page: https://ttchengab.github.io/zest|Ta-Ying Cheng et.al.|
|**2024-04-09**|**[Matching 2D Images in 3D: Metric Relative Pose from Metric Correspondences](http://arxiv.org/abs/2404.06337v1)**|Proceedings of the IEEE/CVF Conference on Computer Vision and   Pattern Recognition (CVPR), 2024|**[link](https://github.com/nianticlabs/mickey)**|None|Axel Barroso-Laguna et.al.|
|**2024-04-09**|**[Enhanced Radar Perception via Multi-Task Learning: Towards Refined Data for Sensor Fusion Applications](http://arxiv.org/abs/2404.06165v1)**|None|None|Accepted by IEEE Intelligent Vehicles Symposium (IV 2024)|Huawei Sun et.al.|
|**2024-04-09**|**[Incremental Joint Learning of Depth, Pose and Implicit Scene Representation on Monocular Camera in Large-scale Scenes](http://arxiv.org/abs/2404.06050v1)**|None|None|None|Tianchen Deng et.al.|
|**2024-04-06**|**[HawkDrive: A Transformer-driven Visual Perception System for Autonomous Driving in Night Scene](http://arxiv.org/abs/2404.04653v2)**|None|None|Accepted by IEEE IV 2024|Ziang Guo et.al.|
|**2024-04-06**|**[Co-Occ: Coupling Explicit Feature Fusion with Volume Rendering Regularization for Multi-Modal 3D Semantic Occupancy Prediction](http://arxiv.org/abs/2404.04561v3)**|IEEE Robotics and Automation Letters, Volume 9 Issue 6, 5687 -   5694, June 2024|None|Accepted by IEEE Robotics and Automation Letters (RA-L)|Jingyi Pan et.al.|
|**2024-04-05**|**[SpatialTracker: Tracking Any 2D Pixels in 3D Space](http://arxiv.org/abs/2404.04319v1)**|None|None|Accepted to CVPR 2024 (selected as highlight paper). Project page:   https://henry123-boy.github.io/SpaTracker/|Yuxi Xiao et.al.|
|**2024-04-04**|**[Know Your Neighbors: Improving Single-View Reconstruction via Spatial Vision-Language Reasoning](http://arxiv.org/abs/2404.03658v1)**|None|**[link](https://github.com/ruili3/Know-Your-Neighbors)**|CVPR 2024. Project page: https://ruili3.github.io/kyn|Rui Li et.al.|
|**2024-04-04**|**[MVD-Fusion: Single-view 3D via Depth-consistent Multi-view Generation](http://arxiv.org/abs/2404.03656v1)**|None|None|Project page: https://mvd-fusion.github.io/|Hanzhe Hu et.al.|
|**2024-04-04**|**[WorDepth: Variational Language Prior for Monocular Depth Estimation](http://arxiv.org/abs/2404.03635v4)**|None|**[link](https://github.com/adonis-galaxy/wordepth)**|None|Ziyao Zeng et.al.|
|**2024-04-04**|**[Adaptive Discrete Disparity Volume for Self-supervised Monocular Depth Estimation](http://arxiv.org/abs/2404.03190v1)**|None|None|None|Jianwei Ren et.al.|
|**2024-04-04**|**[MonoCD: Monocular 3D Object Detection with Complementary Depths](http://arxiv.org/abs/2404.03181v1)**|None|**[link](https://github.com/elvintanhust/monocd)**|Accepted to CVPR 2024|Longfei Yan et.al.|
|**2024-04-02**|**[CHOSEN: Contrastive Hypothesis Selection for Multi-View Depth Refinement](http://arxiv.org/abs/2404.02225v1)**|None|None|None|Di Qiu et.al.|
|**2024-04-02**|**[Improving Bird's Eye View Semantic Segmentation by Task Decomposition](http://arxiv.org/abs/2404.01925v1)**|None|None|Accepted by CVPR 2024|Tianhao Zhao et.al.|
|**2024-04-01**|**[BadPart: Unified Black-box Adversarial Patch Attacks against Pixel-wise Regression Tasks](http://arxiv.org/abs/2404.00924v3)**|None|**[link](https://github.com/bob-cheng/badpart)**|Paper accepted at ICML 2024|Zhiyuan Cheng et.al.|
|**2024-04-01**|**[MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision, Depth, and Inertial Measurements](http://arxiv.org/abs/2404.00923v1)**|None|None|Project Webpage: https://vita-group.github.io/MM3DGS-SLAM|Lisong C. Sun et.al.|
|**2024-03-31**|**[OmniSDF: Scene Reconstruction using Omnidirectional Signed Distance Functions and Adaptive Binoctrees](http://arxiv.org/abs/2404.00678v1)**|Proceedings of the IEEE/CVF Conference on Computer Vision and   Pattern Recognition (CVPR) 2024|None|None|Hakyeong Kim et.al.|
|**2024-03-30**|**[The Devil is in the Edges: Monocular Depth Estimation with Edge-aware Consistency Fusion](http://arxiv.org/abs/2404.00373v1)**|None|None|17 pages, 19 figures|Pengzhi Li et.al.|
|**2024-03-30**|**[Reusable Architecture Growth for Continual Stereo Matching](http://arxiv.org/abs/2404.00360v1)**|None|None|Extended version of CVPR 2022 paper "Continual Stereo Matching of   Continuous Driving Scenes with Growing Architecture" - Accepted to TPAMI in   2024|Chenghao Zhang et.al.|
|**2024-03-30**|**[MaGRITTe: Manipulative and Generative 3D Realization from Image, Topview and Text](http://arxiv.org/abs/2404.00345v1)**|None|None|Project Page: https://hara012.github.io/MaGRITTe-project|Takayuki Hara et.al.|
|**2024-03-29**|**[VSRD: Instance-Aware Volumetric Silhouette Rendering for Weakly Supervised 3D Object Detection](http://arxiv.org/abs/2404.00149v1)**|None|**[link](https://github.com/skmhrk1209/VSRD)**|CVPR 2024|Zihua Liu et.al.|
|**2024-03-29**|**[NeSLAM: Neural Implicit Mapping and Self-Supervised Feature Tracking With Depth Completion and Denoising](http://arxiv.org/abs/2403.20034v1)**|None|**[link](https://github.com/dtc111111/neslam)**|None|Tianchen Deng et.al.|
|**2024-03-28**|**[GlORIE-SLAM: Globally Optimized RGB-only Implicit Encoding Point Cloud SLAM](http://arxiv.org/abs/2403.19549v3)**|None|**[link](https://github.com/zhangganlin/gloire-slam)**|None|Ganlin Zhang et.al.|
|**2024-03-28**|**[CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians](http://arxiv.org/abs/2403.19495v1)**|None|None|Project page: https://people.engr.tamu.edu/nimak/Papers/CoherentGS|Avinash Paliwal et.al.|
|**2024-03-28**|**[FlowDepth: Decoupling Optical Flow for Self-Supervised Monocular Depth Estimation](http://arxiv.org/abs/2403.19294v1)**|None|None|None|Yiyang Sun et.al.|
|**2024-03-28**|**[Neural Fields for 3D Tracking of Anatomy and Surgical Instruments in Monocular Laparoscopic Video Clips](http://arxiv.org/abs/2403.19265v1)**|None|None|None|Beerend G. A. Gerats et.al.|
|**2024-03-27**|**[UniDepth: Universal Monocular Metric Depth Estimation](http://arxiv.org/abs/2403.18913v1)**|None|**[link](https://github.com/lpiccinelli-eth/unidepth)**|None|Luigi Piccinelli et.al.|
|**2024-03-27**|**[ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth Estimation](http://arxiv.org/abs/2403.18807v4)**|None|**[link](https://github.com/aradhye2002/ecodepth)**|IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)   2024|Suraj Patni et.al.|
|**2024-03-27**|**[ModaLink: Unifying Modalities for Efficient Image-to-PointCloud Place Recognition](http://arxiv.org/abs/2403.18762v1)**|None|**[link](https://github.com/haomo-ai/modalink)**|8 pages, 11 figures, conference|Weidong Xie et.al.|
|**2024-03-27**|**[$\mathrm{F^2Depth}$: Self-supervised Indoor Monocular Depth Estimation via Optical Flow Consistency and Feature Map Synthesis](http://arxiv.org/abs/2403.18443v1)**|None|None|None|Xiaotong Guo et.al.|
|**2024-03-26**|**[Track Everything Everywhere Fast and Robustly](http://arxiv.org/abs/2403.17931v1)**|None|None|project page: https://timsong412.github.io/FastOmniTrack/|Yunzhou Song et.al.|
|**2024-03-26**|**[Leveraging Near-Field Lighting for Monocular Depth Estimation from Endoscopy Videos](http://arxiv.org/abs/2403.17915v1)**|None|None|26 pages, 7 tables, 7 figures|Akshay Paruchuri et.al.|
|**2024-03-26**|**[DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing](http://arxiv.org/abs/2403.17822v1)**|None|**[link](https://github.com/maturk/dn-splatter)**|None|Matias Turkulainen et.al.|
|**2024-03-26**|**[Physical 3D Adversarial Attacks against Monocular Depth Estimation in Autonomous Driving](http://arxiv.org/abs/2403.17301v2)**|None|**[link](https://github.com/gandolfczjh/3d2fool)**|Accepted by CVPR 2024|Junhao Zheng et.al.|
|**2024-03-25**|**[Spike-NeRF: Neural Radiance Field Based On Spike Camera](http://arxiv.org/abs/2403.16410v1)**|None|None|This paper is accepted by ICME2024|Yijia Guo et.al.|
|**2024-03-25**|**[Elite360D: Towards Efficient 360 Depth Estimation via Semantic- and Distance-Aware Bi-Projection Fusion](http://arxiv.org/abs/2403.16376v2)**|None|None|8 pages, accepted by CVPR2024|Hao Ai et.al.|
|**2024-03-23**|**[Depth Estimation fusing Image and Radar Measurements with Uncertain Directions](http://arxiv.org/abs/2403.15787v1)**|None|None|Accepted to IJCNN 2024 (International Joint Conference on Neural   Networks)|Masaya Kotani et.al.|
|**2024-03-22**|**[Language-Based Depth Hints for Monocular Depth Estimation](http://arxiv.org/abs/2403.15551v1)**|None|None|8 pages, 1 figure. Work originally done in June 2022|Dylan Auty et.al.|
|**2024-03-21**|**[Learning to Project for Cross-Task Knowledge Distillation](http://arxiv.org/abs/2403.14494v1)**|None|None|None|Dylan Auty et.al.|
|**2024-03-20**|**[DepthFM: Fast Monocular Depth Estimation with Flow Matching](http://arxiv.org/abs/2403.13788v1)**|None|None|None|Ming Gui et.al.|
|**2024-03-19**|**[When Do We Not Need Larger Vision Models?](http://arxiv.org/abs/2403.13043v1)**|None|**[link](https://github.com/bfshi/scaling_on_scales)**|Code: https://github.com/bfshi/scaling_on_scales|Baifeng Shi et.al.|
|**2024-03-19**|**[FutureDepth: Learning to Predict the Future Improves Video Depth Estimation](http://arxiv.org/abs/2403.12953v1)**|None|None|None|Rajeev Yasarla et.al.|
|**2024-03-19**|**[Geometric Constraints in Deep Learning Frameworks: A Survey](http://arxiv.org/abs/2403.12431v1)**|None|None|A preprint|Vibhas K Vats et.al.|
|**2024-03-18**|**[GraphBEV: Towards Robust BEV Feature Alignment for Multi-Modal 3D Object Detection](http://arxiv.org/abs/2403.11848v2)**|None|None|None|Ziying Song et.al.|
|**2024-03-18**|**[SSAP: A Shape-Sensitive Adversarial Patch for Comprehensive Disruption of Monocular Depth Estimation in Autonomous Navigation Applications](http://arxiv.org/abs/2403.11515v1)**|None|None|None|Amira Guesmi et.al.|
|**2024-03-17**|**[Bilateral Propagation Network for Depth Completion](http://arxiv.org/abs/2403.11270v2)**|None|**[link](https://github.com/kakaxi314/bp-net)**|Accepted by CVPR 2024|Jie Tang et.al.|
|**2024-03-15**|**[SwinMTL: A Shared Architecture for Simultaneous Depth Estimation and Semantic Segmentation from Monocular Camera Images](http://arxiv.org/abs/2403.10662v1)**|None|**[link](https://github.com/pardistaghavi/swinmtl)**|None|Pardis Taghavi et.al.|
|**2024-03-15**|**[Robust Shape Fitting for 3D Scene Abstraction](http://arxiv.org/abs/2403.10452v1)**|None|**[link](https://github.com/fkluger/cuboids_revisited)**|Accepted for publication in Transactions on Pattern Analysis and   Machine Intelligence (PAMI). arXiv admin note: substantial text overlap with   arXiv:2105.02047|Florian Kluger et.al.|
|**2024-03-15**|**[Region-aware Distribution Contrast: A Novel Approach to Multi-Task Partially Supervised Learning](http://arxiv.org/abs/2403.10252v1)**|None|None|None|Meixuan Li et.al.|
|**2024-03-14**|**[Improving Distant 3D Object Detection Using 2D Box Supervision](http://arxiv.org/abs/2403.09230v1)**|None|None|Accepted by CVPR 2024|Zetong Yang et.al.|
|**2024-03-13**|**[SM4Depth: Seamless Monocular Metric Depth Estimation across Multiple Cameras and Scenes by One Model](http://arxiv.org/abs/2403.08556v1)**|None|**[link](https://github.com/1hao-liu/sm4depth)**|Project Page: xuefeng-cvr.github.io/SM4Depth|Yihao Liu et.al.|
|**2024-03-13**|**[METER: a mobile vision transformer architecture for monocular depth estimation](http://arxiv.org/abs/2403.08368v1)**|IEEE Transactions on Circuits and Systems for Video Technology,   2023|**[link](https://github.com/lorenzopapa5/meter)**|None|L. Papa et.al.|
|**2024-03-12**|**[Q-SLAM: Quadric Representations for Monocular SLAM](http://arxiv.org/abs/2403.08125v1)**|None|None|None|Chensheng Peng et.al.|
|**2024-03-12**|**[Adaptive Fusion of Single-View and Multi-View Depth for Autonomous Driving](http://arxiv.org/abs/2403.07535v1)**|None|**[link](https://github.com/junda24/afnet)**|Accepted to CVPR 2024|JunDa Cheng et.al.|
|**2024-03-12**|**[D4D: An RGBD diffusion model to boost monocular depth estimation](http://arxiv.org/abs/2403.07516v1)**|None|**[link](https://github.com/lorenzopapa5/diffusion4d)**|None|L. Papa et.al.|
|**2024-03-12**|**[SGE: Structured Light System Based on Gray Code with an Event Camera](http://arxiv.org/abs/2403.07326v1)**|None|None|None|Xingyu Lu et.al.|
|**2024-03-11**|**[Confidence-Aware RGB-D Face Recognition via Virtual Depth Synthesis](http://arxiv.org/abs/2403.06529v2)**|None|None|9 pages, 5 figures|Zijian Chen et.al.|
|**2024-03-09**|**[DO3D: Self-supervised Learning of Decomposed Object-aware 3D Motion and Depth from Monocular Videos](http://arxiv.org/abs/2403.05895v1)**|None|None|24 pages, 14 figures, Tech Report|Xiuzhe Wu et.al.|
|**2024-03-08**|**[OccFusion: Depth Estimation Free Multi-sensor Fusion for 3D Occupancy Prediction](http://arxiv.org/abs/2403.05329v1)**|None|None|None|Ji Zhang et.al.|
|**2024-03-08**|**[Stealing Stable Diffusion Prior for Robust Monocular Depth Estimation](http://arxiv.org/abs/2403.05056v1)**|None|**[link](https://github.com/hitcslj/ssd)**|None|Yifan Mao et.al.|
|**2024-03-06**|**[Multi-task Learning for Real-time Autonomous Driving Leveraging Task-adaptive Attention Generator](http://arxiv.org/abs/2403.03468v1)**|None|None|Accepted at ICRA 2024|Wonhyeok Choi et.al.|
|**2024-03-06**|**[Scene Depth Estimation from Traditional Oriental Landscape Paintings](http://arxiv.org/abs/2403.03408v2)**|None|None|None|Sungho Kang et.al.|
|**2024-03-04**|**[Scalable Vision-Based 3D Object Detection and Monocular Depth Estimation for Autonomous Driving](http://arxiv.org/abs/2403.02037v1)**|None|**[link](https://github.com/owen-liuyuxuan/visionfactory)**|HKUST PhD Thesis; https://github.com/Owen-Liuyuxuan/visionfactory|Yuxuan Liu et.al.|
|**2024-03-04**|**[DD-VNB: A Depth-based Dual-Loop Framework for Real-time Visually Navigated Bronchoscopy](http://arxiv.org/abs/2403.01683v2)**|None|None|None|Qingyao Tian et.al.|
|**2024-03-03**|**[Kick Back & Relax++: Scaling Beyond Ground-Truth Depth with SlowTV & CribsTV](http://arxiv.org/abs/2403.01569v1)**|None|**[link](https://github.com/jspenmar/slowtv_monodepth)**|None|Jaime Spencer et.al.|
|**2024-03-03**|**[Pyramid Feature Attention Network for Monocular Depth Prediction](http://arxiv.org/abs/2403.01440v1)**|None|None|6 pages, 5 figures|Yifang Xu et.al.|
|**2024-03-03**|**[Depth Estimation Algorithm Based on Transformer-Encoder and Feature Fusion](http://arxiv.org/abs/2403.01370v1)**|None|None|ICAACE2024|Linhan Xia et.al.|
|**2024-03-02**|**[Depth Information Assisted Collaborative Mutual Promotion Network for Single Image Dehazing](http://arxiv.org/abs/2403.01105v1)**|None|**[link](https://github.com/zhoushen1/diacmpn)**|None|Yafei Zhang et.al.|
|**2024-02-29**|**[PCDepth: Pattern-based Complementary Learning for Monocular Depth Estimation by Best of Both Worlds](http://arxiv.org/abs/2402.18925v1)**|None|None|Under Review|Haotian Liu et.al.|
|**2024-02-28**|**[CFDNet: A Generalizable Foggy Stereo Matching Network with Contrastive Feature Distillation](http://arxiv.org/abs/2402.18181v2)**|IEEE International Conference on Robotics and Automation   (ICRA2024)|None|None|Zihua Liu et.al.|
|**2024-02-28**|**[Self-Supervised Spatially Variant PSF Estimation for Aberration-Aware Depth-from-Defocus](http://arxiv.org/abs/2402.18175v1)**|International Conference on Acoustics, Speech, and Signal   Processing (ICASSP), 2024|None|None|Zhuofeng Wu et.al.|
|**2024-02-27**|**[A Vanilla Multi-Task Framework for Dense Visual Prediction Solution to 1st VCL Challenge -- Multi-Task Robustness Track](http://arxiv.org/abs/2402.17319v1)**|None|None|Technical Report|Zehui Chen et.al.|
|**2024-02-22**|**[GAM-Depth: Self-Supervised Indoor Depth Estimation Leveraging a Gradient-Aware Mask and Semantic Constraints](http://arxiv.org/abs/2402.14354v1)**|None|**[link](https://github.com/anqicheng1234/gam-depth)**|To be published in 2024 IEEE International Conference on Robotics and   Automation (ICRA)|Anqi Cheng et.al.|
|**2024-02-22**|**[TIE-KD: Teacher-Independent and Explainable Knowledge Distillation for Monocular Depth Estimation](http://arxiv.org/abs/2402.14340v1)**|None|**[link](https://github.com/hpc-lab-koreatech/tie-kd)**|13 pages, 8 figures, under review for a journal|Sangwon Choi et.al.|
|**2024-02-21**|**[Zero-BEV: Zero-shot Projection of Any First-Person Modality to BEV Maps](http://arxiv.org/abs/2402.13848v2)**|None|None|None|Gianluca Monaci et.al.|
|**2024-02-19**|**[An Endoscopic Chisel: Intraoperative Imaging Carves 3D Anatomical Models](http://arxiv.org/abs/2402.11840v1)**|None|None|None|Jan Emily Mangulabnan et.al.|
|**2024-02-19**|**[Unveiling the Depths: A Multi-Modal Fusion Framework for Challenging Scenarios](http://arxiv.org/abs/2402.11826v1)**|None|None|None|Jialei Xu et.al.|
|**2024-02-19**|**[SDGE: Stereo Guided Depth Estimation for 360$^\circ$ Camera Sets](http://arxiv.org/abs/2402.11791v4)**|None|None|None|Jialei Xu et.al.|
|**2024-02-18**|**[MAL: Motion-Aware Loss with Temporal and Distillation Hints for Self-Supervised Depth Estimation](http://arxiv.org/abs/2402.11507v1)**|None|None|Accepted by ICRA 2024; Project homepage:   https://yuejiangdong.github.io/MotionAwareLoss/|Yup-Jiang Dong et.al.|
|**2024-02-16**|**[Efficient Multi-task Uncertainties for Joint Semantic Segmentation and Monocular Depth Estimation](http://arxiv.org/abs/2402.10580v1)**|None|None|17 pages, 5 figures, 10 tables, submitted to peer-reviewed journal|Steven Landgraf et.al.|
|**2024-02-15**|**[X-maps: Direct Depth Lookup for Event-based Structured Light Systems](http://arxiv.org/abs/2402.10061v1)**|2023 IEEE/CVF Conference on Computer Vision and Pattern   Recognition Workshops (CVPRW), Vancouver, BC, Canada, 2023, pp. 4007-4015|None|Accepted at the CVPR 2023 Workshop on Event-based Vision:   https://tub-rip.github.io/eventvision2023/|Wieland Morgenstern et.al.|
|**2024-02-14**|**[Depth-aware Volume Attention for Texture-less Stereo Matching](http://arxiv.org/abs/2402.08931v2)**|None|**[link](https://github.com/ztsrxh/DVANet)**|10 pages, 6 figures|Tong Zhao et.al.|
|**2024-02-09**|**[Hybridnet for depth estimation and semantic segmentation](http://arxiv.org/abs/2402.06539v1)**|None|None|2018 IEEE International Conference on Acoustics, Speech and Signal   Processing (ICASSP). IEEE, 2018|Dalila Sánchez-Escobedo et.al.|
|**2024-02-08**|**[Adaptive Surface Normal Constraint for Geometric Estimation from Monocular Images](http://arxiv.org/abs/2402.05869v2)**|None|None|Accepted by TPAMI. arXiv admin note: substantial text overlap with   arXiv:2103.15483|Xiaoxiao Long et.al.|
|**2024-02-07**|**[Toward Accurate Camera-based 3D Object Detection via Cascade Depth Estimation and Calibration](http://arxiv.org/abs/2402.04883v1)**|None|None|Accepted to ICRA2024|Chaoqun Wang et.al.|
|**2024-02-06**|**[Energy-based Domain-Adaptive Segmentation with Depth Guidance](http://arxiv.org/abs/2402.03795v1)**|None|None|None|Jinjing Zhu et.al.|
|**2024-02-06**|**[MoD-SLAM: Monocular Dense Mapping for Unbounded 3D Scene Reconstruction](http://arxiv.org/abs/2402.03762v5)**|None|None|None|Heng Zhou et.al.|
|**2024-02-05**|**[An Inpainting-Infused Pipeline for Attire and Background Replacement](http://arxiv.org/abs/2402.03501v1)**|None|None|None|Felipe Rodrigues Perche-Mahlow et.al.|
|**2024-02-05**|**[CLIP Can Understand Depth](http://arxiv.org/abs/2402.03251v1)**|None|None|None|Dunam Kim et.al.|
|**2024-02-03**|**[Decomposition-based and Interference Perception for Infrared and Visible Image Fusion in Complex Scenes](http://arxiv.org/abs/2402.02096v1)**|None|None|None|Xilai Li et.al.|
|**2024-02-03**|**[RIDERS: Radar-Infrared Depth Estimation for Robust Sensing](http://arxiv.org/abs/2402.02067v1)**|None|**[link](https://github.com/mmocking/riders)**|13 pages, 13 figures|Han Li et.al.|
|**2024-02-02**|**[Robust Inverse Graphics via Probabilistic Inference](http://arxiv.org/abs/2402.01915v2)**|None|**[link](https://github.com/tensorflow/probability)**|ICML submission. Reworked main body, new appendix figures|Tuan Anh Le et.al.|
|**2024-02-02**|**[Convolution kernel adaptation to calibrated fisheye](http://arxiv.org/abs/2402.01456v1)**|None|**[link](https://github.com/sbrunoberenguel/calibratedconvolutions)**|Previously presented at BMVC: https://proceedings.bmvc2023.org/721/|Bruno Berenguel-Baeta et.al.|
|**2024-02-01**|**[Diffusion-based Light Field Synthesis](http://arxiv.org/abs/2402.00575v1)**|None|None|11 pages,9 figures|Ruisheng Gao et.al.|
|**2024-01-29**|**[Depth Anything in Medical Images: A Comparative Study](http://arxiv.org/abs/2401.16600v1)**|None|None|10 pages, 2 figures, 3 tables|John J. Han et.al.|
|**2024-01-29**|**[Endo-4DGS: Endoscopic Monocular Scene Reconstruction with 4D Gaussian Splatting](http://arxiv.org/abs/2401.16416v4)**|None|**[link](https://github.com/lastbasket/endo-4dgs)**|None|Yiming Huang et.al.|
|**2024-01-25**|**[Range-Agnostic Multi-View Depth Estimation With Keyframe Selection](http://arxiv.org/abs/2401.14401v1)**|None|**[link](https://github.com/andreaconti/ramdepth)**|3DV 2024 Project Page   https://andreaconti.github.io/projects/range_agnostic_multi_view_depth GitHub   Page https://github.com/andreaconti/ramdepth.git|Andrea Conti et.al.|
|**2024-01-24**|**[FoVA-Depth: Field-of-View Agnostic Depth Estimation for Cross-Dataset Generalization](http://arxiv.org/abs/2401.13786v1)**|None|None|3DV 2024 (Oral); Project Website:   https://research.nvidia.com/labs/lpr/fova-depth/|Daniel Lichy et.al.|
|**2024-01-23**|**[EndoGaussian: Real-time Gaussian Splatting for Dynamic Endoscopic Scene Reconstruction](http://arxiv.org/abs/2401.12561v2)**|None|**[link](https://github.com/yifliu3/EndoGaussian)**|None|Yifan Liu et.al.|
|**2024-01-23**|**[InverseMatrixVT3D: An Efficient Projection Matrix-Based Approach for 3D Occupancy Prediction](http://arxiv.org/abs/2401.12422v2)**|None|**[link](https://github.com/danielming123/inversematrixvt3d)**|None|Zhenxing Ming et.al.|
|**2024-01-23**|**[Icy Moon Surface Simulation and Stereo Depth Estimation for Sampling Autonomy](http://arxiv.org/abs/2401.12414v1)**|None|**[link](https://github.com/nasa-jpl/guiss)**|Software: https://github.com/nasa-jpl/guiss. IEEE Aerospace   Conference 2024|Ramchander Bhaskara et.al.|
|**2024-01-22**|**[Stereo-Matching Knowledge Distilled Monocular Depth Estimation Filtered by Multiple Disparity Consistency](http://arxiv.org/abs/2401.12019v2)**|None|None|ICASSP 2024. The first two authors are equally contributed|Woonghyun Ka et.al.|
|**2024-01-22**|**[MVSFormer++: Revealing the Devil in Transformer's Details for Multi-View Stereo](http://arxiv.org/abs/2401.11673v1)**|ICLR(International Conference on Learning Representations) 2024|**[link](https://github.com/maybelx/mvsformerplusplus)**|Accepted to ICLR2024|Chenjie Cao et.al.|
|**2024-01-19**|**[Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data](http://arxiv.org/abs/2401.10891v2)**|None|**[link](https://github.com/LiheYoung/Depth-Anything)**|Accepted by CVPR 2024. Project page: https://depth-anything.github.io|Lihe Yang et.al.|
|**2024-01-14**|**[Self-supervised Event-based Monocular Depth Estimation using Cross-modal Consistency](http://arxiv.org/abs/2401.07218v1)**|None|None|Accepted by IROS2023|Junyu Zhu et.al.|
|**2024-01-11**|**[A Study on Self-Supervised Pretraining for Vision Problems in Gastrointestinal Endoscopy](http://arxiv.org/abs/2401.06278v2)**|None|**[link](https://github.com/esandml/ssl4gie)**|None|Edward Sanderson et.al.|
|**2024-01-11**|**[Surgical-DINO: Adapter Learning of Foundation Models for Depth Estimation in Endoscopic Surgery](http://arxiv.org/abs/2401.06013v2)**|None|**[link](https://github.com/beileicui/surgicaldino)**|Accepted by IPCAI 2024 (IJCAR Special Issue)|Beilei Cui et.al.|
|**2024-01-10**|**[InseRF: Text-Driven Generative Object Insertion in Neural 3D Scenes](http://arxiv.org/abs/2401.05335v1)**|None|None|None|Mohamad Shahbazi et.al.|
|**2024-01-09**|**[RadarCam-Depth: Radar-Camera Fusion for Depth Estimation with Learned Metric Scale](http://arxiv.org/abs/2401.04325v2)**|None|**[link](https://github.com/mmocking/radarcam-depth)**|None|Han Li et.al.|
|**2024-01-08**|**[NeRFmentation: NeRF-based Augmentation for Monocular Depth Estimation](http://arxiv.org/abs/2401.03771v1)**|None|None|None|Casimir Feldmann et.al.|
|**2024-01-06**|**[Hi-Map: Hierarchical Factorized Radiance Field for High-Fidelity Monocular Dense Mapping](http://arxiv.org/abs/2401.03203v1)**|None|None|None|Tongyan Hua et.al.|
|**2024-01-02**|**[Depth-discriminative Metric Learning for Monocular 3D Object Detection](http://arxiv.org/abs/2401.01075v1)**|None|None|Accepted at NeurIPS 2023|Wonhyeok Choi et.al.|
|**2023-12-31**|**[SteinDreamer: Variance Reduction for Text-to-3D Score Distillation via Stein Identity](http://arxiv.org/abs/2401.00604v2)**|None|None|Project page: https://vita-group.github.io/SteinDreamer/|Peihao Wang et.al.|
|**2023-12-26**|**[Learning Deformable Hypothesis Sampling for Accurate PatchMatch Multi-View Stereo](http://arxiv.org/abs/2312.15970v1)**|None|**[link](https://github.com/geo-tell/ds-pmnet)**|None|Hongjie Li et.al.|
|**2023-12-23**|**[MGDepth: Motion-Guided Cost Volume For Self-Supervised Monocular Depth In Dynamic Scenarios](http://arxiv.org/abs/2312.15268v1)**|None|None|None|Kaichen Zhou et.al.|
|**2023-12-22**|**[Lift-Attend-Splat: Bird's-eye-view camera-lidar fusion using transformers](http://arxiv.org/abs/2312.14919v3)**|None|None|Updated method figure; camera ready|James Gunn et.al.|
|**2023-12-22**|**[Harnessing Diffusion Models for Visual Perception with Meta Prompts](http://arxiv.org/abs/2312.14733v1)**|None|**[link](https://github.com/fudan-zvg/meta-prompts)**|None|Qiang Wan et.al.|
|**2023-12-22**|**[Pola4All: survey of polarimetric applications and an open-source toolkit to analyze polarization](http://arxiv.org/abs/2312.14697v1)**|None|**[link](https://github.com/vibot-lab/pola4all_jei_2023)**|None|Joaquin Rodriguez et.al.|
|**2023-12-21**|**[DUSt3R: Geometric 3D Vision Made Easy](http://arxiv.org/abs/2312.14132v1)**|None|**[link](https://github.com/naver/dust3r)**|None|Shuzhe Wang et.al.|
|**2023-12-20**|**[Zero-Shot Metric Depth with a Field-of-View Conditioned Diffusion Model](http://arxiv.org/abs/2312.13252v1)**|None|None|None|Saurabh Saxena et.al.|
|**2023-12-20**|**[PPEA-Depth: Progressive Parameter-Efficient Adaptation for Self-Supervised Monocular Depth Estimation](http://arxiv.org/abs/2312.13066v2)**|None|None|Accepted by AAAI 2024 Project homepage:   https://yuejiangdong.github.io/PPEADepth/|Yue-Jiang Dong et.al.|
|**2023-12-19**|**[Atlantis: Enabling Underwater Depth Estimation with Stable Diffusion](http://arxiv.org/abs/2312.12471v1)**|None|**[link](https://github.com/zkawfanx/atlantis)**|10 pages|Fan Zhang et.al.|
|**2023-12-18**|**[SinMPI: Novel View Synthesis from a Single Image with Expanded Multiplane Images](http://arxiv.org/abs/2312.11037v1)**|None|**[link](https://github.com/trickygo/sinmpi)**|10 pages|Guo Pu et.al.|
|**2023-12-18**|**[Long-Tailed 3D Detection via 2D Late Fusion](http://arxiv.org/abs/2312.10986v3)**|None|None|None|Yechi Ma et.al.|
|**2023-12-15**|**[From-Ground-To-Objects: Coarse-to-Fine Self-supervised Monocular Depth Estimation of Dynamic Objects with Ground Contact Prior](http://arxiv.org/abs/2312.10118v1)**|None|None|None|Jaeho Moon et.al.|
|**2023-12-14**|**[OccNeRF: Advancing 3D Occupancy Prediction in LiDAR-Free Environments](http://arxiv.org/abs/2312.09243v2)**|None|**[link](https://github.com/linshan-bin/occnerf)**|Code: https://github.com/LinShan-Bin/OccNeRF|Chubin Zhang et.al.|
|**2023-12-14**|**[Text2Immersion: Generative Immersive Scene with 3D Gaussians](http://arxiv.org/abs/2312.09242v1)**|None|None|Project page: https://ken-ouyang.github.io/text2immersion/index.html|Hao Ouyang et.al.|
|**2023-12-14**|**[CT-MVSNet: Efficient Multi-View Stereo with Cross-scale Transformer](http://arxiv.org/abs/2312.08594v2)**|None|**[link](https://github.com/wscstrive/ct-mvsnet)**|Accepted at the 30th International Conference on Multimedia   Modeling(MMM'24 Oral)|Sicheng Wang et.al.|
|**2023-12-13**|**[EVP: Enhanced Visual Perception using Inverse Multi-Attentive Feature Refinement and Regularized Image-Text Alignment](http://arxiv.org/abs/2312.08548v1)**|None|**[link](https://github.com/lavreniuk/evp)**|None|Mykola Lavreniuk et.al.|
|**2023-12-13**|**[Instance-aware Multi-Camera 3D Object Detection with Structural Priors Mining and Self-Boosting Learning](http://arxiv.org/abs/2312.08004v1)**|None|None|Accepted to AAAI 2024|Yang Jiao et.al.|
|**2023-12-10**|**[GenDepth: Generalizing Monocular Depth Estimation for Arbitrary Camera Parameters via Ground Plane Embedding](http://arxiv.org/abs/2312.06021v1)**|None|None|None|Karlo Koledić et.al.|
|**2023-12-08**|**[Fine Dense Alignment of Image Bursts through Camera Pose and Depth Estimation](http://arxiv.org/abs/2312.05190v1)**|None|None|None|Bruno Lecouat et.al.|
|**2023-12-07**|**[Camera Height Doesn't Change: Unsupervised Training for Metric Monocular Road-Scene Depth Estimation](http://arxiv.org/abs/2312.04530v2)**|None|None|None|Genki Kinoshita et.al.|
|**2023-12-04**|**[PatchFusion: An End-to-End Tile-Based Framework for High-Resolution Monocular Metric Depth Estimation](http://arxiv.org/abs/2312.02284v1)**|None|**[link](https://github.com/zhyever/PatchFusion)**|None|Zhenyu Li et.al.|
|**2023-12-02**|**[Diffusion Handles: Enabling 3D Edits for Diffusion Models by Lifting Activations to 3D](http://arxiv.org/abs/2312.02190v2)**|None|None|Project Webpage: https://diffusionhandles.github.io/|Karran Pandey et.al.|
|**2023-12-04**|**[GPS-Gaussian: Generalizable Pixel-wise 3D Gaussian Splatting for Real-time Human Novel View Synthesis](http://arxiv.org/abs/2312.02155v3)**|None|**[link](https://github.com/aipixel/gps-gaussian)**|Accepted by CVPR 2024 (Highlight). Project page:   https://shunyuanzheng.github.io/GPS-Gaussian|Shunyuan Zheng et.al.|
|**2023-12-04**|**[Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation](http://arxiv.org/abs/2312.02145v2)**|None|**[link](https://github.com/prs-eth/marigold)**|CVPR 2024 camera ready|Bingxin Ke et.al.|
|**2023-12-04**|**[GIVT: Generative Infinite-Vocabulary Transformers](http://arxiv.org/abs/2312.02116v3)**|None|**[link](https://github.com/google-research/big_vision)**|v2: add related NLP work, loss details. v3: Improved GMM formulation,   added adapter module, larger models, better image generation results. Code   and model checkpoints are available at:   https://github.com/google-research/big_vision|Michael Tschannen et.al.|
|**2023-12-04**|**[BEVNeXt: Reviving Dense BEV Frameworks for 3D Object Detection](http://arxiv.org/abs/2312.01696v2)**|None|None|None|Zhenxin Li et.al.|
|**2023-12-03**|**[Deeper into Self-Supervised Monocular Indoor Depth Estimation](http://arxiv.org/abs/2312.01283v1)**|None|**[link](https://github.com/fcntes/indoordepth)**|None|Chao Fan et.al.|
|**2023-12-01**|**[Enhancing Diffusion Models with 3D Perspective Geometry Constraints](http://arxiv.org/abs/2312.00944v1)**|None|None|Project Webpage: http://visual.ee.ucla.edu/diffusionperspective.htm/|Rishi Upadhyay et.al.|
|**2023-12-01**|**[FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting](http://arxiv.org/abs/2312.00451v2)**|None|None|Project page: https://zehaozhu.github.io/FSGS/|Zehao Zhu et.al.|
|**2023-11-30**|**[Multi-task learning with cross-task consistency for improved depth estimation in colonoscopy](http://arxiv.org/abs/2311.18664v1)**|None|None|19 pages|Pedro Esteban Chavarrias Solano et.al.|
|**2023-11-28**|**[UC-NeRF: Neural Radiance Field for Under-Calibrated Multi-view Cameras in Autonomous Driving](http://arxiv.org/abs/2311.16945v2)**|None|None|See the project page for code, data:   https://kcheng1021.github.io/ucnerf.github.io|Kai Cheng et.al.|
|**2023-11-28**|**[DGNR: Density-Guided Neural Point Rendering of Large Driving Scenes](http://arxiv.org/abs/2311.16664v1)**|None|None|None|Zhuopeng Li et.al.|
|**2023-11-22**|**[Depth-Regularized Optimization for 3D Gaussian Splatting in Few-Shot Images](http://arxiv.org/abs/2311.13398v3)**|None|None|10 pages, 5 figures; Project page: robot0321.github.io/DepthRegGS|Jaeyoung Chung et.al.|
|**2023-11-21**|**[Camera-Independent Single Image Depth Estimation from Defocus Blur](http://arxiv.org/abs/2311.13045v1)**|None|**[link](https://github.com/sleekeagle/defocus_camind)**|None|Lahiru Wijayasingha et.al.|
|**2023-11-21**|**[SelfOcc: Self-Supervised Vision-Based 3D Occupancy Prediction](http://arxiv.org/abs/2311.12754v2)**|None|**[link](https://github.com/huang-yh/selfocc)**|Code is available at: https://github.com/huang-yh/SelfOcc|Yuanhui Huang et.al.|
|**2023-11-21**|**[Transferring to Real-World Layouts: A Depth-aware Framework for Scene Adaptation](http://arxiv.org/abs/2311.12682v1)**|None|None|None|Mu Chen et.al.|
|**2023-11-16**|**[Depth Insight -- Contribution of Different Features to Indoor Single-image Depth Estimation](http://arxiv.org/abs/2311.10042v1)**|None|None|None|Yihong Wu et.al.|
|**2023-11-15**|**[RENI++ A Rotation-Equivariant, Scale-Invariant, Natural Illumination Prior](http://arxiv.org/abs/2311.09361v1)**|None|**[link](https://github.com/jadgardner/ns_reni)**|Project Repo - https://github.com/JADGardner/ns_reni. arXiv admin   note: substantial text overlap with arXiv:2206.03858|James A. D. Gardner et.al.|
|**2023-11-15**|**[Applications of Computer Vision in Autonomous Vehicles: Methods, Challenges and Future Directions](http://arxiv.org/abs/2311.09093v3)**|None|None|None|Xingshuai Dong et.al.|
|**2023-11-14**|**[Learning based Deep Disentangling Light Field Reconstruction and Disparity Estimation Application](http://arxiv.org/abs/2311.08129v1)**|None|None|None|Langqing Shi et.al.|
|**2023-11-13**|**[MonoDiffusion: Self-Supervised Monocular Depth Estimation Using Diffusion Model](http://arxiv.org/abs/2311.07198v1)**|None|**[link](https://github.com/shuweishao/monodiffusion)**|10 pages, 8 figures|Shuwei Shao et.al.|
|**2023-11-13**|**[NDDepth: Normal-Distance Assisted Monocular Depth Estimation and Completion](http://arxiv.org/abs/2311.07166v1)**|None|**[link](https://github.com/ShuweiShao/NDDepth)**|Extension of previous work arXiv:2309.10592|Shuwei Shao et.al.|
|**2023-11-10**|**[MonoProb: Self-Supervised Monocular Depth Estimation with Interpretable Uncertainty](http://arxiv.org/abs/2311.06137v1)**|None|**[link](https://github.com/cea-list/monoprob)**|Accepted at WACV 2024|Rémi Marsal et.al.|
|**2023-11-09**|**[PolyMaX: General Dense Prediction with Mask Transformer](http://arxiv.org/abs/2311.05770v1)**|None|**[link](https://github.com/google-research/deeplab2)**|WACV 2024|Xuan Yang et.al.|
|**2023-11-08**|**[Leveraging a realistic synthetic database to learn Shape-from-Shading for estimating the colon depth in colonoscopy images](http://arxiv.org/abs/2311.05021v1)**|Ruano, J., Gomez, M., Romero, E., & Manzanera, A. (2024).   Leveraging a realistic synthetic database to learn Shape-from-Shading for   estimating the colon depth in colonoscopy images. Computerized Medical   Imaging and Graphics, 102390|None|None|Josué Ruano et.al.|
|**2023-11-07**|**[Analysis of NaN Divergence in Training Monocular Depth Estimation Model](http://arxiv.org/abs/2311.03938v1)**|None|None|10 pages, 3 figures|Bum Jun Kim et.al.|
|**2023-11-06**|**[TSP-Transformer: Task-Specific Prompts Boosted Transformer for Holistic Scene Understanding](http://arxiv.org/abs/2311.03427v1)**|None|**[link](https://github.com/tb2-sy/tsp-transformer)**|WACV 2024|Shuo Wang et.al.|
|**2023-11-04**|**[Continual Learning of Unsupervised Monocular Depth from Videos](http://arxiv.org/abs/2311.02393v1)**|None|**[link](https://github.com/neurai-lab/cude-monodepthcl)**|Accepted at IEEE/CVF Winter Conference on Applications of Computer   Vision (WACV 2024)|Hemang Chawla et.al.|
|**2023-11-03**|**[Bridging the Gap between Multi-focus and Multi-modal: A Focused Integration Framework for Multi-modal Image Fusion](http://arxiv.org/abs/2311.01886v2)**|None|**[link](https://github.com/ixilai/mfif-mmif)**|Accepted to IEEE/CVF Winter Conference on Applications of Computer   Vision (WACV) 2024|Xilai Li et.al.|
|**2023-11-02**|**[Learning to Adapt CLIP for Few-Shot Monocular Depth Estimation](http://arxiv.org/abs/2311.01034v1)**|None|None|Accepted by WACV 2024|Xueting Hu et.al.|
|**2023-10-29**|**[Dynamo-Depth: Fixing Unsupervised Depth Estimation for Dynamical Scenes](http://arxiv.org/abs/2310.18887v1)**|None|None|NeurIPS 2023|Yihong Sun et.al.|
|**2023-10-26**|**[Learning depth from monocular video sequences](http://arxiv.org/abs/2310.17156v1)**|None|None|None|Zhenwei Luo et.al.|
|**2023-10-26**|**[Deep Imbalanced Regression via Hierarchical Classification Adjustment](http://arxiv.org/abs/2310.17154v1)**|None|None|14 pages, 5 figures|Haipeng Xiong et.al.|
|**2023-10-25**|**[PERF: Panoramic Neural Radiance Field from a Single Panorama](http://arxiv.org/abs/2310.16831v2)**|None|**[link](https://github.com/perf-project/PeRF)**|Project Page: https://perf-project.github.io/ , Code:   https://github.com/perf-project/PeRF|Guangcong Wang et.al.|
|**2023-10-25**|**[Metrically Scaled Monocular Depth Estimation through Sparse Priors for Underwater Robots](http://arxiv.org/abs/2310.16750v1)**|None|**[link](https://github.com/ebnerluca/uw_depth)**|Submitted to ICRA 2024|Luca Ebner et.al.|
|**2023-10-25**|**[Towards Explainability in Monocular Depth Estimation](http://arxiv.org/abs/2310.16457v1)**|None|None|None|Vasileios Arampatzakis et.al.|
|**2023-10-24**|**[iNVS: Repurposing Diffusion Inpainters for Novel View Synthesis](http://arxiv.org/abs/2310.16167v1)**|None|None|Accepted to SIGGRAPH Asia, 2023 (Conference Papers)|Yash Kant et.al.|
|**2023-10-24**|**[G2-MonoDepth: A General Framework of Generalized Depth Inference from Monocular RGB+X Data](http://arxiv.org/abs/2310.15422v1)**|None|**[link](https://github.com/wang-xjtu/g2-monodepth)**|18 pages, 16 figures|Haotian Wang et.al.|
|**2023-10-23**|**[RoboDepth: Robust Out-of-Distribution Depth Estimation under Corruptions](http://arxiv.org/abs/2310.15171v1)**|None|**[link](https://github.com/ldkong1205/robodepth)**|NeurIPS 2023; 45 pages, 25 figures, 13 tables; Code at   https://github.com/ldkong1205/RoboDepth|Lingdong Kong et.al.|
|**2023-10-22**|**[Mobile AR Depth Estimation: Challenges & Prospects -- Extended Version](http://arxiv.org/abs/2310.14437v1)**|None|None|None|Ashkan Ganj et.al.|
|**2023-10-22**|**[A Quantitative Evaluation of Dense 3D Reconstruction of Sinus Anatomy from Monocular Endoscopic Video](http://arxiv.org/abs/2310.14364v1)**|None|None|None|Jan Emily Mangulabnan et.al.|
|**2023-10-22**|**[Guidance system for Visually Impaired Persons using Deep Learning and Optical flow](http://arxiv.org/abs/2310.14239v1)**|None|None|None|Shwetang Dubey et.al.|
|**2023-10-18**|**[Towards Abdominal 3-D Scene Rendering from Laparoscopy Surgical Videos using NeRFs](http://arxiv.org/abs/2310.11645v1)**|None|None|The Version of Record of this contribution is published in MLMI 2023   Part I, and is available online at   https://doi.org/10.1007/978-3-031-45673-2_9|Khoa Tuan Nguyen et.al.|
|**2023-10-17**|**[FocDepthFormer: Transformer with LSTM for Depth Estimation from Focus](http://arxiv.org/abs/2310.11178v1)**|None|None|20 pages, 18 figures, journal paper|Xueyang Kang et.al.|
|**2023-10-12**|**[Pseudo-Generalized Dynamic View Synthesis from a Video](http://arxiv.org/abs/2310.08587v3)**|None|None|ICLR 2024; Originally titled as "Is Generalized Dynamic Novel View   Synthesis from Monocular Videos Possible Today?"; Project page:   https://xiaoming-zhao.github.io/projects/pgdvs|Xiaoming Zhao et.al.|
|**2023-10-12**|**[EC-Depth: Exploring the consistency of self-supervised monocular depth estimation in challenging scenes](http://arxiv.org/abs/2310.08044v2)**|None|**[link](https://github.com/RuijieZhu94/EC-Depth)**|Project page: https://ruijiezhu94.github.io/ECDepth_page|Ziyang Song et.al.|
|**2023-10-11**|**[Multi-Task Learning-Enabled Automatic Vessel Draft Reading for Intelligent Maritime Surveillance](http://arxiv.org/abs/2310.07212v1)**|None|None|12 pages,11 figures, submitted to IEEE T-ITS|Jingxiang Qu et.al.|
|**2023-09-16**|**[DEUX: Active Exploration for Learning Unsupervised Depth Perception](http://arxiv.org/abs/2310.06164v1)**|None|None|None|Marvin Chancán et.al.|
|**2023-10-09**|**[WeatherDepth: Curriculum Contrastive Learning for Self-Supervised Depth Estimation under Adverse Weather Conditions](http://arxiv.org/abs/2310.05556v2)**|ICRA 2024|**[link](https://github.com/wangjiyuan9/weatherdepth)**|6 pages, accept by ICRA 2024|Jiyuan Wang et.al.|
|**2023-10-07**|**[Federated Self-Supervised Learning of Monocular Depth Estimators for Autonomous Vehicles](http://arxiv.org/abs/2310.04837v1)**|None|None|16 pages, 8 figures, journal preprint|Elton F. de S. Soares et.al.|
|**2023-10-06**|**[MeSa: Masked, Geometric, and Supervised Pre-training for Monocular Depth Estimation](http://arxiv.org/abs/2310.04551v1)**|None|None|None|Muhammad Osama Khan et.al.|
|**2023-10-06**|**[Sub-token ViT Embedding via Stochastic Resonance Transformers](http://arxiv.org/abs/2310.03967v2)**|None|**[link](https://github.com/donglao/srt)**|None|Dong Lao et.al.|
|**2023-10-05**|**[FreeReg: Image-to-Point Cloud Registration Leveraging Pretrained Diffusion Models and Monocular Depth Estimators](http://arxiv.org/abs/2310.03420v2)**|None|**[link](https://github.com/WHU-USI3DV/FreeReg)**|CameraReady version for ICLR 2024. Project Page:   https://whu-usi3dv.github.io/FreeReg/|Haiping Wang et.al.|
|**2023-10-03**|**[RSRD: A Road Surface Reconstruction Dataset and Benchmark for Safe and Comfortable Autonomous Driving](http://arxiv.org/abs/2310.02262v1)**|None|None|None|Tong Zhao et.al.|
|**2023-10-03**|**[Selective Feature Adapter for Dense Vision Transformers](http://arxiv.org/abs/2310.01843v1)**|None|None|None|Xueqing Deng et.al.|
|**2023-10-03**|**[Skin the sheep not only once: Reusing Various Depth Datasets to Drive the Learning of Optical Flow](http://arxiv.org/abs/2310.01833v1)**|None|None|None|Sheng-Chi Huang et.al.|
|**2023-10-02**|**[Multi-task Learning with 3D-Aware Regularization](http://arxiv.org/abs/2310.00986v1)**|None|**[link](https://github.com/vico-uoe/mtpsl)**|3D-aware Multi-task Learning, Code will be available at   https://github.com/VICO-UoE/MTPSL|Wei-Hong Li et.al.|
|**2023-09-30**|**[InstructCV: Instruction-Tuned Text-to-Image Diffusion Models as Vision Generalists](http://arxiv.org/abs/2310.00390v3)**|None|**[link](https://github.com/AlaaLab/InstructCV)**|ICLR 2024; Code is available at https://github.com/AlaaLab/InstructCV|Yulu Gan et.al.|
|**2023-09-29**|**[Text-image Alignment for Diffusion-based Perception](http://arxiv.org/abs/2310.00031v3)**|None|**[link](https://github.com/damaggu/tadp)**|Project page: https://www.vision.caltech.edu/tadp/, Code page:   github.com/damaggu/TADP|Neehar Kondapaneni et.al.|
|**2023-09-07**|**[Joint Self-supervised Depth and Optical Flow Estimation towards Dynamic Objects](http://arxiv.org/abs/2310.00011v1)**|None|None|None|Zhengyang Lu et.al.|
|**2023-09-29**|**[IFAST: Weakly Supervised Interpretable Face Anti-spoofing from Single-shot Binocular NIR Images](http://arxiv.org/abs/2309.17399v1)**|None|None|None|Jiancheng Huang et.al.|
|**2023-09-29**|**[GSDC Transformer: An Efficient and Effective Cue Fusion for Monocular Multi-Frame Depth Estimation](http://arxiv.org/abs/2309.17059v2)**|None|None|None|Naiyu Fang et.al.|
|**2023-09-28**|**[Gated Cross-Attention Network for Depth Completion](http://arxiv.org/abs/2309.16301v2)**|None|None|None|Xiaogang Jia et.al.|
|**2023-09-26**|**[GasMono: Geometry-Aided Self-Supervised Monocular Depth Estimation for Indoor Scenes](http://arxiv.org/abs/2309.16019v1)**|None|**[link](https://github.com/zxcqlf/gasmono)**|ICCV 2023. Code: https://github.com/zxcqlf/GasMono|Chaoqiang Zhao et.al.|
|**2023-09-27**|**[InfraParis: A multi-modal and multi-task autonomous driving dataset](http://arxiv.org/abs/2309.15751v2)**|None|**[link](https://github.com/ENSTA-U2IS-AI/Multimodal_Deep_segmentation)**|15 pages, 7 figures. Accepted at WACV 2024|Gianni Franchi et.al.|
|**2023-09-27**|**[Finite Scalar Quantization: VQ-VAE Made Simple](http://arxiv.org/abs/2309.15505v2)**|None|**[link](https://github.com/google-research/google-research)**|Code:   https://github.com/google-research/google-research/tree/master/fsq|Fabian Mentzer et.al.|
|**2023-09-26**|**[M$^{3}$3D: Learning 3D priors using Multi-Modal Masked Autoencoders for 2D image and video understanding](http://arxiv.org/abs/2309.15313v1)**|None|None|None|Muhammad Abdullah Jamal et.al.|
|**2023-09-26**|**[ADU-Depth: Attention-based Distillation with Uncertainty Modeling for Depth Estimation](http://arxiv.org/abs/2309.14744v1)**|None|None|accepted by CoRL 2023|Zizhang Wu et.al.|
|**2023-09-25**|**[IEBins: Iterative Elastic Bins for Monocular Depth Estimation](http://arxiv.org/abs/2309.14137v1)**|None|**[link](https://github.com/shuweishao/iebins)**|Accepted by NeurIPS 2023|Shuwei Shao et.al.|
|**2023-09-25**|**[DISeR: Designing Imaging Systems with Reinforcement Learning](http://arxiv.org/abs/2309.13851v1)**|None|None|ICCV 2023. Project Page: https://tzofi.github.io/diser|Tzofi Klinghoffer et.al.|
|**2023-09-24**|**[InSpaceType: Reconsider Space Type in Indoor Monocular Depth Estimation](http://arxiv.org/abs/2309.13516v2)**|None|**[link](https://github.com/DepthComputation/InSpaceType_Benchmark)**|Add Depth-Anything|Cho-Ying Wu et.al.|
|**2023-09-22**|**[SRFNet: Monocular Depth Estimation with Fine-grained Structure via Spatial Reliability-oriented Fusion of Frames and Events](http://arxiv.org/abs/2309.12842v1)**|None|None|None|Tianbo Pan et.al.|
|**2023-09-21**|**[SANPO: A Scene Understanding, Accessibility, Navigation, Pathfinding, Obstacle Avoidance Dataset](http://arxiv.org/abs/2309.12172v1)**|None|None|10 pages plus additional references. 13 figures|Sagar M. Waghmare et.al.|
|**2023-09-20**|**[BroadBEV: Collaborative LiDAR-camera Fusion for Broad-sighted Bird's Eye View Map Construction](http://arxiv.org/abs/2309.11119v4)**|None|None|None|Minsu Kim et.al.|
|**2023-09-20**|**[Dense 2D-3D Indoor Prediction with Sound via Aligned Cross-Modal Distillation](http://arxiv.org/abs/2309.11081v1)**|None|**[link](https://github.com/hs-yn/daps)**|Published to ICCV2023|Heeseung Yun et.al.|
|**2023-09-19**|**[NDDepth: Normal-Distance Assisted Monocular Depth Estimation](http://arxiv.org/abs/2309.10592v2)**|None|None|Accepted by ICCV 2023 (Oral)|Shuwei Shao et.al.|
|**2023-09-18**|**[GEDepth: Ground Embedding for Monocular Depth Estimation](http://arxiv.org/abs/2309.09975v1)**|None|**[link](https://github.com/qcraftai/gedepth)**|ICCV 2023|Xiaodong Yang et.al.|
|**2023-09-18**|**[Robust Geometry-Preserving Depth Estimation Using Differentiable Rendering](http://arxiv.org/abs/2309.09724v1)**|None|None|Accepted by ICCV2023|Chi Zhang et.al.|
|**2023-09-17**|**[Deep Neighbor Layer Aggregation for Lightweight Self-Supervised Monocular Depth Estimation](http://arxiv.org/abs/2309.09272v2)**|None|**[link](https://github.com/boyagesmile/dna-depth)**|None|Wang Boya et.al.|
|**2023-09-15**|**[X-PDNet: Accurate Joint Plane Instance Segmentation and Monocular Depth Estimation with Cross-Task Distillation and Boundary Correction](http://arxiv.org/abs/2309.08424v2)**|None|**[link](https://github.com/caodinhduc/x-pdnet-official)**|Accepted to BMVC 2023|Cao Dinh Duc et.al.|
|**2023-09-14**|**[Depth Estimation from a Single Optical Encoded Image using a Learned Colored-Coded Aperture](http://arxiv.org/abs/2309.08033v1)**|None|None|None|Jhon Lopez et.al.|
|**2023-09-12**|**[AmodalSynthDrive: A Synthetic Amodal Perception Dataset for Autonomous Driving](http://arxiv.org/abs/2309.06547v2)**|None|None|None|Ahmed Rida Sekkat et.al.|
|**2023-09-11**|**[Towards Better Data Exploitation in Self-Supervised Monocular Depth Estimation](http://arxiv.org/abs/2309.05254v3)**|None|**[link](https://github.com/LiuJF1226/BDEdepth)**|8 pages, 6 figures, accepted by IEEE Robotics and Automation Letters   (RA-L 2023)|Jinfeng Liu et.al.|
|**2023-09-08**|**[Robot Localization and Mapping Final Report -- Sequential Adversarial Learning for Self-Supervised Deep Visual Odometry](http://arxiv.org/abs/2309.04147v1)**|None|None|None|Akankshya Kar et.al.|
|**2023-09-07**|**[SimpleNeRF: Regularizing Sparse Input Neural Radiance Fields with Simpler Solutions](http://arxiv.org/abs/2309.03955v2)**|None|None|SIGGRAPH Asia 2023|Nagabhushan Somraj et.al.|
|**2023-09-06**|**[Bayes' Rays: Uncertainty Quantification for Neural Radiance Fields](http://arxiv.org/abs/2309.03185v1)**|None|**[link](https://github.com/BayesRays/BayesRays)**|None|Lily Goli et.al.|
|**2023-09-02**|**[Two-in-One Depth: Bridging the Gap Between Monocular and Binocular Self-supervised Depth Estimation](http://arxiv.org/abs/2309.00933v1)**|None|**[link](https://github.com/zm-zhou/tio-depth_pytorch)**|Accepted to ICCV 2023|Zhengming Zhou et.al.|
|**2023-09-01**|**[SQLdepth: Generalizable Self-Supervised Fine-Structured Monocular Depth Estimation](http://arxiv.org/abs/2309.00526v1)**|None|None|14 pages, 9 figures|Youhong Wang et.al.|
|**2023-08-29**|**[Learning to Upsample by Learning to Sample](http://arxiv.org/abs/2308.15085v1)**|None|**[link](https://github.com/tiny-smart/dysample)**|Accepted by ICCV 2023|Wenze Liu et.al.|
|**2023-08-28**|**[Semi-Supervised Semantic Depth Estimation using Symbiotic Transformer and NearFarMix Augmentation](http://arxiv.org/abs/2308.14400v1)**|None|None|Accepted at WACV 2024|Md Awsafur Rahman et.al.|
|**2023-08-27**|**[Depth self-supervision for single image novel view synthesis](http://arxiv.org/abs/2308.14108v1)**|None|**[link](https://github.com/johnminelli/twowaysynth)**|None|Giovanni Minelli et.al.|
|**2023-08-27**|**[Calibrating Panoramic Depth Estimation for Practical Localization and Mapping](http://arxiv.org/abs/2308.14005v2)**|None|**[link](https://github.com/82magnolia/panoramic-depth-calibration)**|Accepted to ICCV 2023|Junho Kim et.al.|
|**2023-08-24**|**[Panoptic-Depth Color Map for Combination of Depth and Image Segmentation](http://arxiv.org/abs/2308.12937v1)**|None|None|None|Jia-Quan Yu et.al.|
|**2023-08-22**|**[WS-SfMLearner: Self-supervised Monocular Depth and Ego-motion Estimation on Surgical Videos with Unknown Camera Parameters](http://arxiv.org/abs/2308.11776v2)**|None|None|Accepted by SPIE 2024|Ange Lou et.al.|
|**2023-08-22**|**[SAMSNeRF: Segment Anything Model (SAM) Guides Dynamic Surgical Scene Reconstruction by Neural Radiance Field (NeRF)](http://arxiv.org/abs/2308.11774v2)**|None|None|Accepted by SPIE 2024|Ange Lou et.al.|
|**2023-08-21**|**[A step towards understanding why classification helps regression](http://arxiv.org/abs/2308.10603v1)**|None|**[link](https://github.com/silvialaurapintea/reg-cls)**|Accepted at ICCV-2023|Silvia L. Pintea et.al.|
|**2023-08-21**|**[Real-time Monocular Depth Estimation on Embedded Systems](http://arxiv.org/abs/2308.10569v2)**|None|None|7 pages, ICIP2024 Accepted|Cheng Feng et.al.|
|**2023-08-21**|**[LightDepth: Single-View Depth Self-Supervision from Illumination Decline](http://arxiv.org/abs/2308.10525v2)**|None|None|None|Javier Rodríguez-Puigvert et.al.|
|**2023-08-19**|**[AltNeRF: Learning Robust Neural Radiance Field via Alternating Depth-Pose Optimization](http://arxiv.org/abs/2308.10001v2)**|None|None|Accepted by AAAI-24|Kun Wang et.al.|
|**2023-08-19**|**[Textureless-aware Segmentation and Correlative Refinement Guided Multi-View Stereo](http://arxiv.org/abs/2308.09990v2)**|None|None|None|Zhenlong Yuan et.al.|
|**2023-08-18**|**[Robust Monocular Depth Estimation under Challenging Conditions](http://arxiv.org/abs/2308.09711v1)**|None|None|ICCV 2023. Source code and data: https://md4all.github.io|Stefano Gasperini et.al.|
|**2023-08-17**|**[Discretization-Induced Dirichlet Posterior for Robust Uncertainty Quantification on Regression](http://arxiv.org/abs/2308.09065v2)**|None|**[link](https://github.com/ensta-u2is/dido)**|23 pages with main paper and supplymentary material. Accepted at AAAI   2024|Xuanlong Yu et.al.|
|**2023-08-17**|**[ARAI-MVSNet: A multi-view stereo depth estimation network with adaptive depth range and depth interval](http://arxiv.org/abs/2308.09022v1)**|None|**[link](https://github.com/zs670980918/arai-mvsnet)**|None|Song Zhang et.al.|
|**2023-08-16**|**[Improving Depth Gradient Continuity in Transformers: A Comparative Study on Monocular Depth Estimation with CNN](http://arxiv.org/abs/2308.08333v3)**|None|None|None|Jiawei Yao et.al.|
|**2023-08-14**|**[DS-Depth: Dynamic and Static Depth Estimation via a Fusion Cost Volume](http://arxiv.org/abs/2308.07225v1)**|None|**[link](https://github.com/xingy038/ds-depth)**|None|Xingyu Miao et.al.|
|**2023-08-11**|**[DatasetDM: Synthesizing Data with Perception Annotations Using Diffusion Models](http://arxiv.org/abs/2308.06160v2)**|Proc. Advances In Neural Information Processing Systems (NeurIPS   2023)|**[link](https://github.com/showlab/datasetdm)**|None|Weijia Wu et.al.|
|**2023-08-11**|**[Out-of-Distribution Detection for Monocular Depth Estimation](http://arxiv.org/abs/2308.06072v1)**|None|**[link](https://github.com/jhornauer/mde_ood)**|Accepted to ICCV 2023|Julia Hornauer et.al.|
|**2023-08-10**|**[FrozenRecon: Pose-free 3D Scene Reconstruction with Frozen Depth Models](http://arxiv.org/abs/2308.05733v1)**|None|None|Accepted to ICCV 2023. Project webpage is at:   https://aim-uofa.github.io/FrozenRecon/|Guangkai Xu et.al.|
|**2023-08-10**|**[Self-Supervised Monocular Depth Estimation by Direction-aware Cumulative Convolution Network](http://arxiv.org/abs/2308.05605v1)**|None|**[link](https://github.com/wencheng256/daccn)**|ICCV2023|Wencheng Han et.al.|
|**2023-08-06**|**[Syn-Mediverse: A Multimodal Synthetic Dataset for Intelligent Scene Understanding of Healthcare Facilities](http://arxiv.org/abs/2308.03193v1)**|None|None|None|Rohit Mohan et.al.|
|**2023-08-04**|**[EndoDepthL: Lightweight Endoscopic Monocular Depth Estimation with CNN-Transformer](http://arxiv.org/abs/2308.02716v2)**|None|None|None|Yangke Li et.al.|
|**2023-08-04**|**[Diffusion-Augmented Depth Prediction with Sparse Annotations](http://arxiv.org/abs/2308.02283v1)**|None|None|Accepted by ACM MM'2023|Jiaqi Li et.al.|
|**2023-08-04**|**[Robust Self-Supervised Extrinsic Self-Calibration](http://arxiv.org/abs/2308.02153v2)**|The IEEE/RSJ International Conference on Intelligent Robots and   Systems (IROS), 2023|None|Project page: https://sites.google.com/view/tri-sesc|Takayuki Kanai et.al.|
|**2023-08-02**|**[Hand tracking for clinical applications: validation of the Google MediaPipe Hand (GMH) and the depth-enhanced GMH-D frameworks](http://arxiv.org/abs/2308.01088v1)**|None|**[link](https://github.com/gianluca-amprimo/gmh-d)**|None|Gianluca Amprimo et.al.|
|**2023-07-31**|**[Digging Into Uncertainty-based Pseudo-label for Robust Stereo Matching](http://arxiv.org/abs/2307.16509v1)**|None|**[link](https://github.com/gallenszl/ucfnet)**|Accepted by TPAMI|Zhelun Shen et.al.|
|**2023-07-27**|**[The RoboDepth Challenge: Methods and Advancements Towards Robust Depth Estimation](http://arxiv.org/abs/2307.15061v1)**|None|**[link](https://github.com/ldkong1205/robodepth)**|Technical Report; 65 pages, 34 figures, 24 tables; Code at   https://github.com/ldkong1205/RoboDepth|Lingdong Kong et.al.|
|**2023-07-27**|**[Learning Depth Estimation for Transparent and Mirror Surfaces](http://arxiv.org/abs/2307.15052v1)**|None|None|Accepted at ICCV 2023. Project Page:   https://cvlab-unibo.github.io/Depth4ToM|Alex Costanzino et.al.|
|**2023-07-27**|**[Towards Deeply Unified Depth-aware Panoptic Segmentation with Bi-directional Guidance Learning](http://arxiv.org/abs/2307.14786v2)**|None|**[link](https://github.com/jwh97nn/DeepDPS)**|to be published in ICCV 2023|Junwen He et.al.|
|**2023-07-27**|**[FS-Depth: Focal-and-Scale Depth Estimation from a Single Image in Unseen Indoor Scene](http://arxiv.org/abs/2307.14624v1)**|None|None|None|Chengrui Wei et.al.|
|**2023-07-27**|**[NeRF-Det: Learning Geometry-Aware Volumetric Representation for Multi-View 3D Object Detection](http://arxiv.org/abs/2307.14620v1)**|None|**[link](https://github.com/open-mmlab/mmdetection3d)**|Accepted by ICCV 2023|Chenfeng Xu et.al.|
|**2023-07-26**|**[MiDaS v3.1 -- A Model Zoo for Robust Monocular Relative Depth Estimation](http://arxiv.org/abs/2307.14460v1)**|None|**[link](https://github.com/isl-org/MiDaS)**|14 pages, 2 figures|Reiner Birkl et.al.|
|**2023-07-26**|**[MAMo: Leveraging Memory and Attention for Monocular Video Depth Estimation](http://arxiv.org/abs/2307.14336v2)**|None|None|Accepted at ICCV 2023|Rajeev Yasarla et.al.|
|**2023-07-25**|**[PlaneRecTR: Unified Query Learning for 3D Plane Recovery from a Single View](http://arxiv.org/abs/2307.13756v2)**|None|**[link](https://github.com/sjingjia/planerectr)**|To be published in Proceedings of IEEE International Conference on   Computer Vision (ICCV 2023). Camera Ready Version. Codes:   https://github.com/SJingjia/PlaneRecTR , Video: https://youtu.be/YBB7totHGJg|Jingjia Shi et.al.|
|**2023-07-24**|**[LiDAR Meta Depth Completion](http://arxiv.org/abs/2307.12761v2)**|None|**[link](https://github.com/wbkit/reslan)**|Accepted at IROS 2023, v2 has updated author list and fixed a figure   caption|Wolfgang Boettcher et.al.|
|**2023-07-23**|**[FDCT: Fast Depth Completion for Transparent Objects](http://arxiv.org/abs/2307.12274v2)**|IEEE Robotics and Automation Letters (RA-L), 2023|**[link](https://github.com/nonmy/fdct)**|9pages,7figures|Tianan Li et.al.|
|**2023-07-20**|**[Metric3D: Towards Zero-shot Metric 3D Prediction from A Single Image](http://arxiv.org/abs/2307.10984v1)**|None|**[link](https://github.com/yvanyin/metric3d)**|Accepted to ICCV 2023. Won the championship in the 2nd Monocular   Depth Estimation Challenge. The code is available at   https://github.com/YvanYin/Metric3D|Wei Yin et.al.|
|**2023-07-20**|**[OCTraN: 3D Occupancy Convolutional Transformer Network in Unstructured Traffic Scenarios](http://arxiv.org/abs/2307.10934v1)**|None|None|This work was accepted as a spotlight presentation at the   Transformers for Vision Workshop @CVPR 2023|Aditya Nalgunda Ganesh et.al.|
|**2023-07-20**|**[Kick Back & Relax: Learning to Reconstruct the World by Watching SlowTV](http://arxiv.org/abs/2307.10713v1)**|None|**[link](https://github.com/jspenmar/slowtv_monodepth)**|Accepted to ICCV2023|Jaime Spencer et.al.|
|**2023-07-16**|**[RayMVSNet++: Learning Ray-based 1D Implicit Fields for Accurate Multi-View Stereo](http://arxiv.org/abs/2307.10233v1)**|None|None|IEEE Transactions on Pattern Analysis and Machine Intelligence. arXiv   admin note: substantial text overlap with arXiv:2204.01320|Yifei Shi et.al.|
|**2023-07-19**|**[Measuring and Modeling Uncertainty Degree for Monocular Depth Estimation](http://arxiv.org/abs/2307.09929v1)**|None|None|None|Mochu Xiang et.al.|
|**2023-07-17**|**[Neural Video Depth Stabilizer](http://arxiv.org/abs/2307.08695v2)**|None|**[link](https://github.com/raymondwang987/nvds)**|Accepted by ICCV2023|Yiran Wang et.al.|
|**2023-07-17**|**[Self-supervised Monocular Depth Estimation: Let's Talk About The Weather](http://arxiv.org/abs/2307.08357v1)**|None|None|ICCV'23|Kieran Saunders et.al.|
|**2023-07-17**|**[On Point Affiliation in Feature Upsampling](http://arxiv.org/abs/2307.08198v1)**|None|**[link](https://github.com/tiny-smart/sapa)**|17 pages. Extended version of NeurIPS 2022 paper "SAPA:   Similarity-Aware Point Affiliation for Feature Upsampling" at   arXiv:2209.12866v1. arXiv admin note: text overlap with arXiv:2209.12866|Wenze Liu et.al.|
|**2023-07-16**|**[Multi-Object Discovery by Low-Dimensional Object Motion](http://arxiv.org/abs/2307.08027v1)**|None|None|ICCV 2023|Sadra Safadoust et.al.|
|**2023-07-09**|**[TransPose: A Transformer-based 6D Object Pose Estimation Network with Depth Refinement](http://arxiv.org/abs/2307.05561v1)**|None|None|None|Mahmoud Abdulsalam et.al.|
|**2023-07-11**|**[DFR: Depth from Rotation by Uncalibrated Image Rectification with Latitudinal Motion Assumption](http://arxiv.org/abs/2307.05129v1)**|None|**[link](https://github.com/zhangtaxue/dfr)**|None|Yongcong Zhang et.al.|
|**2023-07-07**|**[Depth Estimation Analysis of Orthogonally Divergent Fisheye Cameras with Distortion Removal](http://arxiv.org/abs/2307.03602v1)**|None|None|None|Matvei Panteleev et.al.|
|**2023-07-05**|**[SVDM: Single-View Diffusion Model for Pseudo-Stereo 3D Object Detection](http://arxiv.org/abs/2307.02270v1)**|None|None|arXiv admin note: text overlap with arXiv:2203.02112,   arXiv:2303.01469 by other authors|Yuguang Shi et.al.|
|**2023-07-04**|**[Consistent Multimodal Generation via A Unified GAN Framework](http://arxiv.org/abs/2307.01425v1)**|None|None|In review|Zhen Zhu et.al.|
|**2023-06-30**|**[FlipNeRF: Flipped Reflection Rays for Few-shot Novel View Synthesis](http://arxiv.org/abs/2306.17723v4)**|None|**[link](https://github.com/shawn615/FlipNeRF)**|ICCV 2023. Project Page: https://shawn615.github.io/flipnerf/|Seunghyeon Seo et.al.|
|**2023-06-29**|**[Towards Zero-Shot Scale-Aware Monocular Depth Estimation](http://arxiv.org/abs/2306.17253v1)**|None|None|Project page: https://sites.google.com/view/tri-zerodepth|Vitor Guizilini et.al.|
|**2023-06-27**|**[MIMIC: Masked Image Modeling with Image Correspondences](http://arxiv.org/abs/2306.15128v4)**|None|**[link](https://github.com/raivnlab/mimic)**|None|Kalyani Marathe et.al.|
|**2023-06-26**|**[Learnable Differencing Center for Nighttime Depth Perception](http://arxiv.org/abs/2306.14538v4)**|None|None|8 pages|Zhiqiang Yan et.al.|
|**2023-06-22**|**[Continuous Online Extrinsic Calibration of Fisheye Camera and LiDAR](http://arxiv.org/abs/2306.13240v1)**|None|None|4 pages|Jack Borer et.al.|
|**2023-06-20**|**[Self-supervised Multi-task Learning Framework for Safety and Health-Oriented Connected Driving Environment Perception using Onboard Camera](http://arxiv.org/abs/2306.11822v1)**|None|None|None|Shaocheng Jia et.al.|
|**2023-06-20**|**[BEVScope: Enhancing Self-Supervised Depth Estimation Leveraging Bird's-Eye-View in Dynamic Scenarios](http://arxiv.org/abs/2306.11598v1)**|None|None|None|Yucheng Mao et.al.|
|**2023-06-20**|**[Depth and DOF Cues Make A Better Defocus Blur Detector](http://arxiv.org/abs/2306.11334v1)**|None|**[link](https://github.com/yuxinjin-whu/d-dffnet)**|Code: https://github.com/yuxinjin-whu/D-DFFNet|Yuxin Jin et.al.|
|**2023-06-19**|**[Tame a Wild Camera: In-the-Wild Monocular Camera Calibration](http://arxiv.org/abs/2306.10988v2)**|NeurIPS 2023|**[link](https://github.com/shngjz/wildcamera)**|None|Shengjie Zhu et.al.|
|**2023-06-19**|**[Understanding Depth Map Progressively: Adaptive Distance Interval Separation for Monocular 3d Object Detection](http://arxiv.org/abs/2306.10921v1)**|None|None|None|Xianhui Cheng et.al.|
|**2023-06-16**|**[C2F2NeUS: Cascade Cost Frustum Fusion for High Fidelity and Generalizable Neural Surface Reconstruction](http://arxiv.org/abs/2306.10003v2)**|None|None|Accepted by ICCV2023|Luoyuan Xu et.al.|
|**2023-06-14**|**[Predict to Detect: Prediction-guided 3D Object Detection using Sequential Images](http://arxiv.org/abs/2306.08528v3)**|None|**[link](https://github.com/sanmin0312/P2D)**|ICCV 2023, Code: https://github.com/sanmin0312/P2D|Sanmin Kim et.al.|
|**2023-06-09**|**[Lightweight Monocular Depth Estimation via Token-Sharing Transformer](http://arxiv.org/abs/2306.05682v1)**|None|None|ICRA 2023|Dong-Jae Lee et.al.|
|**2023-06-08**|**[Tracking Objects with 3D Representation from Videos](http://arxiv.org/abs/2306.05416v1)**|None|None|Technical report|Jiawei He et.al.|
|**2023-06-08**|**[SparseTrack: Multi-Object Tracking by Performing Scene Decomposition based on Pseudo-Depth](http://arxiv.org/abs/2306.05238v2)**|None|**[link](https://github.com/hustvl/sparsetrack)**|12 pages, 8 figures|Zelin Liu et.al.|
|**2023-06-08**|**[A Dynamic Feature Interaction Framework for Multi-task Visual Perception](http://arxiv.org/abs/2306.05061v1)**|None|None|Accepted by International Journal of Computer Vision. arXiv admin   note: text overlap with arXiv:2011.09796|Yuling Xi et.al.|
|**2023-06-05**|**[Single-Stage 3D Geometry-Preserving Depth Estimation Model Training on Dataset Mixtures with Uncalibrated Stereo Data](http://arxiv.org/abs/2306.02878v1)**|CVPR 2022|None|None|Nikolay Patakin et.al.|
|**2023-06-02**|**[Towards In-context Scene Understanding](http://arxiv.org/abs/2306.01667v2)**|None|None|None|Ivana Balažević et.al.|
|**2023-06-02**|**[PanoGRF: Generalizable Spherical Radiance Fields for Wide-baseline Panoramas](http://arxiv.org/abs/2306.01531v2)**|None|None|accepted to NeurIPS2023; Project Page:   https://thucz.github.io/PanoGRF/|Zheng Chen et.al.|
|**2023-05-31**|**[A technique to jointly estimate depth and depth uncertainty for unmanned aerial vehicles](http://arxiv.org/abs/2305.19780v1)**|None|**[link](https://github.com/michael-fonder/m4depthu)**|The code is available at https://github.com/michael-fonder/M4DepthU|Michaël Fonder et.al.|
|**2023-05-30**|**[DaRF: Boosting Radiance Fields from Sparse Inputs with Monocular Depth Adaptation](http://arxiv.org/abs/2305.19201v2)**|None|**[link](https://github.com/KU-CVLAB/DaRF)**|To appear at NeurIPS 2023. Project Page:   https://ku-cvlab.github.io/DaRF/|Jiuhn Song et.al.|
|**2023-05-30**|**[Independent Component Alignment for Multi-Task Learning](http://arxiv.org/abs/2305.19000v1)**|CVPR2023|**[link](https://github.com/samsunglabs/mtl)**|None|Dmitry Senushkin et.al.|
|**2023-05-30**|**[HQDec: Self-Supervised Monocular Depth Estimation Based on a High-Quality Decoder](http://arxiv.org/abs/2305.18706v1)**|None|**[link](https://github.com/fwucas/hqdec)**|None|Fei Wang et.al.|
|**2023-05-28**|**[OccCasNet: Occlusion-aware Cascade Cost Volume for Light Field Depth Estimation](http://arxiv.org/abs/2305.17710v1)**|None|**[link](https://github.com/chaowentao/occcasnet)**|None|Wentao Chao et.al.|
|**2023-05-27**|**[USIM-DAL: Uncertainty-aware Statistical Image Modeling-based Dense Active Learning for Super-resolution](http://arxiv.org/abs/2305.17520v1)**|None|None|Accepted at UAI 2023|Vikrant Rangnekar et.al.|
|**2023-05-25**|**[SimHaze: game engine simulated data for real-world dehazing](http://arxiv.org/abs/2305.16481v1)**|None|None|Submitted to ICIP 2023|Zhengyang Lou et.al.|
|**2023-05-25**|**[Learning Occupancy for Monocular 3D Object Detection](http://arxiv.org/abs/2305.15694v1)**|None|**[link](https://github.com/spengliang/occupancym3d)**|None|Liang Peng et.al.|
|**2023-05-24**|**[Polarimetric Imaging for Perception](http://arxiv.org/abs/2305.14787v1)**|None|None|None|Michael Baltaxe et.al.|
|**2023-05-24**|**[AutoDepthNet: High Frame Rate Depth Map Reconstruction using Commodity Depth and RGB Cameras](http://arxiv.org/abs/2305.14731v1)**|None|None|None|Peyman Gholami et.al.|
|**2023-05-22**|**[FEDORA: Flying Event Dataset fOr Reactive behAvior](http://arxiv.org/abs/2305.14392v2)**|None|None|None|Amogh Joshi et.al.|
|**2023-05-22**|**[Gated Stereo: Joint Depth Estimation from Gated and Wide-Baseline Active Stereo Cues](http://arxiv.org/abs/2305.12955v1)**|None|None|None|Stefanie Walz et.al.|
|**2023-05-19**|**[Text2NeRF: Text-Driven 3D Scene Generation with Neural Radiance Fields](http://arxiv.org/abs/2305.11588v2)**|None|**[link](https://github.com/eckertzhang/text2nerf)**|Accepted by TVCG; Homepage:   https://eckertzhang.github.io/Text2NeRF.github.io/   Code:https://github.com/eckertzhang/Text2NeRF|Jingbo Zhang et.al.|
|**2023-05-19**|**[Brain Captioning: Decoding human brain activity into images and text](http://arxiv.org/abs/2305.11560v1)**|None|None|None|Matteo Ferrante et.al.|
|**2023-05-16**|**[PanelNet: Understanding 360 Indoor Environment via Panel Representation](http://arxiv.org/abs/2305.09078v1)**|None|None|To appear in CVPR 2023|Haozheng Yu et.al.|
|**2023-05-13**|**[MetaMorphosis: Task-oriented Privacy Cognizant Feature Generation for Multi-task Learning](http://arxiv.org/abs/2305.07815v1)**|None|None|Preprint version, 22 pages. Keywords: Multi-task learning, neural   networks, collaborative intelligence, differential privacy, task privacy|Md Adnan Arefeen et.al.|
|**2023-05-12**|**[Learning Monocular Depth in Dynamic Environment via Context-aware Temporal Attention](http://arxiv.org/abs/2305.07397v1)**|None|None|accepted by IJCAI 2023; 9 pages, 5 figures|Zizhang Wu et.al.|
|**2023-05-11**|**[Virtual Occlusions Through Implicit Depth](http://arxiv.org/abs/2305.07014v1)**|None|**[link](https://github.com/nianticlabs/implicit-depth)**|Accepted to CVPR 2023|Jamie Watson et.al.|
|**2023-05-10**|**[A Multi-modal Approach to Single-modal Visual Place Classification](http://arxiv.org/abs/2305.06179v2)**|None|None|7 pages, 6 figures, 1 table|Tomoya Iwasaki et.al.|
|**2023-05-10**|**[FusionDepth: Complement Self-Supervised Monocular Depth Estimation with Cost Volume](http://arxiv.org/abs/2305.06036v1)**|None|None|None|Zhuofei Huang et.al.|
|**2023-05-08**|**[Improving 2D face recognition via fine-level facial depth generation and RGB-D complementary feature learning](http://arxiv.org/abs/2305.04426v1)**|None|None|None|Wenhao Hu et.al.|
|**2023-05-04**|**[Edge-aware Consistent Stereo Video Depth Estimation](http://arxiv.org/abs/2305.02645v1)**|None|None|None|Elena Kosheleva et.al.|
|**2023-05-02**|**[High-Resolution Synthetic RGB-D Datasets for Monocular Depth Estimation](http://arxiv.org/abs/2305.01732v1)**|None|None|None|Aakash Rajpal et.al.|
|**2023-05-02**|**[AutoColor: Learned Light Power Control for Multi-Color Holograms](http://arxiv.org/abs/2305.01611v2)**|None|**[link](https://github.com/complight/autocolor)**|6 pages, 2 figures, SPIE VR|AR|MR 2024|Yicheng Zhan et.al.|
|**2023-04-28**|**[ViP-NeRF: Visibility Prior for Sparse Input Neural Radiance Fields](http://arxiv.org/abs/2305.00041v1)**|ACM SIGGRAPH 2023 Conference Proceedings, Article 71, Pages 1-11|**[link](https://github.com/NagabhushanSN95/ViP-NeRF)**|SIGGRAPH 2023|Nagabhushan Somraj et.al.|
|**2023-04-25**|**[Depth-Relative Self Attention for Monocular Depth Estimation](http://arxiv.org/abs/2304.12849v1)**|None|None|Accepted for IJCAI 2023|Kyuhong Shim et.al.|
|**2023-04-25**|**[Exploring the Mutual Influence between Self-Supervised Single-Frame and Multi-Frame Depth Estimation](http://arxiv.org/abs/2304.12685v2)**|None|**[link](https://github.com/xjixzz/mism)**|Accepted for publication in the IEEE Robotics and Automation Letters   (RA-L). 8 pages, 3figures|Jie Xiang et.al.|
|**2023-04-22**|**[Dehazing-NeRF: Neural Radiance Fields from Hazy Images](http://arxiv.org/abs/2304.11448v1)**|None|None|None|Tian Li et.al.|
|**2023-04-02**|**[altiro3D: Scene representation from single image and novel view synthesis](http://arxiv.org/abs/2304.11161v2)**|None|**[link](https://github.com/canessae/altiro3D)**|In press (2023) Springer International Journal of Information   Technology (IJIT) 10 pages, 3 figures|E. Canessa et.al.|
|**2023-04-20**|**[A geometry-aware deep network for depth estimation in monocular endoscopy](http://arxiv.org/abs/2304.10241v1)**|None|**[link](https://github.com/yym-sia/lingmi-mr)**|None|Yongming Yang et.al.|
|**2023-04-19**|**[CrossFusion: Interleaving Cross-modal Complementation for Noise-resistant 3D Object Detection](http://arxiv.org/abs/2304.09694v1)**|None|None|None|Yang Yang et.al.|
|**2023-04-19**|**[DarSwin: Distortion Aware Radial Swin Transformer](http://arxiv.org/abs/2304.09691v4)**|None|None|18 pages, 12 figures|Akshaya Athwale et.al.|
|**2023-04-19**|**[Reference-guided Controllable Inpainting of Neural Radiance Fields](http://arxiv.org/abs/2304.09677v2)**|None|None|Project Page: https://ashmrz.github.io/reference-guided-3d|Ashkan Mirzaei et.al.|
|**2023-04-18**|**[Learning to Fuse Monocular and Multi-view Cues for Multi-frame Depth Estimation in Dynamic Scenes](http://arxiv.org/abs/2304.08993v1)**|None|**[link](https://github.com/ruili3/dynamic-multiframe-depth)**|Accepted by CVPR 2023. Code and models are available at:   https://github.com/ruili3/dynamic-multiframe-depth|Rui Li et.al.|
|**2023-04-18**|**[Pose Constraints for Consistent Self-supervised Monocular Depth and Ego-motion](http://arxiv.org/abs/2304.08916v1)**|None|**[link](https://github.com/zshn25/pc4consistentdepth)**|Scandinavian Conference on Image Analysis (SCIA) 2023|Zeeshan Khan Suri et.al.|
|**2023-04-17**|**[360$^\circ$ High-Resolution Depth Estimation via Uncertainty-aware Structural Knowledge Transfer](http://arxiv.org/abs/2304.07967v3)**|None|None|10 pages|Zidong Cao et.al.|
|**2023-04-16**|**[EGformer: Equirectangular Geometry-biased Transformer for 360 Depth Estimation](http://arxiv.org/abs/2304.07803v2)**|None|None|12 pages, Accepted to ICCV23, Camera ready version|Ilwi Yun et.al.|
|**2023-04-15**|**[Temporally Consistent Online Depth Estimation Using Point-Based Fusion](http://arxiv.org/abs/2304.07435v3)**|CVPR 2023|**[link](https://github.com/facebookresearch/TemporallyConsistentDepth)**|Source code:   https://github.com/facebookresearch/TemporallyConsistentDepth|Numair Khan et.al.|
|**2023-04-14**|**[The Second Monocular Depth Estimation Challenge](http://arxiv.org/abs/2304.07051v3)**|None|None|Published at CVPRW2023|Jaime Spencer et.al.|
|**2023-04-14**|**[Self-Supervised Learning based Depth Estimation from Monocular Images](http://arxiv.org/abs/2304.06966v1)**|None|**[link](https://github.com/nyu-ce-projects/depth-estimation)**|None|Mayank Poddar et.al.|
|**2023-04-13**|**[Event-based tracking of human hands](http://arxiv.org/abs/2304.06534v1)**|Sensor Review, Vol. 41 No. 4, pp. 382-389 (2021)|None|None|Laura Duarte et.al.|
|**2023-04-13**|**[iDisc: Internal Discretization for Monocular Depth Estimation](http://arxiv.org/abs/2304.06334v1)**|None|**[link](https://github.com/SysCV/idisc)**|Accepted at CVPR 2023|Luigi Piccinelli et.al.|
|**2023-04-11**|**[Improving Neural Radiance Fields with Depth-aware Optimization for Novel View Synthesis](http://arxiv.org/abs/2304.05218v2)**|None|**[link](https://github.com/xtu-pr-lab/sfmnerf)**|None|Shu Chen et.al.|
|**2023-04-07**|**[DualRefine: Self-Supervised Depth and Pose Estimation Through Iterative Epipolar Sampling and Refinement Toward Equilibrium](http://arxiv.org/abs/2304.03560v2)**|None|**[link](https://github.com/antabangun/dualrefine)**|CVPR 2023. Project page:   https://antabangun.github.io/projects/DualRefine/ Code:   https://github.com/antabangun/DualRefine|Antyanta Bangunharcana et.al.|
|**2023-04-06**|**[EGA-Depth: Efficient Guided Attention for Self-Supervised Multi-Camera Depth Estimation](http://arxiv.org/abs/2304.03369v1)**|None|None|CVPR 2023 Workshop on Autonomous Driving|Yunxiao Shi et.al.|
|**2023-04-06**|**[DeLiRa: Self-Supervised Depth, Light, and Radiance Fields](http://arxiv.org/abs/2304.02797v1)**|None|None|Project page: https://sites.google.com/view/tri-delira|Vitor Guizilini et.al.|
|**2023-04-05**|**[DEFLOW: Self-supervised 3D Motion Estimation of Debris Flow](http://arxiv.org/abs/2304.02569v1)**|None|None|Photogrammetric Computer Vision Workshop, CVPRW 2023, camera ready|Liyuan Zhu et.al.|
|**2023-04-04**|**[FineRecon: Depth-aware Feed-forward Network for Detailed 3D Reconstruction](http://arxiv.org/abs/2304.01480v2)**|None|**[link](https://github.com/apple/ml-finerecon)**|ICCV 2023|Noah Stier et.al.|
|**2023-04-03**|**[Joint 2D-3D Multi-Task Learning on Cityscapes-3D: 3D Detection, Segmentation, and Depth Estimation](http://arxiv.org/abs/2304.00971v3)**|ICLR 2023|**[link](https://github.com/prismformore/multi-task-transformer)**|A supplementary document for "TaskPrompter: Spatial-Channel   Multi-Task Prompting for Dense Scene Understanding" accepted by ICLR 2023.   Project page:   https://github.com/prismformore/Multi-Task-Transformer/tree/main/TaskPrompter|Hanrong Ye et.al.|
|**2023-03-31**|**[SemHint-MD: Learning from Noisy Semantic Labels for Self-Supervised Monocular Depth Estimation](http://arxiv.org/abs/2303.18219v1)**|None|None|None|Shan Lin et.al.|
|**2023-03-31**|**[Single Image Depth Prediction Made Better: A Multivariate Gaussian Take](http://arxiv.org/abs/2303.18164v2)**|None|None|Accepted to IEEE/CVF CVPR 2023. Draft info: 17 pages, 13 Figures, 9   Tables|Ce Liu et.al.|
|**2023-03-31**|**[3D-aware Image Generation using 2D Diffusion Models](http://arxiv.org/abs/2303.17905v1)**|None|None|Website: https://jeffreyxiang.github.io/ivid/|Jianfeng Xiang et.al.|
|**2023-03-31**|**[EA-LSS: Edge-aware Lift-splat-shot Framework for 3D BEV Object Detection](http://arxiv.org/abs/2303.17895v4)**|None|**[link](https://github.com/hht1996ok/ea-bev)**|None|Haotian Hu et.al.|
|**2023-03-31**|**[Joint Depth Estimation and Mixture of Rain Removal From a Single Image](http://arxiv.org/abs/2303.17766v1)**|None|**[link](https://github.com/yz-wang/demore-net)**|11 pages, 7 figures, 5 tables|Yongzhen Wang et.al.|
|**2023-03-30**|**[TiDy-PSFs: Computational Imaging with Time-Averaged Dynamic Point-Spread-Functions](http://arxiv.org/abs/2303.17583v1)**|None|None|13 pages, 16 figures|Sachin Shah et.al.|
|**2023-03-30**|**[DDP: Diffusion Model for Dense Visual Prediction](http://arxiv.org/abs/2303.17559v2)**|None|**[link](https://github.com/jiyuanfeng/ddp)**|Added controlnet exp|Yuanfeng Ji et.al.|
|**2023-03-29**|**[An intelligent modular real-time vision-based system for environment perception](http://arxiv.org/abs/2303.16710v1)**|None|**[link](https://github.com/pandas-team/autonomous-vehicle-environment-perception)**|Accepted in NeurIPS 2022 Workshop on Machine Learning for Autonomous   Driving|Amirhossein Kazerouni et.al.|
|**2023-03-29**|**[DORT: Modeling Dynamic Objects in Recurrent for Multi-Camera 3D Object Detection and Tracking](http://arxiv.org/abs/2303.16628v2)**|None|**[link](https://github.com/smartbot-pjlab/dort)**|None|Qing Lian et.al.|
|**2023-03-28**|**[4K-HAZE: A Dehazing Benchmark with 4K Resolution Hazy and Haze-Free Images](http://arxiv.org/abs/2303.15848v1)**|None|**[link](https://github.com/zzr-idam/4KDehazing)**|None|Zhuoran Zheng et.al.|
|**2023-03-26**|**[On the Importance of Accurate Geometry Data for Dense 3D Vision Tasks](http://arxiv.org/abs/2303.14840v1)**|None|**[link](https://github.com/junggy/hammer-dataset)**|Accepted at CVPR 2023, Main Paper + Supp. Mat. arXiv admin note:   substantial text overlap with arXiv:2205.04565|HyunJun Jung et.al.|
|**2023-03-26**|**[Multi-Frame Self-Supervised Depth Estimation with Multi-Scale Feature Fusion in Dynamic Scenes](http://arxiv.org/abs/2303.14628v2)**|None|None|11 pages, 8 figures, ACM MM'23 accepted|Jiquan Zhong et.al.|
|**2023-03-23**|**[SCADE: NeRFs from Space Carving with Ambiguity-Aware Depth Estimates](http://arxiv.org/abs/2303.13582v1)**|None|None|CVPR 2023|Mikaela Angelina Uy et.al.|
|**2023-03-23**|**[Collaboration Helps Camera Overtake LiDAR in 3D Detection](http://arxiv.org/abs/2303.13560v1)**|None|**[link](https://github.com/mediabrain-sjtu/coca3d)**|Accepted by CVPR23|Yue Hu et.al.|
|**2023-03-22**|**[LFM-3D: Learnable Feature Matching Across Wide Baselines Using 3D Signals](http://arxiv.org/abs/2303.12779v3)**|None|None|3DV 2024, oral paper|Arjun Karpur et.al.|
|**2023-03-21**|**[Monocular Visual-Inertial Depth Estimation](http://arxiv.org/abs/2303.12134v1)**|None|**[link](https://github.com/isl-org/vi-depth)**|Accepted for publication at ICRA'23|Diana Wofk et.al.|
|**2023-03-21**|**[Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models](http://arxiv.org/abs/2303.11989v2)**|None|**[link](https://github.com/lukashoel/text2room)**|Accepted to ICCV 2023 (Oral) video: https://youtu.be/fjRnFL91EZc   project page: https://lukashoel.github.io/text-to-room/ code:   https://github.com/lukasHoel/text2room|Lukas Höllein et.al.|
|**2023-03-21**|**[HRDFuse: Monocular 360°Depth Estimation by Collaboratively Learning Holistic-with-Regional Depth Distributions](http://arxiv.org/abs/2303.11616v3)**|None|None|To appear at CVPR2023, 20 pages|Hao Ai et.al.|
|**2023-03-20**|**[Versatile Depth Estimator Based on Common Relative Depth Estimation and Camera-Specific Relative-to-Metric Depth Conversion](http://arxiv.org/abs/2303.10991v1)**|None|None|None|Jinyoung Jun et.al.|
|**2023-03-20**|**[Boosting Weakly Supervised Object Detection using Fusion and Priors from Hallucinated Depth](http://arxiv.org/abs/2303.10937v2)**|None|None|None|Cagri Gungor et.al.|
|**2023-03-17**|**[Spectrum-inspired Low-light Image Translation for Saliency Detection](http://arxiv.org/abs/2303.10145v1)**|None|None|Presented at The Indian Conference on Computer Vision, Graphics and   Image Processing (ICVGIP) 2022|Kitty Varghese et.al.|
|**2023-03-17**|**[A Simple Framework for 3D Occupancy Estimation in Autonomous Driving](http://arxiv.org/abs/2303.10076v5)**|None|**[link](https://github.com/ganwanshui/simpleoccupancy)**|15 pages, 8 figures|Wanshui Gan et.al.|
|**2023-03-17**|**[Exploring Sparse Visual Prompt for Domain Adaptive Dense Prediction](http://arxiv.org/abs/2303.09792v3)**|None|**[link](https://github.com/iccv2595/svdp)**|Accepted by AAAI 2024|Senqiao Yang et.al.|
|**2023-03-14**|**[A Simple Baseline for Supervised Surround-view Depth Estimation](http://arxiv.org/abs/2303.07759v3)**|None|None|None|Xianda Guo et.al.|
|**2023-03-14**|**[Do More With What You Have: Transferring Depth-Scale from Labeled to Unlabeled Domains](http://arxiv.org/abs/2303.07662v3)**|None|None|None|Alexandra Dana et.al.|
|**2023-03-13**|**[DEHRFormer: Real-time Transformer for Depth Estimation and Haze Removal from Varicolored Haze Scenes](http://arxiv.org/abs/2303.06905v1)**|None|None|Accepted to ICASSP'2023|Sixiang Chen et.al.|
|**2023-03-09**|**[DiffusionDepth: Diffusion Denoising Approach for Monocular Depth Estimation](http://arxiv.org/abs/2303.05021v4)**|None|**[link](https://github.com/duanyiqun/diffusiondepth)**|None|Yiqun Duan et.al.|
|**2023-03-08**|**[Aberration-Aware Depth-from-Focus](http://arxiv.org/abs/2303.04654v2)**|None|**[link](https://github.com/vccimaging/aberration-aware-depth-from-focus)**|[ICCP & TPAMI 2023] Considering optical aberrations during network   training can improve the generalizability|Xinge Yang et.al.|
|**2023-03-08**|**[RM-Depth: Unsupervised Learning of Recurrent Monocular Depth in Dynamic Scenes](http://arxiv.org/abs/2303.04456v1)**|None|**[link](https://github.com/twhui/rm-depth)**|Accepted to CVPR 2022 (paper is updated)|Tak-Wai Hui et.al.|
|**2023-03-06**|**[DwinFormer: Dual Window Transformers for End-to-End Monocular Depth Estimation](http://arxiv.org/abs/2303.02968v2)**|IEEE Sensors Journal (Volume: 23, Issue: 18, 15 September 2023)|None|None|Md Awsafur Rahman et.al.|
|**2023-03-03**|**[Unleashing Text-to-Image Diffusion Models for Visual Perception](http://arxiv.org/abs/2303.02153v1)**|None|**[link](https://github.com/wl-zhao/VPD)**|project page: https://vpd.ivg-research.xyz|Wenliang Zhao et.al.|
|**2023-03-03**|**[Towards Domain Generalization for Multi-view 3D Object Detection in Bird-Eye-View](http://arxiv.org/abs/2303.01686v1)**|None|None|Accepted to CVPR 2023|Shuo Wang et.al.|
|**2023-03-02**|**[DejaVu: Conditional Regenerative Learning to Enhance Dense Prediction](http://arxiv.org/abs/2303.01573v2)**|None|None|Accepted to CVPR 2023|Shubhankar Borse et.al.|
|**2023-03-02**|**[3D generation on ImageNet](http://arxiv.org/abs/2303.01416v1)**|ICLR 2023|None|ICLR 2023 (Oral)|Ivan Skorokhodov et.al.|
|**2023-03-02**|**[APARATE: Adaptive Adversarial Patch for CNN-based Monocular Depth Estimation for Autonomous Navigation](http://arxiv.org/abs/2303.01351v2)**|None|None|None|Amira Guesmi et.al.|
|**2023-03-02**|**[I2P-Rec: Recognizing Images on Large-scale Point Cloud Maps through Bird's Eye View Projections](http://arxiv.org/abs/2303.01043v2)**|None|None|Accepted by IROS 2023|Shuhang Zheng et.al.|
|**2023-02-28**|**[Monocular Depth Estimation using Diffusion Models](http://arxiv.org/abs/2302.14816v1)**|None|None|None|Saurabh Saxena et.al.|
|**2023-02-25**|**[SUPS: A Simulated Underground Parking Scenario Dataset for Autonomous Driving](http://arxiv.org/abs/2302.12966v1)**|None|**[link](https://github.com/jarvishou829/sups)**|Accepted for publication at the 25th IEEE Intelligent Transportation   Systems Conference (ITSC 2022)|Jiawei Hou et.al.|
|**2023-02-23**|**[ZoeDepth: Zero-shot Transfer by Combining Relative and Metric Depth](http://arxiv.org/abs/2302.12288v1)**|None|**[link](https://github.com/isl-org/ZoeDepth)**|None|Shariq Farooq Bhat et.al.|
|**2023-02-23**|**[VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene Completion](http://arxiv.org/abs/2302.12251v2)**|None|**[link](https://github.com/nvlabs/voxformer)**|CVPR 2023 Highlight (10% of accepted papers, 2.5% of submissions)|Yiming Li et.al.|
|**2023-02-21**|**[Bokeh Rendering Based on Adaptive Depth Calibration Network](http://arxiv.org/abs/2302.10808v1)**|None|None|6 pages, 6 figures|Lu Liu et.al.|
|**2023-02-21**|**[Learning 3D Photography Videos via Self-supervised Diffusion on Single Images](http://arxiv.org/abs/2302.10781v1)**|None|None|10 pages, 7 figures|Xiaodong Wang et.al.|
|**2023-02-21**|**[Depth Estimation and Image Restoration by Deep Learning from Defocused Images](http://arxiv.org/abs/2302.10730v2)**|IEEE Transactions on Computational Imaging, vol. 9, pp. 607-619,   2023|None|None|Saqib Nazir et.al.|
|**2023-02-21**|**[MonoPGC: Monocular 3D Object Detection with Pixel Geometry Contexts](http://arxiv.org/abs/2302.10549v1)**|None|None|Accepted by ICRA 2023|Zizhang Wu et.al.|
|**2023-02-20**|**[On the Metrics for Evaluating Monocular Depth Estimation](http://arxiv.org/abs/2302.10007v1)**|None|None|11 pages, 8 figures|Akhil Gurram et.al.|
|**2023-02-20**|**[GlocalFuse-Depth: Fusing Transformers and CNNs for All-day Self-supervised Monocular Depth Estimation](http://arxiv.org/abs/2302.09884v1)**|None|None|None|Zezheng Zhang et.al.|
|**2023-02-20**|**[Self-Supervised Monocular Depth Estimation with Self-Reference Distillation and Disparity Offset Refinement](http://arxiv.org/abs/2302.09789v2)**|None|**[link](https://github.com/rnlee1998/SRD)**|None|Zhong Liu et.al.|
|**2023-02-17**|**[Long Range Object-Level Monocular Depth Estimation for UAVs](http://arxiv.org/abs/2302.08943v1)**|None|None|16 pages, SCIA 2023|David Silva et.al.|
|**2023-02-17**|**[MixNeRF: Modeling a Ray with Mixture Density for Novel View Synthesis from Sparse Inputs](http://arxiv.org/abs/2302.08788v2)**|None|**[link](https://github.com/shawn615/MixNeRF)**|CVPR 2023. Project Page: https://shawn615.github.io/mixnerf/|Seunghyeon Seo et.al.|
|**2023-02-16**|**[URCDC-Depth: Uncertainty Rectified Cross-Distillation with CutFlip for Monocular Depth Estimation](http://arxiv.org/abs/2302.08149v2)**|None|**[link](https://github.com/shuweishao/urcdc-depth)**|9 pages|Shuwei Shao et.al.|
|**2023-02-16**|**[Spectral 3D Computer Vision -- A Review](http://arxiv.org/abs/2302.08054v1)**|None|None|None|Yajie Sun et.al.|
|**2023-02-08**|**[SkyEye: Self-Supervised Bird's-Eye-View Semantic Mapping Using Monocular Frontal View Images](http://arxiv.org/abs/2302.04233v1)**|None|None|14 pages, 7 figures|Nikhil Gosala et.al.|
|**2023-02-08**|**[EVEN: An Event-Based Framework for Monocular Depth Estimation at Adverse Night Conditions](http://arxiv.org/abs/2302.03860v1)**|None|None|None|Peilun Shi et.al.|
|**2023-02-06**|**[Structure and Content-Guided Video Synthesis with Diffusion Models](http://arxiv.org/abs/2302.03011v1)**|None|None|Project page at https://research.runwayml.com/gen1|Patrick Esser et.al.|
|**2023-02-05**|**[A Disparity Refinement Framework for Learning-based Stereo Matching Methods in Cross-domain Setting for Laparoscopic Images](http://arxiv.org/abs/2302.02294v1)**|None|None|None|Zixin Yang et.al.|
|**2023-02-02**|**[STEPS: Joint Self-supervised Nighttime Image Enhancement and Depth Estimation](http://arxiv.org/abs/2302.01334v1)**|None|**[link](https://github.com/ucaszyp/steps)**|Accepted by ICRA 2023, Code: https://github.com/ucaszyp/STEPS|Yupeng Zheng et.al.|
|**2023-02-01**|**[Uncertainty-Driven Dense Two-View Structure from Motion](http://arxiv.org/abs/2302.00523v2)**|None|None|Accepted for publication at IEEE Robotics and Automation Letters   (RA-L) 2023|Weirong Chen et.al.|
|**2023-01-31**|**[Adversarial Training of Self-supervised Monocular Depth Estimation against Physical-World Attacks](http://arxiv.org/abs/2301.13487v3)**|None|**[link](https://github.com/Bob-cheng/DepthModelHardening)**|Initially accepted at ICLR2023 (Spotlight)|Zhiyuan Cheng et.al.|
|**2023-01-31**|**[Recurrent Structure Attention Guidance for Depth Super-Resolution](http://arxiv.org/abs/2301.13419v1)**|None|None|Accepted by AAAI-2023|Jiayi Yuan et.al.|
|**2023-01-30**|**[AudioEar: Single-View Ear Reconstruction for Personalized Spatial Audio](http://arxiv.org/abs/2301.12613v1)**|None|**[link](https://github.com/seanywang0408/audioear)**|Accepted by Thirty-Seventh AAAI Conference on Artificial Intelligence   (AAAI 2023)|Xiaoyang Huang et.al.|
|**2023-01-26**|**[Learning Good Features to Transfer Across Tasks and Domains](http://arxiv.org/abs/2301.11310v1)**|None|None|Extended version of the paper "Learning Across Tasks and Domains"   presented at ICCV 2019. Accepted at TPAMI|Pierluigi Zama Ramirez et.al.|
|**2023-01-25**|**[On the Adversarial Robustness of Camera-based 3D Object Detection](http://arxiv.org/abs/2301.10766v2)**|None|**[link](https://github.com/daniel-xsy/bev-attack)**|Transactions on Machine Learning Research, 2024. ISSN 2835-8856|Shaoyuan Xie et.al.|
|**2023-01-20**|**[Unsupervised Light Field Depth Estimation via Multi-view Feature Matching with Occlusion Prediction](http://arxiv.org/abs/2301.08433v2)**|None|None|None|Shansi Zhang et.al.|
|**2023-01-20**|**[FG-Depth: Flow-Guided Unsupervised Monocular Depth Estimation](http://arxiv.org/abs/2301.08414v2)**|None|None|Accepted by ICRA2023|Junyu Zhu et.al.|
|**2023-01-19**|**[Booster: a Benchmark for Depth from Images of Specular and Transparent Surfaces](http://arxiv.org/abs/2301.08245v3)**|None|None|Extension of the paper "Open Challenges in Deep Stereo: the Booster   Dataset" presented at CVPR 2022. Accepted at TPAMI|Pierluigi Zama Ramirez et.al.|
|**2023-01-17**|**[SwinDepth: Unsupervised Depth Estimation using Monocular Sequences via Swin Transformer and Densely Cascaded Network](http://arxiv.org/abs/2301.06715v1)**|None|**[link](https://github.com/dsshim0125/SwinDepth)**|ICRA 2023|Dongseok Shim et.al.|
|**2023-01-14**|**[Dyna-DepthFormer: Multi-frame Transformer for Self-Supervised Depth Estimation in Dynamic Scenes](http://arxiv.org/abs/2301.05871v2)**|None|None|ICRA 2023|Songchun Zhang et.al.|
|**2023-01-14**|**[${S}^{2}$Net: Accurate Panorama Depth Estimation on Spherical Surface](http://arxiv.org/abs/2301.05845v1)**|None|None|Accepted by IEEE Robotics and Automation Letters|Meng Li et.al.|
|**2023-01-09**|**[Deep Planar Parallax for Monocular Depth Estimation](http://arxiv.org/abs/2301.03178v2)**|None|None|None|Haoqian Liang et.al.|
|**2023-01-09**|**[A Study on the Generality of Neural Network Structures for Monocular Depth Estimation](http://arxiv.org/abs/2301.03169v3)**|None|**[link](https://github.com/sjg02122/MonoFormer)**|Accepted in TPAMI|Jinwoo Bae et.al.|
|**2023-01-05**|**[All in Tokens: Unifying Output Space of Visual Tasks via Soft Token](http://arxiv.org/abs/2301.02229v2)**|None|**[link](https://github.com/swintransformer/ait)**|None|Jia Ning et.al.|
|**2023-01-05**|**[DepthP+P: Metric Accurate Monocular Depth Estimation using Planar and Parallax](http://arxiv.org/abs/2301.02092v1)**|None|None|None|Sadra Safadoust et.al.|
|**2023-01-03**|**[BS3D: Building-scale 3D Reconstruction from RGB-D Images](http://arxiv.org/abs/2301.01057v1)**|None|None|None|Janne Mustaniemi et.al.|
|**2022-12-29**|**[Learning 3D Human Pose Estimation from Dozens of Datasets using a Geometry-Aware Autoencoder to Bridge Between Skeleton Formats](http://arxiv.org/abs/2212.14474v1)**|None|None|Accepted at the 2023 IEEE/CVF Winter Conference on Applications of   Computer Vision (WACV'23)|István Sárándi et.al.|
|**2022-12-24**|**[HandsOff: Labeled Dataset Generation With No Additional Human Annotations](http://arxiv.org/abs/2212.12645v2)**|None|None|22 pages, 20 figures. CVPR 2023|Austin Xu et.al.|
|**2022-12-22**|**[Depth Estimation maps of lidar and stereo images](http://arxiv.org/abs/2212.11741v1)**|None|None|10 pages, 13 figures|Fei Wu et.al.|
|**2022-12-20**|**[Scene-aware Egocentric 3D Human Pose Estimation](http://arxiv.org/abs/2212.11684v3)**|None|**[link](https://github.com/jianwang-mpi/SceneEgo)**|None|Jian Wang et.al.|
|**2022-12-22**|**[Vision-Based Environmental Perception for Autonomous Driving](http://arxiv.org/abs/2212.11453v1)**|None|None|39 pages, 17 figures|Fei Liu et.al.|
|**2022-12-21**|**[Lightweight Monocular Depth Estimation](http://arxiv.org/abs/2212.11363v1)**|None|None|None|Ruilin Ma et.al.|
|**2022-12-21**|**[MaskingDepth: Masked Consistency Regularization for Semi-supervised Monocular Depth Estimation](http://arxiv.org/abs/2212.10806v3)**|None|**[link](https://github.com/ku-cvlab/maskingdepth)**|Project page: https://ku-cvlab.github.io/MaskingDepth/|Jongbeom Baek et.al.|
|**2022-12-17**|**[Are We Ready for Vision-Centric Driving Streaming Perception? The ASAP Benchmark](http://arxiv.org/abs/2212.08914v1)**|None|**[link](https://github.com/jeffwang987/asap)**|code: https://github.com/JeffWang987/ASAP|Xiaofeng Wang et.al.|
|**2022-12-17**|**[DAG: Depth-Aware Guidance with Denoising Diffusion Probabilistic Models](http://arxiv.org/abs/2212.08861v2)**|None|**[link](https://github.com/KU-CVLAB/DAG)**|Project page is available at https://ku-cvlab.github.io/DAG/|Gyeongnyeon Kim et.al.|
|**2022-12-15**|**[Solve the Puzzle of Instance Segmentation in Videos: A Weakly Supervised Framework with Spatio-Temporal Collaboration](http://arxiv.org/abs/2212.07592v1)**|IEEE Transactions on Circuits and Systems for Video Technology   (2022)|None|None|Liqi Yan et.al.|
|**2022-12-12**|**[Towards Practical Plug-and-Play Diffusion Models](http://arxiv.org/abs/2212.05973v2)**|None|**[link](https://github.com/riiid/ppap)**|CVPR 2023 camera-ready|Hyojun Go et.al.|
|**2022-12-12**|**[ROIFormer: Semantic-Aware Region of Interest Transformer for Efficient Self-Supervised Monocular Depth Estimation](http://arxiv.org/abs/2212.05729v3)**|None|None|Camera Ready for AAAI 2023|Daitao Xing et.al.|
|**2022-12-10**|**[Mind The Edge: Refining Depth Edges in Sparsely-Supervised Monocular Depth Estimation](http://arxiv.org/abs/2212.05315v3)**|None|**[link](https://github.com/liortalker/MindTheEdge)**|Appears in CVPR24'|Lior Talker et.al.|
|**2022-12-09**|**[Cross-Domain Synthetic-to-Real In-the-Wild Depth and Normal Estimation for 3D Scene Understanding](http://arxiv.org/abs/2212.05040v3)**|None|None|Accepted to OmniCV 2024|Jay Bhanushali et.al.|
|**2022-12-09**|**[4K-NeRF: High Fidelity Neural Radiance Fields at Ultra High Resolutions](http://arxiv.org/abs/2212.04701v2)**|None|**[link](https://github.com/frozoul/4k-nerf)**|None|Zhongshu Wang et.al.|
|**2022-12-06**|**[Event-based Monocular Dense Depth Estimation with Recurrent Transformers](http://arxiv.org/abs/2212.02791v1)**|None|None|10 pages, 5 figures|Xu Liu et.al.|
|**2022-12-05**|**[GARF:Geometry-Aware Generalized Neural Radiance Field](http://arxiv.org/abs/2212.02280v2)**|None|None|None|Yue Shi et.al.|
|**2022-12-04**|**[3D Object Aided Self-Supervised Monocular Depth Estimation](http://arxiv.org/abs/2212.01768v1)**|None|None|None|Songlin Wei et.al.|
|**2022-12-03**|**[Multi-resolution Monocular Depth Map Fusion by Self-supervised Gradient-based Composition](http://arxiv.org/abs/2212.01538v1)**|None|**[link](https://github.com/yuinsky/gradient-based-depth-map-fusion)**|19 pages (with supplementary material)|Yaqiao Dai et.al.|
|**2022-12-02**|**[Geometry-Aware Network for Domain Adaptive Semantic Segmentation](http://arxiv.org/abs/2212.00920v2)**|None|None|AAAI 2023|Yinghong Liao et.al.|
|**2022-12-01**|**[BEV-LGKD: A Unified LiDAR-Guided Knowledge Distillation Framework for BEV 3D Object Detection](http://arxiv.org/abs/2212.00623v1)**|None|**[link](https://github.com/northsummer/lgkd)**|12pages|Jianing Li et.al.|
|**2022-11-30**|**[ObjCAViT: Improving Monocular Depth Estimation Using Natural Language Models And Image-Object Cross-Attention](http://arxiv.org/abs/2211.17232v1)**|None|**[link](https://github.com/dylanauty/objcavit)**|9 pages, 4 figures. Code is released at   https://github.com/DylanAuty/ObjCAViT|Dylan Auty et.al.|
|**2022-11-30**|**[Weakly Supervised 3D Multi-person Pose Estimation for Large-scale Scenes based on Monocular Camera and Single LiDAR](http://arxiv.org/abs/2211.16951v1)**|None|None|Accepted by AAAI 2023|Peishan Cong et.al.|
|**2022-11-30**|**[Attention-Based Depth Distillation with 3D-Aware Positional Encoding for Monocular 3D Object Detection](http://arxiv.org/abs/2211.16779v2)**|None|None|Accepted by AAAI2023|Zizhang Wu et.al.|
|**2022-11-29**|**[NeuralLift-360: Lifting An In-the-wild 2D Photo to A 3D Object with 360° Views](http://arxiv.org/abs/2211.16431v2)**|None|**[link](https://github.com/VITA-Group/NeuralLift-360)**|Project page: https://vita-group.github.io/NeuralLift-360/|Dejia Xu et.al.|
|**2022-11-29**|**[Generalized Face Anti-Spoofing via Multi-Task Learning and One-Side Meta Triplet Loss](http://arxiv.org/abs/2211.15955v1)**|None|None|2023 IEEE International Conference on Automatic Face and Gesture   Recognition (FG)|Chu-Chun Chuang et.al.|
|**2022-11-28**|**[SuperFusion: Multilevel LiDAR-Camera Fusion for Long-Range HD Map Generation](http://arxiv.org/abs/2211.15656v2)**|None|**[link](https://github.com/haomo-ai/superfusion)**|None|Hao Dong et.al.|
|**2022-11-25**|**[Copy-Pasting Coherent Depth Regions Improves Contrastive Learning for Urban-Scene Segmentation](http://arxiv.org/abs/2211.14074v1)**|None|**[link](https://github.com/leungtsang/cpcdr)**|BMVC 2022 Best Student Paper Award(Honourable Mention)|Liang Zeng et.al.|
|**2022-11-23**|**[Lite-Mono: A Lightweight CNN and Transformer Architecture for Self-Supervised Monocular Depth Estimation](http://arxiv.org/abs/2211.13202v2)**|None|**[link](https://github.com/noahzn/lite-mono)**|Accepted to CVPR2023|Ning Zhang et.al.|
|**2022-11-22**|**[Dynamic Depth-Supervised NeRF for Multi-View RGB-D Operating Room Images](http://arxiv.org/abs/2211.12436v2)**|None|None|Accepted to the Workshop on Ambient Intelligence for HealthCare 2023|Beerend G. A. Gerats et.al.|
|**2022-11-22**|**[Event Transformer+. A multi-purpose solution for efficient event data processing](http://arxiv.org/abs/2211.12222v2)**|None|None|arXiv admin note: text overlap with arXiv:2204.03355|Alberto Sabater et.al.|
|**2022-11-22**|**[The Monocular Depth Estimation Challenge](http://arxiv.org/abs/2211.12174v1)**|None|**[link](https://github.com/jspenmar/monodepth_benchmark)**|WACV-Workshops 2023|Jaime Spencer et.al.|
|**2022-11-20**|**[Hybrid Transformer Based Feature Fusion for Self-Supervised Monocular Depth Estimation](http://arxiv.org/abs/2211.11066v1)**|None|None|Presented at the Advances in Image Manipulation Workshop at ECCV 2022|Snehal Singh Tomar et.al.|
|**2022-11-19**|**[A Practical Stereo Depth System for Smart Glasses](http://arxiv.org/abs/2211.10551v2)**|None|None|Accepted at CVPR2023|Jialiang Wang et.al.|
|**2022-11-18**|**[Improving Pixel-Level Contrastive Learning by Leveraging Exogenous Depth Information](http://arxiv.org/abs/2211.10177v1)**|None|None|Accepted for WACV 2023|Ahmed Ben Saad et.al.|
|**2022-11-16**|**[SelfOdom: Self-supervised Egomotion and Depth Learning via Bi-directional Coarse-to-Fine Scale Recovery](http://arxiv.org/abs/2211.08904v2)**|None|None|14 pages, 8 figures, in submission|Hao Qu et.al.|
|**2022-11-16**|**[LightDepth: A Resource Efficient Depth Estimation Approach for Dealing with Ground Truth Sparsity via Curriculum Learning](http://arxiv.org/abs/2211.08608v2)**|None|**[link](https://github.com/fatemehkarimii/lightdepth)**|13 pages, 4 figures|Fatemeh Karimi et.al.|
|**2022-11-10**|**[Unifying Flow, Stereo and Depth Estimation](http://arxiv.org/abs/2211.05783v3)**|None|**[link](https://github.com/autonomousvision/unimatch)**|TPAMI 2023, Project Page: https://haofeixu.github.io/unimatch, Code:   https://github.com/autonomousvision/unimatch, Demo:   https://huggingface.co/spaces/haofeixu/unimatch|Haofei Xu et.al.|
|**2022-11-07**|**[Efficient Single-Image Depth Estimation on Mobile Devices, Mobile AI & AIM 2022 Challenge: Report](http://arxiv.org/abs/2211.04470v1)**|None|None|arXiv admin note: substantial text overlap with arXiv:2105.08630,   arXiv:2211.03885; text overlap with arXiv:2105.08819, arXiv:2105.08826,   arXiv:2105.08629, arXiv:2105.07809, arXiv:2105.07825|Andrey Ignatov et.al.|
|**2022-11-07**|**[SC-DepthV3: Robust Self-supervised Monocular Depth Estimation for Dynamic Scenes](http://arxiv.org/abs/2211.03660v2)**|None|**[link](https://github.com/JiawangBian/sc_depth_pl)**|Accepted for publication in TPAMI; The code will be available at   https://github.com/JiawangBian/sc_depth_pl|Libo Sun et.al.|
|**2022-11-04**|**[RCDPT: Radar-Camera fusion Dense Prediction Transformer](http://arxiv.org/abs/2211.02432v2)**|None|**[link](https://github.com/lochenchou/rcdpt)**|5 pages, 2 figures and 1 table, accepted to ICASSP2023|Chen-Chou Lo et.al.|
|**2022-11-02**|**[OPA-3D: Occlusion-Aware Pixel-Wise Aggregation for Monocular 3D Object Detection](http://arxiv.org/abs/2211.01142v1)**|None|None|None|Yongzhi Su et.al.|
|**2022-10-31**|**[Multi-Camera Calibration Free BEV Representation for 3D Object Detection](http://arxiv.org/abs/2210.17252v1)**|None|None|15 pages, 7 figures|Hongxiang Jiang et.al.|
|**2022-10-29**|**[Boosting Monocular 3D Object Detection with Object-Centric Auxiliary Depth Supervision](http://arxiv.org/abs/2210.16574v1)**|None|None|Accepted by IEEE Transaction on Intelligent Transportation System   (T-ITS)|Youngseok Kim et.al.|
|**2022-10-28**|**[Matching entropy based disparity estimation from light field](http://arxiv.org/abs/2210.15948v2)**|None|None|None|Ligen Shi et.al.|
|**2022-10-27**|**[Robust Monocular Localization of Drones by Adapting Domain Maps to Depth Prediction Inaccuracies](http://arxiv.org/abs/2210.15559v1)**|None|None|None|Priyesh Shukla et.al.|
|**2022-10-27**|**[2T-UNET: A Two-Tower UNet with Depth Clues for Robust Stereo Depth Estimation](http://arxiv.org/abs/2210.15374v1)**|None|None|None|Rohit Choudhary et.al.|
|**2022-10-23**|**[Photo-realistic Neural Domain Randomization](http://arxiv.org/abs/2210.12682v1)**|None|None|Accepted to European Conference on Computer Vision (ECCV), 2022|Sergey Zakharov et.al.|
|**2022-10-21**|**[Context-Enhanced Stereo Transformer](http://arxiv.org/abs/2210.11719v1)**|None|**[link](https://github.com/guoweiyu/context-enhanced-stereo-transformer)**|Accepted by ECCV2022|Weiyu Guo et.al.|
|**2022-10-19**|**[CroCo: Self-Supervised Pre-training for 3D Vision Tasks by Cross-View Completion](http://arxiv.org/abs/2210.10716v2)**|None|**[link](https://github.com/naver/croco)**|NeurIPS 2022|Philippe Weinzaepfel et.al.|
|**2022-10-18**|**[Hierarchical Normalization for Robust Monocular Depth Estimation](http://arxiv.org/abs/2210.09670v1)**|None|None|Accepted to NeurIPS 2022|Chi Zhang et.al.|
|**2022-10-17**|**[Attention Attention Everywhere: Monocular Depth Prediction with Skip Attention](http://arxiv.org/abs/2210.09071v1)**|None|**[link](https://github.com/ashutosh1807/pixelformer)**|Accepted at IEEE/CVF Winter Conference on Applications of Computer   Vision (WACV) 2023|Ashutosh Agarwal et.al.|
|**2022-10-14**|**[MonoDVPS: A Self-Supervised Monocular Depth Estimation Approach to Depth-aware Video Panoptic Segmentation](http://arxiv.org/abs/2210.07577v1)**|None|None|WACV 2023|Andra Petrovai et.al.|
|**2022-10-13**|**[Composite Learning for Robust and Effective Dense Predictions](http://arxiv.org/abs/2210.07239v1)**|None|None|Winter Conference on Applications of Computer Vision (WACV), 2023|Menelaos Kanakis et.al.|
|**2022-10-13**|**[MonoNeRF: Learning Generalizable NeRFs from Monocular Videos without Camera Pose](http://arxiv.org/abs/2210.07181v2)**|None|None|ICML 2023 camera ready version. Project page:   https://oasisyang.github.io/mononerf|Yang Fu et.al.|
|**2022-10-13**|**[Multi-Task Meta Learning: learn how to adapt to unseen tasks](http://arxiv.org/abs/2210.06989v4)**|None|**[link](https://github.com/ricupa/mtml-learn-how-to-adapt-to-unseen-tasks)**|None|Richa Upadhyay et.al.|
|**2022-10-13**|**[Improving the Reliability for Confidence Estimation](http://arxiv.org/abs/2210.06776v1)**|None|None|Accepted by ECCV 2022|Haoxuan Qu et.al.|
|**2022-10-11**|**[Frequency-Aware Self-Supervised Monocular Depth Estimation](http://arxiv.org/abs/2210.05479v2)**|None|**[link](https://github.com/xingyuuchen/freq-aware-depth)**|8 pages, 5 figures, published to WACV2023|Xingyu Chen et.al.|
|**2022-10-08**|**[Detaching and Boosting: Dual Engine for Scale-Invariant Self-Supervised Monocular Depth Estimation](http://arxiv.org/abs/2210.03952v2)**|IEEE Robotics and Automation Letters 7.4 (2022): 12094-12101|**[link](https://github.com/attackonmuggle/dab_net0)**|Accepted by ICLR and IEEE Robotics and Automation Letters (RAL)|Peizhe Jiang et.al.|
|**2022-10-07**|**[IronDepth: Iterative Refinement of Single-View Depth using Surface Normal and its Uncertainty](http://arxiv.org/abs/2210.03676v1)**|None|**[link](https://github.com/baegwangbin/irondepth)**|BMVC 2022|Gwangbin Bae et.al.|
|**2022-10-06**|**[Self-Supervised Monocular Depth Underwater](http://arxiv.org/abs/2210.03206v1)**|None|None|None|Shlomi Amitai et.al.|
|**2022-10-06**|**[FloatingFusion: Depth from ToF and Image-stabilized Stereo Cameras](http://arxiv.org/abs/2210.02785v1)**|ECCV 2022, Part I, LNCS 13661|None|None|Andreas Meuleman et.al.|
|**2022-10-05**|**[Depth Is All You Need for Monocular 3D Detection](http://arxiv.org/abs/2210.02493v1)**|None|None|None|Dennis Park et.al.|
|**2022-10-05**|**[Image Masking for Robust Self-Supervised Monocular Depth Estimation](http://arxiv.org/abs/2210.02357v2)**|None|**[link](https://github.com/neurai-lab/mimdepth)**|Accepted at 2023 IEEE International Conference on Robotics and   Automation (ICRA)|Hemang Chawla et.al.|
|**2022-10-05**|**[MOTSLAM: MOT-assisted monocular dynamic SLAM using single-view depth estimation](http://arxiv.org/abs/2210.02038v1)**|None|None|None|Hanwei Zhang et.al.|
|**2022-10-05**|**[Multi-Camera Collaborative Depth Prediction via Consistent Structure Estimation](http://arxiv.org/abs/2210.02009v1)**|None|None|None|Jialei Xu et.al.|
|**2022-10-04**|**[PlaneDepth: Self-supervised Depth Estimation via Orthogonal Planes](http://arxiv.org/abs/2210.01612v3)**|None|**[link](https://github.com/svip-lab/planedepth)**|Accepted by CVPR 2023. Code and models are available at:   https://github.com/svip-lab/PlaneDepth|Ruoyu Wang et.al.|
|**2022-10-04**|**[FreDSNet: Joint Monocular Depth and Semantic Segmentation with Fast Fourier Convolutions](http://arxiv.org/abs/2210.01595v2)**|None|**[link](https://github.com/sbrunoberenguel/fredsnet)**|7 pages, 5 figures, 3 tables|Bruno Berenguel-Baeta et.al.|
|**2022-10-04**|**[Non-learning Stereo-aided Depth Completion under Mis-projection via Selective Stereo Matching](http://arxiv.org/abs/2210.01436v1)**|in IEEE Access, vol. 9, pp. 136674-136686, 2021|None|15 pages, 13 figures|Yasuhiro Yao et.al.|
|**2022-10-03**|**[Probabilistic Volumetric Fusion for Dense Monocular SLAM](http://arxiv.org/abs/2210.01276v2)**|None|None|9 pages, 6 figures, 2 tables|Antoni Rosinol et.al.|
|**2022-10-03**|**[Expediting Large-Scale Vision Transformer for Dense Prediction without Fine-tuning](http://arxiv.org/abs/2210.01035v1)**|None|**[link](https://github.com/Expedit-LargeScale-Vision-Transformer/Expedit-Segmenter)**|Accepted at NeurIPS 2022, camera-ready version, 22 pages, 14 figures|Weicong Liang et.al.|
|**2022-10-02**|**[Self-Supervised Monocular Depth Estimation: Solving the Edge-Fattening Problem](http://arxiv.org/abs/2210.00411v3)**|None|**[link](https://github.com/xingyuuchen/tri-depth)**|8 pages, 7 figures, published to WACV2023|Xingyu Chen et.al.|
|**2022-09-29**|**[Lightweight Monocular Depth Estimation with an Edge Guided Network](http://arxiv.org/abs/2209.14829v1)**|None|None|None|Xingshuai Dong et.al.|
|**2022-09-27**|**[Scalable and Equivariant Spherical CNNs by Discrete-Continuous (DISCO) Convolutions](http://arxiv.org/abs/2209.13603v3)**|None|None|19 pages, 7 figures, accepted by ICLR 2023|Jeremy Ocampo et.al.|
|**2022-09-27**|**[Towards Multimodal Multitask Scene Understanding Models for Indoor Mobile Agents](http://arxiv.org/abs/2209.13156v1)**|None|None|Submitted to ICRA2023|Yao-Hung Hubert Tsai et.al.|
|**2022-09-26**|**[SAPA: Similarity-Aware Point Affiliation for Feature Upsampling](http://arxiv.org/abs/2209.12866v2)**|None|**[link](https://github.com/poppinace/sapa)**|Accepted to NeurIPS 2022. Code is available at   https://github.com/poppinace/sapa|Hao Lu et.al.|
|**2022-09-26**|**[DeepFusion: A Robust and Modular 3D Object Detector for Lidars, Cameras and Radars](http://arxiv.org/abs/2209.12729v2)**|None|None|None|Florian Drews et.al.|
|**2022-09-26**|**[UDepth: Fast Monocular Depth Estimation for Visually-guided Underwater Robots](http://arxiv.org/abs/2209.12358v2)**|None|**[link](https://github.com/uf-robopi/udepth)**|10 pages, 6 figures|Boxiao Yu et.al.|
|**2022-09-23**|**[Image-to-Image Translation for Autonomous Driving from Coarsely-Aligned Image Pairs](http://arxiv.org/abs/2209.11673v1)**|None|None|Submitted to the International Conference on Robotics and Automation   (ICRA) 2023|Youya Xia et.al.|
|**2022-09-19**|**[3D-PL: Domain Adaptive Depth Estimation with 3D-aware Pseudo-Labeling](http://arxiv.org/abs/2209.09231v1)**|None|**[link](https://github.com/ccc870206/3d-pl)**|Accepted in ECCV 2022. Project page:   https://ccc870206.github.io/3D-PL/|Yu-Ting Yen et.al.|
|**2022-09-19**|**[On Robust Cross-View Consistency in Self-Supervised Monocular Depth Estimation](http://arxiv.org/abs/2209.08747v3)**|Machine Intelligence Research 2024|**[link](https://github.com/sunnyhelen/rcvc-depth)**|None|Haimei Zhao et.al.|
|**2022-09-18**|**[SF2SE3: Clustering Scene Flow into SE(3)-Motions via Proposal and Selection](http://arxiv.org/abs/2209.08532v2)**|None|**[link](https://github.com/lmb-freiburg/sf2se3)**|German Conference on Pattern Recognition 2022, Konstanz, Germany|Leonhard Sommer et.al.|
|**2022-09-18**|**[TODE-Trans: Transparent Object Depth Estimation with Transformer](http://arxiv.org/abs/2209.08455v1)**|None|**[link](https://github.com/yuchendoudou/tode)**|Submitted to ICRA2023|Kang Chen et.al.|
|**2022-09-15**|**[Self-distilled Feature Aggregation for Self-supervised Monocular Depth Estimation](http://arxiv.org/abs/2209.07088v1)**|None|**[link](https://github.com/ZM-Zhou/SMDE-Pytorch)**|Accepted to ECCV 2022|Zhengming Zhou et.al.|
|**2022-09-13**|**[A Benchmark and a Baseline for Robust Multi-view Depth Estimation](http://arxiv.org/abs/2209.06681v1)**|None|**[link](https://github.com/lmb-freiburg/robustmvd)**|Accepted at 3DV 2022|Philipp Schröppel et.al.|
|**2022-09-14**|**[FCDSN-DC: An Accurate and Lightweight Convolutional Neural Network for Stereo Estimation with Depth Completion](http://arxiv.org/abs/2209.06525v1)**|None|**[link](https://github.com/thedodo/fcdsn-dc)**|None|Dominik Hirner et.al.|
|**2022-09-14**|**[DevNet: Self-supervised Monocular Depth Learning via Density Volume Construction](http://arxiv.org/abs/2209.06351v4)**|None|**[link](https://github.com/gitkaichenzhou/devnet)**|Accepted by European Conference on Computer Vision 2022 (ECCV2022)|Kaichen Zhou et.al.|
|**2022-09-12**|**[StructNeRF: Neural Radiance Fields for Indoor Scenes with Structural Hints](http://arxiv.org/abs/2209.05277v1)**|None|None|None|Zheng Chen et.al.|
|**2022-09-07**|**[BiFuse++: Self-supervised and Efficient Bi-projection Fusion for 360 Depth Estimation](http://arxiv.org/abs/2209.02952v1)**|None|**[link](https://github.com/fuenwang/bifusev2)**|Accepted in TPAMI 2022; Code: https://github.com/fuenwang/BiFusev2|Fu-En Wang et.al.|
|**2022-09-03**|**[A comprehensive survey on recent deep learning-based methods applied to surgical data](http://arxiv.org/abs/2209.01435v4)**|None|None|This paper is to be submitted to International journal of computer   vision|Mansoor Ali et.al.|
|**2022-09-02**|**[LiteDepth: Digging into Fast and Accurate Depth Estimation on Mobile Devices](http://arxiv.org/abs/2209.00961v1)**|None|**[link](https://github.com/zhyever/litedepth)**|Accepted to European Conference on Computer Vision Workshop (ECCVW   2022)|Zhenyu Li et.al.|
|**2022-08-31**|**[SimpleRecon: 3D Reconstruction Without 3D Convolutions](http://arxiv.org/abs/2208.14743v1)**|None|None|ECCV2022 version with improved timings. 14 pages + 5 pages of   references|Mohamed Sayed et.al.|
|**2022-08-30**|**[Synthehicle: Multi-Vehicle Multi-Camera Tracking in Virtual Cities](http://arxiv.org/abs/2208.14167v1)**|None|**[link](https://github.com/fubel/synthehicle)**|None|Fabian Herzog et.al.|
|**2022-08-29**|**[SphereDepth: Panorama Depth Estimation from Spherical Domain](http://arxiv.org/abs/2208.13714v3)**|None|None|Conference accept at 3DV 2022|Qingsong Yan et.al.|
|**2022-08-29**|**[A Practical Calibration Method for RGB Micro-Grid Polarimetric Cameras](http://arxiv.org/abs/2208.13485v1)**|Robotics and Automation Letters - Volume: 7 - Issue: 4 - October   2022 - Pages: 9921 - 9928|**[link](https://github.com/vibot-lab/policalibration)**|This is a preprint version of the paper to appear at IEEE Robotics   and Automation Letters (RAL). The final journal version will be available at   https://doi.org/10.1109/LRA.2022.3192655|Joaquin Rodriguez et.al.|
|**2022-08-29**|**[Rethinking Skip Connections in Encoder-decoder Networks for Monocular Depth Estimation](http://arxiv.org/abs/2208.13441v1)**|None|None|None|Zhitong Lai et.al.|
|**2022-08-28**|**[Towards Accurate Reconstruction of 3D Scene Shape from A Single Monocular Image](http://arxiv.org/abs/2208.13241v2)**|None|**[link](https://github.com/aim-uofa/depth)**|20 pages. Journal version of the conference paper "Learning to   Recover 3D Scene Shape from a Single Image". arXiv admin note: substantial   text overlap with arXiv:2012.09365|Wei Yin et.al.|
|**2022-08-27**|**[Neural Camera Models](http://arxiv.org/abs/2208.12903v1)**|None|None|PhD thesis|Igor Vasiljevic et.al.|
|**2022-08-26**|**[Uncertainty Guided Depth Fusion for Spike Camera](http://arxiv.org/abs/2208.12653v2)**|None|None|18 pages, 11 figures|Jianing Li et.al.|
|**2022-08-26**|**[Unsupervised Spike Depth Estimation via Cross-modality Cross-domain Knowledge Transfer](http://arxiv.org/abs/2208.12527v2)**|None|**[link](https://github.com/theia-4869/bicross)**|None|Jiaming Liu et.al.|
|**2022-08-26**|**[Dense Depth Distillation with Out-of-Distribution Simulated Images](http://arxiv.org/abs/2208.12464v3)**|None|None|None|Junjie Hu et.al.|
|**2022-08-23**|**[DepthFake: a depth-based strategy for detecting Deepfake videos](http://arxiv.org/abs/2208.11074v1)**|Springer 2022|None|2022 ICPR Workshop on Artificial Intelligence for Multimedia   Forensics and Disinformation Detection|Luca Maiano et.al.|
|**2022-08-23**|**[PIFu for the Real World: A Self-supervised Framework to Reconstruct Dressed Human from Single-view Images](http://arxiv.org/abs/2208.10769v2)**|None|None|CVM 2024|Zhangyang Xiong et.al.|
|**2022-08-23**|**[Depth Map Decomposition for Monocular Depth Estimation](http://arxiv.org/abs/2208.10762v1)**|None|**[link](https://github.com/jyjunmcl/Depth-Map-Decomposition)**|None|Jinyoung Jun et.al.|
|**2022-08-22**|**[Minimizing the Effect of Noise and Limited Dataset Size in Image Classification Using Depth Estimation as an Auxiliary Task with Deep Multitask Learning](http://arxiv.org/abs/2208.10390v1)**|None|None|None|Khashayar Namdar et.al.|
|**2022-08-21**|**[Multi-task Learning for Monocular Depth and Defocus Estimations with Real Images](http://arxiv.org/abs/2208.09848v1)**|None|None|None|Renzhi He et.al.|
|**2022-08-20**|**[Learning Sub-Pixel Disparity Distribution for Light Field Depth Estimation](http://arxiv.org/abs/2208.09688v3)**|None|**[link](https://github.com/chaowentao/subfocal)**|Accepted by IEEE Transactions on Computational Imaging|Wentao Chao et.al.|
|**2022-08-20**|**[Net2Brain: A Toolbox to compare artificial vision models with human brain responses](http://arxiv.org/abs/2208.09677v2)**|None|**[link](https://github.com/toastydom/net2brain)**|4 Pages, 3 figures, submitted and accepted to CCNeuro 2022. For   associated repository, see https://github.com/ToastyDom/Net2Brain Update 1:   Changed Citation|Domenic Bersch et.al.|
|**2022-08-19**|**[MonoSIM: Simulating Learning Behaviors of Heterogeneous Point Cloud Object Detectors for Monocular 3D Object Detection](http://arxiv.org/abs/2208.09446v2)**|None|**[link](https://github.com/sunh18/monosim)**|None|Han Sun et.al.|
|**2022-08-19**|**[Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning](http://arxiv.org/abs/2208.09170v1)**|None|**[link](https://github.com/jeffwang987/movedepth)**|code: https://github.com/JeffWang987/MOVEDepth|Xiaofeng Wang et.al.|
|**2022-08-17**|**[Self-Supervised Depth Estimation in Laparoscopic Image using 3D Geometric Consistency](http://arxiv.org/abs/2208.08407v2)**|None|**[link](https://github.com/br0202/M3Depth)**|Accepted by MICCAI2022|Baoru Huang et.al.|
|**2022-08-16**|**[Rain Removal from Light Field Images with 4D Convolution and Multi-scale Gaussian Process](http://arxiv.org/abs/2208.07735v2)**|IEEE Transactions on Image Processing (2023), v32, pages 921-936|**[link](https://github.com/yt3dvision/4d-mgp-srrnet)**|This paper has been published on IEEE Transactions on Image   Processing|Tao Yan et.al.|
|**2022-08-06**|**[MonoViT: Self-Supervised Monocular Depth Estimation with a Vision Transformer](http://arxiv.org/abs/2208.03543v1)**|None|**[link](https://github.com/zxcqlf/monovit)**|Accepted by 3DV 2022|Chaoqiang Zhao et.al.|
|**2022-08-03**|**[Gradient-based Uncertainty for Monocular Depth Estimation](http://arxiv.org/abs/2208.02005v1)**|None|**[link](https://github.com/jhornauer/grumodepth)**|Accepted to ECCV 2022|Julia Hornauer et.al.|
|**2022-08-03**|**[Neural Contourlet Network for Monocular 360 Depth Estimation](http://arxiv.org/abs/2208.01817v1)**|None|**[link](https://github.com/zhijieshen-bjtu/neural-contourlet-network-for-mode)**|IEEE Transactions on Circuits and Systems for Video Technology|Zhijie Shen et.al.|
|**2022-08-02**|**[Deconstructing Self-Supervised Monocular Reconstruction: The Design Decisions that Matter](http://arxiv.org/abs/2208.01489v4)**|Transactions of Machine Learning Research 2022|**[link](https://github.com/jspenmar/monodepth_benchmark)**|https://github.com/jspenmar/monodepth_benchmark|Jaime Spencer et.al.|
|**2022-08-01**|**[Ithaca365: Dataset and Driving Perception under Repeated and Challenging Weather Conditions](http://arxiv.org/abs/2208.01166v1)**|None|None|Accepted by CVPR 2022|Carlos A. Diaz-Ruiz et.al.|
|**2022-07-31**|**[Less is More: Consistent Video Depth Estimation with Masked Frames Modeling](http://arxiv.org/abs/2208.00380v2)**|None|**[link](https://github.com/raymondwang987/fmnet)**|Accepted by ACM MM 2022|Yiran Wang et.al.|
|**2022-07-30**|**[Learning Feature Decomposition for Domain Adaptive Monocular Depth Estimation](http://arxiv.org/abs/2208.00160v1)**|None|None|Accepted at IEEE/RSJ International Conference on Intelligent Robots   and Systems (IROS) 2022|Shao-Yuan Lo et.al.|
|**2022-07-28**|**[Depth Field Networks for Generalizable Multi-view Scene Representation](http://arxiv.org/abs/2207.14287v1)**|None|None|Accepted to ECCV 2022. Project page:   https://sites.google.com/view/tri-define|Vitor Guizilini et.al.|
|**2022-07-26**|**[Monocular 3D Object Detection with Depth from Motion](http://arxiv.org/abs/2207.12988v2)**|None|**[link](https://github.com/tai-wang/depth-from-motion)**|ECCV 2022 Oral|Tai Wang et.al.|
|**2022-07-25**|**[Cost Volume Pyramid Network with Multi-strategies Range Searching for Multi-view Stereo](http://arxiv.org/abs/2207.12032v1)**|None|**[link](https://github.com/SibylGao/MSCVP-MVSNet)**|Accepted by CGI2022|Shiyu Gao et.al.|
|**2022-07-25**|**[RA-Depth: Resolution Adaptive Self-Supervised Monocular Depth Estimation](http://arxiv.org/abs/2207.11984v2)**|None|**[link](https://github.com/hmhemu/ra-depth)**|Accepted to ECCV'22|Mu He et.al.|
|**2022-07-21**|**[DEVIANT: Depth EquiVarIAnt NeTwork for Monocular 3D Object Detection](http://arxiv.org/abs/2207.10758v1)**|None|**[link](https://github.com/abhi1kumar/deviant)**|ECCV 2022|Abhinav Kumar et.al.|
|**2022-07-20**|**[Latent Discriminant deterministic Uncertainty](http://arxiv.org/abs/2207.10130v1)**|None|**[link](https://github.com/ensta-u2is/ldu)**|24 pages. Accepted at ECCV 2022|Gianni Franchi et.al.|
|**2022-07-20**|**[Densely Constrained Depth Estimator for Monocular 3D Object Detection](http://arxiv.org/abs/2207.10047v3)**|None|**[link](https://github.com/bravegroup/dcd)**|Accepted by ECCV2022|Yingyan Li et.al.|
|**2022-07-20**|**[Learning Depth from Focus in the Wild](http://arxiv.org/abs/2207.09658v2)**|None|**[link](https://github.com/wcy199705/dffinthewild)**|None|Changyeon Won et.al.|
|**2022-07-18**|**[MonoIndoor++:Towards Better Practice of Self-Supervised Monocular Depth Estimation for Indoor Environments](http://arxiv.org/abs/2207.08951v1)**|None|None|Journal version of "MonoIndoor: Towards Good Practice of   Self-Supervised Monocular Depth Estimation for Indoor   Environments"(ICCV-2021). arXiv admin note: substantial text overlap with   arXiv:2107.12429|Runze Li et.al.|
|**2022-07-18**|**[DID-M3D: Decoupling Instance Depth for Monocular 3D Object Detection](http://arxiv.org/abs/2207.08531v2)**|None|**[link](https://github.com/spengliang/did-m3d)**|ECCV 2022|Liang Peng et.al.|
|**2022-07-16**|**[DiffuStereo: High Quality Human Reconstruction via Diffusion-based Stereo Using Sparse Cameras](http://arxiv.org/abs/2207.08000v2)**|None|None|Accepted by ECCV2022|Ruizhi Shao et.al.|
|**2022-07-16**|**[Mutual Adaptive Reasoning for Monocular 3D Multi-Person Pose Estimation](http://arxiv.org/abs/2207.07900v1)**|None|None|Accepted by ACM MM 2022|Juze Zhang et.al.|
|**2022-07-16**|**[JPerceiver: Joint Perception Network for Depth, Pose and Layout Estimation in Driving Scenes](http://arxiv.org/abs/2207.07895v1)**|None|**[link](https://github.com/sunnyhelen/jperceiver)**|Accepted by ECCV 2022|Haimei Zhao et.al.|
|**2022-07-14**|**[Adversarial Attacks on Monocular Pose Estimation](http://arxiv.org/abs/2207.07032v1)**|None|**[link](https://github.com/neurai-lab/mono-pose-attack)**|Accepted at the 2022 IEEE/RSJ International Conference on Intelligent   Robots and Systems (IROS 2022)|Hemang Chawla et.al.|
|**2022-07-14**|**[BayesCap: Bayesian Identity Cap for Calibrated Uncertainty in Frozen Neural Networks](http://arxiv.org/abs/2207.06873v1)**|None|**[link](https://github.com/explainableml/bayescap)**|Accepted at ECCV 2022. Code is available at   https://github.com/ExplainableML/BayesCap|Uddeshya Upadhyay et.al.|
|**2022-07-13**|**[Joint Prediction of Monocular Depth and Structure using Planar and Parallax Geometry](http://arxiv.org/abs/2207.06351v1)**|None|None|Pattern Recognition, May 2022|Hao Xing et.al.|
|**2022-07-13**|**[Robust and accurate depth estimation by fusing LiDAR and Stereo](http://arxiv.org/abs/2207.06139v1)**|Meas. Sci. Technol. 34 125107 (2023)|None|None|Guangyao Xu et.al.|
|**2022-07-11**|**[Hybrid Skip: A Biologically Inspired Skip Connection for the UNet Architecture](http://arxiv.org/abs/2207.04721v1)**|IEEE Access, Volume 10, 53928 - 53939, 17 May 2022|None|Project page at https://vcl3d.github.io/HybridSkip/|Nikolaos Zioulis et.al.|
|**2022-07-11**|**[Physical Attack on Monocular Depth Estimation with Optimal Adversarial Patches](http://arxiv.org/abs/2207.04718v1)**|None|**[link](https://github.com/Bob-cheng/MDE_Attack)**|ECCV2022|Zhiyuan Cheng et.al.|
|**2022-07-10**|**[Depth Perspective-aware Multiple Object Tracking](http://arxiv.org/abs/2207.04551v2)**|None|None|In review PR journal|Kha Gia Quach et.al.|
|**2022-07-10**|**[Depthformer : Multiscale Vision Transformer For Monocular Depth Estimation With Local Global Information Fusion](http://arxiv.org/abs/2207.04535v2)**|International Conference on Image Processing (ICIP), 2022|**[link](https://github.com/ashutosh1807/depthformer)**|None|Ashutosh Agarwal et.al.|
|**2022-07-09**|**[Direct Handheld Burst Imaging to Simulated Defocus](http://arxiv.org/abs/2207.04175v2)**|None|None|ICIP 2022|Meng-Lin Wu et.al.|
|**2022-07-08**|**[BlindSpotNet: Seeing Where We Cannot See](http://arxiv.org/abs/2207.03870v1)**|None|None|None|Taichi Fukuda et.al.|
|**2022-07-07**|**[False Negative Reduction in Semantic Segmentation under Domain Shift using Depth Estimation](http://arxiv.org/abs/2207.03513v2)**|None|**[link](https://github.com/kmaag/fn-reduction-using-depth)**|None|Kira Maag et.al.|
|**2022-07-07**|**[DRL-ISP: Multi-Objective Camera ISP with Deep Reinforcement Learning](http://arxiv.org/abs/2207.03081v1)**|None|None|Accepted by IEEE/RSJ International Conference on Intelligent Robots   and Systems (IROS), 2022 (*First two authors are equal contributed)|Ukcheol Shin et.al.|
|**2022-07-06**|**[Network Binarization via Contrastive Learning](http://arxiv.org/abs/2207.02970v3)**|None|**[link](https://github.com/42Shawn/CMIM)**|Accepted to ECCV 2022|Yuzhang Shang et.al.|
|**2022-07-06**|**[Gaze-Vergence-Controlled See-Through Vision in Augmented Reality](http://arxiv.org/abs/2207.02645v1)**|None|None|11 papges, 13 figures|Zhimin Wang et.al.|
|**2022-07-05**|**[Multiview Detection with Cardboard Human Modeling](http://arxiv.org/abs/2207.02013v5)**|None|**[link](https://github.com/zichengduan/mvchm)**|None|Jiahao Ma et.al.|
|**2022-07-03**|**[Beyond Visual Field of View: Perceiving 3D Environment with Echoes and Vision](http://arxiv.org/abs/2207.01136v2)**|None|None|None|Lingyu Zhu et.al.|
|**2022-07-03**|**[Can Language Understand Depth?](http://arxiv.org/abs/2207.01077v3)**|ACM Multimedia 2022 (Brave New Idea)|**[link](https://github.com/adonis-galaxy/depthclip)**|None|Renrui Zhang et.al.|
|**2022-07-01**|**[How Far Can I Go ? : A Self-Supervised Approach for Deterministic Video Depth Forecasting](http://arxiv.org/abs/2207.00506v2)**|None|**[link](https://github.com/sauradip/depthforecasting)**|Accepted in ML4AD Workshop, NeurIPS 2021|Sauradip Nag et.al.|
|**2022-07-01**|**[Recovering Detail in 3D Shapes Using Disparity Maps](http://arxiv.org/abs/2207.00182v2)**|None|None|None|Marissa Ramirez de Chanlatte et.al.|
|**2022-06-28**|**[Accurate and Real-time Pseudo Lidar Detection: Is Stereo Neural Network Really Necessary?](http://arxiv.org/abs/2206.13858v1)**|None|None|None|Haitao Meng et.al.|
|**2022-06-27**|**[LaRa: Latents and Rays for Multi-Camera Bird's-Eye-View Semantic Segmentation](http://arxiv.org/abs/2206.13294v2)**|CoRL 2022 https://openreview.net/forum?id=abd_D-iVjk0|**[link](https://github.com/valeoai/LaRa)**|None|Florent Bartoccioni et.al.|
|**2022-06-27**|**[Monocular Depth Decomposition of Semi-Transparent Volume Renderings](http://arxiv.org/abs/2206.13282v2)**|None|**[link](https://github.com/xeTaiz/MonocularDepthDecomposition)**|accepted at IEEE TVCG 2023|Dominik Engel et.al.|
|**2022-06-27**|**[MGNet: Monocular Geometric Scene Understanding for Autonomous Driving](http://arxiv.org/abs/2206.13199v1)**|2021 IEEE/CVF International Conference on Computer Vision (ICCV),   2021, pp. 15784-15795|**[link](https://github.com/markusschoen/mgnet)**|None|Markus Schön et.al.|
|**2022-06-24**|**[Ev-NeRF: Event Based Neural Radiance Field](http://arxiv.org/abs/2206.12455v2)**|None|None|Accepted to WACV 2023|Inwoo Hwang et.al.|
|**2022-06-23**|**[Learning Viewpoint-Agnostic Visual Representations by Recovering Tokens in 3D Space](http://arxiv.org/abs/2206.11895v4)**|None|**[link](https://github.com/elicassion/3dtrl)**|NeurIPS 2022. Our code is at https://github.com/elicassion/3DTRL Our   project page is at https://www3.cs.stonybrook.edu/~jishang/3dtrl/3dtrl.html   v3, v4 for minor updates on figures and visualizations|Jinghuan Shang et.al.|
|**2022-06-22**|**[Monocular Spherical Depth Estimation with Explicitly Connected Weak Layout Cues](http://arxiv.org/abs/2206.11358v1)**|ISPRS Journal of Photogrammetry and Remote Sensing, Volume 183,   January 2022, Pages 269-285|None|Project page at https://vcl3d.github.io/ExplicitLayoutDepth/|Nikolaos Zioulis et.al.|
|**2022-06-22**|**[A High Resolution Multi-exposure Stereoscopic Image & Video Database of Natural Scenes](http://arxiv.org/abs/2206.11095v1)**|None|None|None|Rohit Choudhary et.al.|
|**2022-06-21**|**[Semantics-Depth-Symbiosis: Deeply Coupled Semi-Supervised Learning of Semantics and Depth](http://arxiv.org/abs/2206.10562v2)**|None|None|None|Nitin Bansal et.al.|
|**2022-06-21**|**[MEStereo-Du2CNN: A Novel Dual Channel CNN for Learning Robust Depth Estimates from Multi-exposure Stereo Images for HDR 3D Applications](http://arxiv.org/abs/2206.10375v1)**|None|None|None|Rohit Choudhary et.al.|
|**2022-06-21**|**[BEVDepth: Acquisition of Reliable Depth for Multi-view 3D Object Detection](http://arxiv.org/abs/2206.10092v2)**|None|**[link](https://github.com/megvii-basedetection/bevdepth)**|Accepted by AAAI2023|Yinhao Li et.al.|
|**2022-06-18**|**[Analysis & Computational Complexity Reduction of Monocular and Stereo Depth Estimation Techniques](http://arxiv.org/abs/2206.09071v1)**|None|**[link](https://github.com/rajeevpatwari/anynet)**|None|Rajeev Patwari et.al.|
|**2022-06-17**|**[Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks](http://arxiv.org/abs/2206.08916v2)**|None|None|None|Jiasen Lu et.al.|
|**2022-06-17**|**[Colonoscopy 3D Video Dataset with Paired Depth from 2D-3D Registration](http://arxiv.org/abs/2206.08903v3)**|None|None|None|Taylor L. Bobrow et.al.|
|**2022-06-15**|**[LET-3D-AP: Longitudinal Error Tolerant 3D Average Precision for Camera-Only 3D Detection](http://arxiv.org/abs/2206.07705v2)**|None|**[link](https://github.com/waymo-research/waymo-open-dataset)**|Find the primary metrics for the 2022 Waymo Open Dataset 3D   Camera-Only Detection Challenge at   https://waymo.com/open/challenges/2022/3d-camera-only-detection/ . Find the   code at https://github.com/waymo-research/waymo-open-dataset|Wei-Chih Hung et.al.|
|**2022-06-15**|**[MonoGround: Detecting Monocular 3D Objects from the Ground](http://arxiv.org/abs/2206.07372v1)**|None|**[link](https://github.com/cfzd/monoground)**|CVPR22|Zequn Qin et.al.|
|**2022-06-14**|**[TriHorn-Net: A Model for Accurate Depth-Based 3D Hand Pose Estimation](http://arxiv.org/abs/2206.07117v2)**|None|**[link](https://github.com/mrezaei92/TriHorn-Net)**|None|Mohammad Rezaei et.al.|
|**2022-06-08**|**[Learning Ego 3D Representation as Ray Tracing](http://arxiv.org/abs/2206.04042v3)**|None|**[link](https://github.com/fudan-zvg/ego3rt)**|ECCV 2022. Code is available at https://github.com/fudan-zvg/Ego3RT|Jiachen Lu et.al.|
|**2022-06-08**|**[Dyna-DM: Dynamic Object-aware Self-supervised Monocular Depth Maps](http://arxiv.org/abs/2206.03799v3)**|None|**[link](https://github.com/kieran514/dyna-dm)**|None|Kieran Saunders et.al.|
|**2022-06-08**|**[Unsupervised Learning of 3D Scene Flow from Monocular Camera](http://arxiv.org/abs/2206.03673v1)**|2021 IEEE International Conference on Robotics and Automation   (ICRA)|**[link](https://github.com/irmvlab/3dunmonoflow)**|ICRA2021|Guangming Wang et.al.|
|**2022-06-08**|**[Depth Estimation Matters Most: Improving Per-Object Depth Estimation for Monocular 3D Detection and Tracking](http://arxiv.org/abs/2206.03666v1)**|ICRA2022|None|None|Longlong Jing et.al.|
|**2022-06-08**|**[Delving into the Pre-training Paradigm of Monocular 3D Object Detection](http://arxiv.org/abs/2206.03657v2)**|None|None|None|Zhuoling Li et.al.|
|**2022-06-07**|**[Layered Depth Refinement with Mask Guidance](http://arxiv.org/abs/2206.03048v1)**|None|None|Accepted to CVPR 2022 (camera-ready version)|Soo Ye Kim et.al.|
|**2022-06-01**|**[PanopticDepth: A Unified Framework for Depth-aware Panoptic Segmentation](http://arxiv.org/abs/2206.00468v1)**|None|**[link](https://github.com/naiyugao/panopticdepth)**|CVPR2022|Naiyu Gao et.al.|
|**2022-05-30**|**[Self-Supervised Pre-training of Vision Transformers for Dense Prediction Tasks](http://arxiv.org/abs/2205.15173v2)**|None|None|None|Jaonary Rabarisoa et.al.|
|**2022-05-30**|**[SMUDLP: Self-Teaching Multi-Frame Unsupervised Endoscopic Depth Estimation with Learnable Patchmatch](http://arxiv.org/abs/2205.15034v1)**|None|None|10 pages|Shuwei Shao et.al.|
|**2022-05-28**|**[RIAV-MVS: Recurrent-Indexing an Asymmetric Volume for Multi-View Stereo](http://arxiv.org/abs/2205.14320v3)**|None|None|CVPR 2023|Changjiang Cai et.al.|
|**2022-05-26**|**[Revealing the Dark Secrets of Masked Image Modeling](http://arxiv.org/abs/2205.13543v2)**|None|**[link](https://github.com/SwinTransformer/MIM-Depth-Estimation)**|None|Zhenda Xie et.al.|
|**2022-05-24**|**[Wavelet Feature Maps Compression for Image-to-Image CNNs](http://arxiv.org/abs/2205.12268v4)**|None|**[link](https://github.com/BGUCompSci/WaveletCompressedConvolution)**|None|Shahaf E. Finder et.al.|
|**2022-05-24**|**[Single-View View Synthesis in the Wild with Learned Adaptive Multiplane Images](http://arxiv.org/abs/2205.11733v1)**|None|None|ACM SIGGRAPH 2022. Project page: https://yxuhan.github.io/AdaMPI/|Yuxuan Han et.al.|
|**2022-05-23**|**[Deep Digging into the Generalization of Self-Supervised Monocular Depth Estimation](http://arxiv.org/abs/2205.11083v3)**|None|**[link](https://github.com/sjg02122/MonoFormer)**|Accepted to AAAI 2023|Jinwoo Bae et.al.|
|**2022-05-20**|**[Self-Supervised Depth Estimation with Isometric-Self-Sample-Based Learning](http://arxiv.org/abs/2205.10006v1)**|None|None|None|Geonho Cha et.al.|
|**2022-05-19**|**[Diversity Matters: Fully Exploiting Depth Clues for Reliable Monocular 3D Object Detection](http://arxiv.org/abs/2205.09373v1)**|None|None|This paper has been accepted as an oral presentation of CVPR2022|Zhuoling Li et.al.|
|**2022-05-18**|**[Positional Information is All You Need: A Novel Pipeline for Self-Supervised SVDE from Videos](http://arxiv.org/abs/2205.08851v1)**|None|None|None|Juan Luis Gonzalez Bello et.al.|
|**2022-05-18**|**[Visual Attention-based Self-supervised Absolute Depth Estimation using Geometric Priors in Autonomous Driving](http://arxiv.org/abs/2205.08780v3)**|IEEE Robotics and Automation Letters, vol. 7, no. 4, pp.   11998-12005, Oct. 2022|**[link](https://github.com/xjixzz/vadepth-net)**|Published on IEEE Robotics and Automation Letters (RA-L)|Jie Xiang et.al.|
|**2022-05-18**|**[Learning Monocular Depth Estimation via Selective Distillation of Stereo Knowledge](http://arxiv.org/abs/2205.08668v1)**|None|None|None|Kyeongseob Song et.al.|
|**2022-05-17**|**[MulT: An End-to-End Multitask Learning Transformer](http://arxiv.org/abs/2205.08303v1)**|None|None|Accepted to CVPR 2022|Deblina Bhattacharjee et.al.|
|**2022-05-17**|**[Efficient Stereo Depth Estimation for Pseudo LiDAR: A Self-Supervised Approach Based on Multi-Input ResNet Encoder](http://arxiv.org/abs/2205.08089v1)**|None|None|9 pages, 5 figures|Sabir Hossain et.al.|
|**2022-05-11**|**[Review on Panoramic Imaging and Its Applications in Scene Understanding](http://arxiv.org/abs/2205.05570v2)**|None|None|Accepted to IEEE Transactions on Instrumentation and Measurement. 34   pages, 15 figures, 420 references|Shaohua Gao et.al.|
|**2022-05-09**|**[Is my Depth Ground-Truth Good Enough? HAMMER -- Highly Accurate Multi-Modal Dataset for DEnse 3D Scene Regression](http://arxiv.org/abs/2205.04565v1)**|None|None|None|HyunJun Jung et.al.|
|**2022-05-05**|**[FisheyeDistill: Self-Supervised Monocular Depth Estimation with Ordinal Distillation for Fisheye Cameras](http://arxiv.org/abs/2205.02930v1)**|None|None|None|Qingan Yan et.al.|
|**2022-05-05**|**[Exploiting Correspondences with All-pairs Correlations for Multi-view Depth Estimation](http://arxiv.org/abs/2205.02481v1)**|None|None|10 pages, 9 figures|Kai Cheng et.al.|
|**2022-05-03**|**[3D Semantic Scene Perception using Distributed Smart Edge Sensors](http://arxiv.org/abs/2205.01460v1)**|None|**[link](https://github.com/ais-bonn/jetsontrtperception)**|17th International Conference on Intelligent Autonomous Systems   (IAS), Zagreb, Croatia, June 2022|Simon Bultmann et.al.|
|**2022-05-03**|**[Outdoor Monocular Depth Estimation: A Research Review](http://arxiv.org/abs/2205.01399v1)**|None|None|None|Pulkit Vyas et.al.|
|**2022-05-02**|**[MUTR3D: A Multi-camera Tracking Framework via 3D-to-2D Queries](http://arxiv.org/abs/2205.00613v1)**|None|**[link](https://github.com/a1600012888/mutr3d)**|Appear on CVPR 2022 Workshop on Autonomous Driving|Tianyuan Zhang et.al.|
|**2022-04-30**|**[Unsupervised Visible-light Images Guided Cross-Spectrum Depth Estimation from Dual-Modality Cameras](http://arxiv.org/abs/2205.00257v1)**|None|None|None|Yubin Guo et.al.|
|**2022-04-29**|**[SideRT: A Real-time Pure Transformer Architecture for Single Image Depth Estimation](http://arxiv.org/abs/2204.13892v1)**|None|None|7 pages, 5 figures|Chang Shu et.al.|
|**2022-04-28**|**[Depth Estimation with Simplified Transformer](http://arxiv.org/abs/2204.13791v3)**|None|None|Accepted for the CVPR 2022 Transformers For Vision (T4V) workshop|John Yang et.al.|
|**2022-04-28**|**[Semi-MoreGAN: A New Semi-supervised Generative Adversarial Network for Mixture of Rain Removal](http://arxiv.org/abs/2204.13420v2)**|None|**[link](https://github.com/syy-whu/semi-moregan)**|18 pages|Yiyang Shen et.al.|
|**2022-04-24**|**[Simulating Fluids in Real-World Still Images](http://arxiv.org/abs/2204.11335v1)**|None|**[link](https://github.com/simon3dv/slr-sfs)**|Technical Report, 19 pages, 17 figures, project page:   https://slr-sfs.github.io/ code: https://github.com/simon3dv/SLR-SFS|Siming Fan et.al.|
|**2022-04-24**|**[RealNet: Combining Optimized Object Detection with Information Fusion Depth Estimation Co-Design Method on IoT](http://arxiv.org/abs/2204.11216v1)**|None|**[link](https://github.com/edithlzh/VNL_Estimation)**|None|Zhuohao Li et.al.|
|**2022-04-23**|**[Investigating Neural Architectures by Synthetic Dataset Design](http://arxiv.org/abs/2204.11045v1)**|None|**[link](https://github.com/AdrienCourtois/neural-networks-properties)**|Accepted at the VDU2022 workshop hosted at CVPR2022|Adrien Courtois et.al.|
|**2022-04-21**|**[Monocular Depth Estimation Using Cues Inspired by Biological Vision Systems](http://arxiv.org/abs/2204.10384v2)**|None|**[link](https://github.com/dylanauty/mde-biological-vision-systems)**|7 pages, 2 figures. Accepted to International Conference on Pattern   Recognition (ICPR) 2022. Code available at   https://github.com/DylanAuty/MDE-biological-vision-systems|Dylan Auty et.al.|
|**2022-04-19**|**[Photometric single-view dense 3D reconstruction in endoscopy](http://arxiv.org/abs/2204.09083v1)**|None|None|7 pages, 7 figures, submitted to IROS 2022|Victor M. Batlle et.al.|
|**2022-04-18**|**[Cylin-Painting: Seamless {360\textdegree} Panoramic Image Outpainting and Beyond](http://arxiv.org/abs/2204.08563v2)**|None|**[link](https://github.com/kangliao929/cylin-painting)**|None|Kang Liao et.al.|
|**2022-04-15**|**[Multi-Frame Self-Supervised Depth with Transformers](http://arxiv.org/abs/2204.07616v2)**|None|None|Accepted to CVPR 2022 (correct project page)|Vitor Guizilini et.al.|
|**2022-04-15**|**[MVSTER: Epipolar Transformer for Efficient Multi-View Stereo](http://arxiv.org/abs/2204.07346v1)**|None|**[link](https://github.com/jeffwang987/mvster)**|Code: https://github.com/JeffWang987/MVSTER|Xiaofeng Wang et.al.|
|**2022-04-14**|**[Joint Forecasting of Panoptic Segmentations with Difference Attention](http://arxiv.org/abs/2204.07157v1)**|None|**[link](https://github.com/cgraber/psf-diffattn)**|Accepted by CVPR 2022 (Oral)|Colin Graber et.al.|
|**2022-04-13**|**[Does depth estimation help object detection?](http://arxiv.org/abs/2204.06512v1)**|None|None|Accepted to Image and Vision Computing|Bedrettin Cetinkaya et.al.|
|**2022-04-11**|**[HiMODE: A Hybrid Monocular Omnidirectional Depth Estimation Model](http://arxiv.org/abs/2204.05007v1)**|None|None|IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR   2022)|Masum Shah Junayed et.al.|
|**2022-04-07**|**[SurroundDepth: Entangling Surrounding Views for Self-Supervised Multi-Camera Depth Estimation](http://arxiv.org/abs/2204.03636v3)**|None|**[link](https://github.com/weiyithu/surrounddepth)**|Accepted to CoRL 2022. Project page:   https://surrounddepth.ivg-research.xyz Code:   https://github.com/weiyithu/SurroundDepth|Yi Wei et.al.|
|**2022-04-07**|**[Task-Aware Active Learning for Endoscopic Image Analysis](http://arxiv.org/abs/2204.03440v1)**|None|**[link](https://github.com/thetna/endo-active-learn)**|None|Shrawan Kumar Thapa et.al.|
|**2022-04-05**|**[Depth-Guided Sparse Structure-from-Motion for Movies and TV Shows](http://arxiv.org/abs/2204.02509v1)**|None|**[link](https://github.com/amazon-research/small-baseline-camera-tracking)**|None|Sheng Liu et.al.|
|**2022-04-05**|**[Pyramid Frequency Network with Spatial Attention Residual Refinement Module for Monocular Depth Estimation](http://arxiv.org/abs/2204.02386v1)**|None|None|None|Zhengyang Lu et.al.|
|**2022-04-05**|**[P3Depth: Monocular Depth Estimation with a Piecewise Planarity Prior](http://arxiv.org/abs/2204.02091v1)**|None|**[link](https://github.com/syscv/p3depth)**|Accepted at CVPR 2022|Vaishakh Patil et.al.|
|**2022-04-04**|**[Monitoring social distancing with single image depth estimation](http://arxiv.org/abs/2204.01693v2)**|None|None|Accepted for pubblication on IEEE Transactions on Emerging Topics in   Computational Intelligence (TETCI)|Alessio Mingozzi et.al.|
|**2022-04-04**|**[MultiMAE: Multi-modal Multi-task Masked Autoencoders](http://arxiv.org/abs/2204.01678v1)**|None|**[link](https://github.com/EPFL-VILAB/MultiMAE)**|Project page at https://multimae.epfl.ch|Roman Bachmann et.al.|
|**2022-04-04**|**[Improving Monocular Visual Odometry Using Learned Depth](http://arxiv.org/abs/2204.01268v1)**|None|None|None|Libo Sun et.al.|
|**2022-04-03**|**[BinsFormer: Revisiting Adaptive Bins for Monocular Depth Estimation](http://arxiv.org/abs/2204.00987v1)**|None|**[link](https://github.com/zhyever/monocular-depth-estimation-toolbox)**|None|Zhenyu Li et.al.|
|**2022-03-31**|**[Casual 6-DoF: free-viewpoint panorama using a handheld 360 camera](http://arxiv.org/abs/2203.16756v1)**|None|None|12 pages, 13 figures|Rongsen Chen et.al.|
|**2022-03-30**|**[Towards Multimodal Depth Estimation from Light Fields](http://arxiv.org/abs/2203.16542v2)**|None|None|None|Titus Leistner et.al.|
|**2022-03-29**|**[Learning Structured Gaussians to Approximate Deep Ensembles](http://arxiv.org/abs/2203.15485v1)**|None|None|Accepted at CVPR 2022|Ivor J. A. Simpson et.al.|
|**2022-03-29**|**[Light Field Depth Estimation via Stitched Epipolar Plane Images](http://arxiv.org/abs/2203.15201v3)**|None|**[link](https://github.com/pingzhou-lf/light-field-depth-estimation-based-on-stitched-epis)**|16 pages|Ping Zhou et.al.|
|**2022-03-29**|**[Self-Supervised Light Field Depth Estimation Using Epipolar Plane Images](http://arxiv.org/abs/2203.15171v1)**|3DV 2021: International Conference on 3D Vision|None|None|Kunyuan Li et.al.|
|**2022-03-28**|**[LocalBins: Improving Depth Estimation by Learning Local Distributions](http://arxiv.org/abs/2203.15132v1)**|None|**[link](https://github.com/shariqfarooq123/localbins)**|19 pages|Shariq Farooq Bhat et.al.|
|**2022-03-28**|**[Learning Optical Flow, Depth, and Scene Flow without Real-World Labels](http://arxiv.org/abs/2203.15089v2)**|None|None|Accepted to RA-L + ICRA 2022 (correct project page)|Vitor Guizilini et.al.|
|**2022-03-27**|**[DepthFormer: Exploiting Long-Range Correlation and Local Information for Accurate Monocular Depth Estimation](http://arxiv.org/abs/2203.14211v1)**|Machine Intelligence Research 2023|**[link](https://github.com/zhyever/Monocular-Depth-Estimation-Toolbox/tree/main/configs/depthformer)**|None|Zhenyu Li et.al.|
|**2022-03-26**|**[Learn to Adapt for Monocular Depth Estimation](http://arxiv.org/abs/2203.14005v1)**|None|None|None|Qiyu Sun et.al.|
|**2022-03-26**|**[On the Viability of Monocular Depth Pre-training for Semantic Segmentation](http://arxiv.org/abs/2203.13987v4)**|None|None|None|Dong Lao et.al.|
|**2022-03-24**|**[Unsupervised Simultaneous Learning for Camera Re-Localization and Depth Estimation from Video](http://arxiv.org/abs/2203.12804v1)**|None|None|8 pages, 6 figures|Shun Taguchi et.al.|
|**2022-03-23**|**[CroMo: Cross-Modal Learning for Monocular Depth Estimation](http://arxiv.org/abs/2203.12485v2)**|None|None|Accepted for publication at CVPR2022|Yannick Verdié et.al.|
|**2022-03-21**|**[DiffPoseNet: Direct Differentiable Camera Pose Estimation](http://arxiv.org/abs/2203.11174v1)**|None|None|10 pages, 5 figures, Accepted to CVPR 2022|Chethan M. Parameshwara et.al.|
|**2022-03-21**|**[MonoDTR: Monocular 3D Object Detection with Depth-Aware Transformer](http://arxiv.org/abs/2203.10981v2)**|None|**[link](https://github.com/kuanchihhuang/monodtr)**|Accepted to CVPR 2022|Kuan-Chih Huang et.al.|
|**2022-03-21**|**[Learning Occlusion-Aware Coarse-to-Fine Depth Map for Self-supervised Monocular Depth Estimation](http://arxiv.org/abs/2203.10925v2)**|None|**[link](https://github.com/ZM-Zhou/SMDE-Pytorch)**|Accepted at ACM Multimedia 2022|Zhengming Zhou et.al.|
|**2022-03-21**|**[Depth Completion using Geometry-Aware Embedding](http://arxiv.org/abs/2203.10912v2)**|None|**[link](https://github.com/Wenchao-Du/GAENet)**|Accepted by ICRA22|Wenchao Du et.al.|
|**2022-03-20**|**[Depth Estimation by Combining Binocular Stereo and Monocular Structured-Light](http://arxiv.org/abs/2203.10493v1)**|None|**[link](https://github.com/yuhuaxu/monostereofusion)**|CVPR 2022|Yuhua Xu et.al.|
|**2022-03-18**|**[Semi-Supervised Learning with Mutual Distillation for Monocular Depth Estimation](http://arxiv.org/abs/2203.09737v1)**|IEEE Conference on Robotics and Automation (ICRA) 2022|None|None|Jongbeom Baek et.al.|
|**2022-03-18**|**[Distortion-Tolerant Monocular Depth Estimation On Omnidirectional Images Using Dual-cubemap](http://arxiv.org/abs/2203.09733v1)**|None|None|Accepted by ICME2021, poster|Zhijie Shen et.al.|
|**2022-03-17**|**[PanoFormer: Panorama Transformer for Indoor 360 Depth Estimation](http://arxiv.org/abs/2203.09283v2)**|None|**[link](https://github.com/zhijieshen-bjtu/panoformer)**|Accepted to ECCV2022|Zhijie Shen et.al.|
|**2022-03-16**|**[MonoJSG: Joint Semantic and Geometric Cost Volume for Monocular 3D Object Detection](http://arxiv.org/abs/2203.08563v1)**|None|**[link](https://github.com/lianqing11/monojsg)**|Accepted to CVPR 2022|Qing Lian et.al.|
|**2022-03-10**|**[SelfTune: Metrically Scaled Monocular Depth Estimation through Self-Supervised Learning](http://arxiv.org/abs/2203.05332v1)**|None|None|None|Jaehoon Choi et.al.|
|**2022-03-09**|**[Joint Learning of Salient Object Detection, Depth Estimation and Contour Extraction](http://arxiv.org/abs/2203.04895v2)**|None|**[link](https://github.com/xiaoqi-zhao-dlut/mmft)**|Accepted by IEEE TIP|Xiaoqi Zhao et.al.|
|**2022-03-09**|**[A high-precision self-supervised monocular visual odometry in foggy weather based on robust cycled generative adversarial networks and multi-task learning aided depth estimation](http://arxiv.org/abs/2203.04812v1)**|None|None|None|Xiuyuan Li et.al.|
|**2022-03-09**|**[ChiTransformer:Towards Reliable Stereo from Cues](http://arxiv.org/abs/2203.04554v4)**|None|**[link](https://github.com/isl-cv/chitransformer)**|Published as a main conference paper at CVPR 2022|Qing Su et.al.|
|**2022-03-09**|**[Monocular Depth Distribution Alignment with Low Computation](http://arxiv.org/abs/2203.04538v1)**|None|**[link](https://github.com/yilim1/danet)**|Accepted by ICRA 2022|Fei Sheng et.al.|
|**2022-03-08**|**[Lightweight Monocular Depth Estimation through Guided Decoding](http://arxiv.org/abs/2203.04206v1)**|None|**[link](https://github.com/mic-rud/guideddecoding)**|Accepted to ICRA 2022|Michael Rudolph et.al.|
|**2022-03-06**|**[Point Spread Function Estimation of Defocus](http://arxiv.org/abs/2203.02953v2)**|None|**[link](https://github.com/cubhe/precise-point-spread-function-estimation)**|None|Renzhi He et.al.|
|**2022-03-04**|**[Real-Time Hybrid Mapping of Populated Indoor Scenes using a Low-Cost Monocular UAV](http://arxiv.org/abs/2203.02453v1)**|None|None|Submitted to IROS 2022|Stuart Golodetz et.al.|
|**2022-03-04**|**[Time-to-Label: Temporal Consistency for Self-Supervised Monocular 3D Object Detection](http://arxiv.org/abs/2203.02193v1)**|None|None|None|Issa Mouawad et.al.|
|**2022-03-04**|**[PatchMVSNet: Patch-wise Unsupervised Multi-View Stereo for Weakly-Textured Surface Reconstruction](http://arxiv.org/abs/2203.02156v1)**|None|None|None|Haonan Dong et.al.|
|**2022-03-04**|**[Pseudo-Stereo for Monocular 3D Object Detection in Autonomous Driving](http://arxiv.org/abs/2203.02112v1)**|None|**[link](https://github.com/revisitq/Pseudo-Stereo-3D)**|Accepted to CVPR 2022|Yi-Nan Chen et.al.|
|**2022-03-03**|**[Fast Neural Architecture Search for Lightweight Dense Prediction Networks](http://arxiv.org/abs/2203.01994v3)**|None|None|15 pages, 11 figures, 8 tables. arXiv admin note: substantial text   overlap with arXiv:2108.11105|Lam Huynh et.al.|
|**2022-03-03**|**[Occlusion-Aware Cost Constructor for Light Field Depth Estimation](http://arxiv.org/abs/2203.01576v1)**|None|**[link](https://github.com/yingqianwang/oacc-net)**|Accepted to CVPR 2022|Yingqian Wang et.al.|
|**2022-03-02**|**[MUAD: Multiple Uncertainties for Autonomous Driving, a benchmark for multiple uncertainty types and tasks](http://arxiv.org/abs/2203.01437v2)**|None|**[link](https://github.com/ENSTA-U2IS-AI/torch-uncertainty)**|Accepted at BMVC 2022|Gianni Franchi et.al.|
|**2022-03-02**|**[Detecting Adversarial Perturbations in Multi-Task Perception](http://arxiv.org/abs/2203.01177v2)**|None|**[link](https://github.com/ifnspaml/advattackdet)**|Accepted at IROS 2022|Marvin Klingner et.al.|
|**2022-03-02**|**[OmniFusion: 360 Monocular Depth Estimation via Geometry-Aware Fusion](http://arxiv.org/abs/2203.00838v2)**|None|**[link](https://github.com/yuyanli0831/omnifusion)**|CVPR 2022, accepted as Oral|Yuyan Li et.al.|
|**2022-02-26**|**[How Much Depth Information can Radar Contribute to a Depth Estimation Model?](http://arxiv.org/abs/2202.13220v2)**|None|None|published on EI2023, 7 pages, 4 figures, 2 tables|Chen-Chou Lo et.al.|
|**2022-02-26**|**[Uncertainty-Aware Deep Multi-View Photometric Stereo](http://arxiv.org/abs/2202.13071v2)**|None|None|Accepted for publication in IEEE/CVF CVPR 2022. (11 Pages, 6 Figures,   3 Tables)|Berk Kaya et.al.|
|**2022-02-26**|**[Deep Depth from Focal Stack with Defocus Model for Camera-Setting Invariance](http://arxiv.org/abs/2202.13055v1)**|None|None|13 pages|Yuki Fujimura et.al.|
|**2022-02-24**|**[Light Robust Monocular Depth Estimation For Outdoor Environment Via Monochrome And Color Camera Fusion](http://arxiv.org/abs/2202.12108v1)**|None|None|None|Hyeonsoo Jang et.al.|

